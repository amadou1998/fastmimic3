### FILE: .\condensator.py ###
import os
from pathlib import Path

def write_codebase_to_file(root_dir, output_file):
    with open(output_file, 'w') as outfile:
        for subdir, _, files in os.walk(root_dir):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(subdir, file)
                    print(file_path)
                    outfile.write(f"### FILE: {file_path} ###\n")
                    with open(file_path, 'r') as infile:
                        outfile.write(infile.read())
                        outfile.write("\n\n")

if __name__ == "__main__":
    root_directory = '.'  # Replace with the path to your codebase
    output_filename = 'condensed_codebase.txt'  # Replace with your desired output file name
    write_codebase_to_file(root_directory, output_filename)
    print(f"Codebase has been written to {output_filename}")

### FILE: .\docs\source\conf.py ###
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

from dotenv import load_dotenv
import os
import sys
sys.path.insert(0, os.path.abspath('../src'))
load_dotenv()

project = 'open-mimic-iii'
copyright = '2024, Amadou Wolfgang Cisse'
author = 'Amadou Wolfgang Cisse'
release = '-'

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.napoleon",
    "sphinx.ext.viewcode",
]

templates_path = ['_templates']
exclude_patterns = [
]

exclude_patterns = ["../src/managers.py"]

# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output

html_theme = 'sphinx_rtd_theme'
html_static_path = ['_static']


### FILE: .\entry_point\main.py ###
import os
import argparse
from src import CaseHandler

os.environ["DEBUG"] = "1"

if __name__ == "__main__":
    case = CaseHandler()
    parser = argparse.ArgumentParser(
        description='Run the MIMIC pipeline with configuration from the directory configuration.'
        ' The model configuration need to contain a data_config.json, '
        'pipeline_config.json and model_config.json')
    parser.add_argument('path',
                        type=str,
                        help='Absolute path to the directory containint the config.json.')
    args = parser.parse_args()
    case.run(args.path)
    #


### FILE: .\examples\example_utils.py ###
from utils import *
from datasets import ProcessedSetReader
from typing import Dict, List, Tuple
from examples.settings import YERVA_SPLIT
from datasets.readers import SplitSetReader


def print_split_info(reader: ProcessedSetReader, split_set: Dict[str, List[str]]):
    train_subjects = split_set["train"]
    val_subjects = split_set["val"]
    test_subjects = split_set["test"]
    train_percent = len(train_subjects) / len(reader.subject_ids) * 100
    val_percent = len(val_subjects) / len(reader.subject_ids) * 100
    test_percent = len(test_subjects) / len(reader.subject_ids) * 100

    max_len = max(len(train_subjects), len(val_subjects), len(test_subjects))
    width = len(str(max_len))

    info_io(f"Train: {len(train_subjects):<{width}} ({train_percent:0.2f}%) \n"
            f"Val:   {len(val_subjects):<{width}} ({val_percent:0.2f}%) \n"
            f"Test:  {len(test_subjects):<{width}} ({test_percent:0.2f}%)")


def benchmark_split_subjects() -> Tuple[List[int], List[int]]:
    print("Test and validation sets found...")
    test_subjects = pd.read_csv(Path(YERVA_SPLIT, "testset.csv"), header=None)
    test_subjects.columns = ["subjects", "affiliation"]
    test_subjects = test_subjects[test_subjects["affiliation"] == 1]["subjects"].astype(
        int).tolist()

    val_subjects = pd.read_csv(Path(YERVA_SPLIT, "valset.csv"), header=None)
    val_subjects.columns = ["subjects", "affiliation"]
    val_subjects = val_subjects[val_subjects["affiliation"] == 1]["subjects"].astype(int).tolist()
    return test_subjects, val_subjects


def benchmark_split_reader(reader: ProcessedSetReader, test_subjects: List[str],
                           val_subjects: List[str]) -> SplitSetReader:
    # Split data as in original set
    test_subjects = list(set(reader.subject_ids) & set(test_subjects))
    val_subjects = list(set(reader.subject_ids) & set(val_subjects))
    train_subjects = list(set(reader.subject_ids) - set(test_subjects) - set(val_subjects))

    split_sets = {"test": test_subjects, "val": val_subjects, "train": train_subjects}
    split_reader = SplitSetReader(reader.root_path, split_sets)

    # Print result
    print_split_info(reader, split_sets)
    return split_reader


### FILE: .\examples\settings.py ###
import os
from river import optim
from pathlib import Path
from dotenv import load_dotenv

__all__ = [
    "TEMP_DIR", "EXAMPLE_DIR", "EXAMPLE_DATA_DEMO", "EXAMPLE_DATA", "TEMP_DIR", "YERVA_SPLIT",
    "LOG_METRICS", "NETWORK_METRICS", "BENCHMARK_MODEL", "LOG_REG_PARAMS", "STANDARD_LSTM_PARAMS",
    "STANDARD_LSTM_DS_PARAMS", "CHANNEL_WISE_LSTM_PARAMS"
]

load_dotenv()

EXAMPLE_DIR = Path(os.getenv("EXAMPLES"))
EXAMPLE_DATA = Path(EXAMPLE_DIR, "data")
EXAMPLE_DATA_DEMO = Path(
    EXAMPLE_DATA,
    "physionet.org",
    "files",
    "mimiciii-demo",
    "1.4",
)
TEMP_DIR = Path(EXAMPLE_DATA, "temp")
YERVA_SPLIT = Path(EXAMPLE_DIR, "yerva_nn_benchmark", "data_split")
LOG_METRICS = {
    "PHENO": ["micro_roc_auc", "macro_roc_auc"],
    "DECOMP": ["roc_auc", "pr_auc", "classification_report"],
    "IHM": ["roc_auc", "pr_auc", "classification_report"],
    "LOS": ["cohen_kappa", "mae", "los_classification_report"]
}
LOG_REG_PARAMS = {
    "PHENO": {
        "l2": 0.1,
    },
    "DECOMP": {
        "l2": 0.001,
    },
    "IHM": {
        "l2": 0.001,
    },
    "LOS": {
        "optimizer": optim.SGD(lr=0.00001),
    }
}
NETWORK_METRICS = {
    "PHENO": ["micro_roc_auc", "macro_roc_auc"],
    "DECOMP": [
        "roc_auc",
        "pr_auc",
    ],
    "IHM": ["roc_auc", "pr_auc"],
    "LOS": ["cohen_kappa", "mae"]
}
STANDARD_LSTM_PARAMS = {
    "PHENO": {
        "model": {
            "input_dim": 59,
            "dropout": 0.3,
            "layer_size": 256,
            "depth": 1,
            "final_activation": "sigmoid",
            "output_dim": 25
        },
        "generator_options": {
            "batch_size": 8
        },
        "compile_options": {
            "loss": "binary_crossentropy",
            "optimizer": "adam"
        },
        "training": {
            "epochs": 100
        }
    },
    "DECOMP": {
        "model": {
            "input_dim": 59,
            "final_activation": "sigmoid",
            "layer_size": 128,
            "depth": 1
        },
        "compile_options": {
            "loss": "binary_crossentropy",
            "optimizer": "adam"
        },
        "generator_options": {
            "batch_size": 8,
        },
        "training": {
            "epochs": 100
        }
    },
    "IHM": {
        "model": {
            "input_dim": 59,
            "dropout": 0.3,
            "final_activation": "sigmoid",
            "output_dim": 1,
            "layer_size": 16,
            "depth": 2
        },
        "compile_options": {
            "loss": "binary_crossentropy",
            "optimizer": "adam"
        },
        "generator_options": {
            "batch_size": 8,
        },
        "training": {
            "epochs": 100
        }
    },
    "LOS": {
        "model": {
            "input_dim": 59,
            "dropout": 0.3,
            "layer_size": 64,
            "depth": 1,
            "final_activation": "softmax",  # if partition is none then relu
            "output_dim": 10  # if partition is none then only 1
        },
        "compile_options": {
            "loss":
                "sparse_categorical_crossentropy",  # mean_squared_logarithmic_error if partition is None
            "optimizer": "adam"
        },
        "generator_options": {
            "batch_size": 8,
            "partition":
                "custom"  # Can be custom, log and None and has effects on the bining of the target 
        },
        "training": {
            "epochs": 100
        }
    }
}

STANDARD_LSTM_DS_PARAMS = {
    "PHENO": {
        "dim": 256,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "target_repl_coef": 0.5
    },
    "DECOMP": {
        "dim": 128,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "deep_supervision": True
    },
    "IHM": {
        "dim": 32,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "target_repl_coef": 0.5
    },
    "LOS": {
        "dim": 128,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "partition": "custom",
        "deep_supervision": True
    }
}

CHANNEL_WISE_LSTM_PARAMS = {
    "PHENO": {
        "dim": 16,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "size_coef": 8.0
    },
    "DECOMP": {
        "dim": 16,
        "depth": 1,
        "batch_size": 8,
        "size_coef": 4.0
    },
    "IHM": {
        "dim": 8,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "size_coef": 4.0
    },
    "LOS": {
        "dim": 16,
        "depth": 1,
        "batch_size": 8,
        "size_coef": 8.0,
        "partition": "custom"
    }
}

BENCHMARK_MODEL = Path(EXAMPLE_DATA, "benchmark_models")


### FILE: .\examples\etc\convert_columns.py ###
import pandas as pd
from datasets.mimic_utils import upper_case_column_names
from tests.settings import *

for csv in TEST_DATA_DEMO.iterdir():
    if csv.is_dir() or csv.suffix != ".csv":
        continue

    df = pd.read_csv(csv,
                     dtype={
                         "ROW_ID": 'Int64',
                         "ICUSTAY_ID": 'Int64',
                         "HADM_ID": 'Int64',
                         "SUBJECT_ID": 'Int64',
                         "row_id": 'Int64',
                         "icustay_id": 'Int64',
                         "hadm_id": 'Int64',
                         "subject_id": 'Int64'
                     },
                     low_memory=False)
    df = upper_case_column_names(df)
    df.to_csv(csv, index=False)


### FILE: .\examples\yerva_nn_benchmark\__init__.py ###
import argparse
import datasets
import pandas as pd
from pathlib import Path
from datasets.readers import SplitSetReader
from typing import List
from utils.IO import *
from examples.settings import *
from examples.example_utils import benchmark_split_reader, benchmark_split_subjects
from examples.yerva_nn_benchmark.scripts.logistic_regression import run_log_reg
from examples.yerva_nn_benchmark.scripts.lstm import run_standard_lstm
# from .decomp import logistic_regression, lstm_channel_wise, lstm, river_models
# from .ihm import logistic_regression, lstm_channel_wise, lstm, river_models
# from .los import logistic_regression, lstm_channel_wise, lstm, river_models
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Creation of the original Yerva NN benchmark"
                                     "We also added some models for the fun of it :). "
                                     "Multitasking will be added during future maintenance.")
    parser.add_argument('--mimic_dir',
                        type=str,
                        default=EXAMPLE_DATA,
                        help="Path to your MIMIC-III dataset installation. "
                        "If not provided, the downloaded example dataset"
                        " will be used but the results will be invalid.")

    parser.add_argument('--tasks',
                        type=List[str],
                        default=["IHM", "LOS", "DECOMP", "PHENO"],
                        help="This is the list of tasks for which the benchmark is to be created. ")

    parser.add_argument(
        '--models',
        type=List[str],
        default=["logistic_regression", "lstm_channel_wise", "lstm", "river_models"],
        help="This is the list of models for which the benchmark is tó be created. ")

    args, _ = parser.parse_known_args()

    if not Path(YERVA_SPLIT, "testset.csv").is_file() or not Path(YERVA_SPLIT,
                                                                  "valset.csv").is_file():
        raise FileNotFoundError(
            f"The testset.csv or valset.csv file is missing. Please run the bash script "
            f"from examples/etc/setup.sh or .ps1"
            f"\nExpected location was {YERVA_SPLIT}")
    else:
        # These are fetched from the original github https://github.com/YerevaNN/mimic3-benchmarks
        test_subjects, val_subjects = benchmark_split_subjects()

    for task_name in args.tasks:
        info_io(f"Creating benchmark for task {task_name}")
        if not set(["lstm_channel_wise", "lstm"]) - set(args.models):
            reader = datasets.load_data(chunksize=75836,
                                        source_path=EXAMPLE_DATA_DEMO,
                                        storage_path=TEMP_DIR,
                                        discretize=True,
                                        time_step_size=1.0,
                                        start_at_zero=True,
                                        impute_strategy='previous',
                                        task=task_name)

            info_io(f"Splitting data for task {task_name}", level=0)

            split_reader = benchmark_split_reader(reader, test_subjects, val_subjects)

            if "lstm" in args.models:
                storage_path = Path(BENCHMARK_MODEL, task_name, "lstm")
                storage_path.mkdir(parents=True, exist_ok=True)

                run_standard_lstm(task_name=task_name,
                                  reader=split_reader,
                                  storage_path=storage_path,
                                  metrics=NETWORK_METRICS[task_name],
                                  params=STANDARD_LSTM_PARAMS[task_name])

        # Classical classifiers
        if False and not set(["logistic_regression", "river_models"]) - set(args.models):
            reader = datasets.load_data(chunksize=75836,
                                        source_path=EXAMPLE_DATA_DEMO,
                                        storage_path=TEMP_DIR,
                                        engineer=True,
                                        task=task_name)
            info_io(f"Splitting data for task {task_name}", level=0)

            split_reader = benchmark_split_reader(reader, test_subjects, val_subjects)

            if "logistic_regression" in args.models:
                storage_path = Path(BENCHMARK_MODEL, task_name, "logistic_regression")
                storage_path.mkdir(parents=True, exist_ok=True)

                run_log_reg(task_name=task_name,
                            reader=split_reader,
                            storage_path=storage_path,
                            metrics=LOG_METRICS[task_name],
                            params=LOG_REG_PARAMS[task_name])
            pass


### FILE: .\examples\yerva_nn_benchmark\scripts\logistic_regression.py ###
import datasets
from river import multiclass
from pathlib import Path
from datasets.readers import SplitSetReader
from models.stream.linear_model import LogisticRegression, LinearRegression, MultiOutputLogisticRegression
from pipelines.stream import RiverPipeline


def run_log_reg(task_name: str, reader: SplitSetReader, storage_path: Path, metrics: list,
                params: dict):
    if task_name == "LOS":
        run_linear_reg(reader=reader, storage_path=storage_path, metrics=metrics, params=params)
    elif task_name in ["DECOMP", "IHM"]:
        run_binary_log_reg(reader=reader, storage_path=storage_path, metrics=metrics, params=params)
    elif task_name == "PHENO":
        run_multioutput_log_reg(reader=reader,
                                storage_path=storage_path,
                                metrics=metrics,
                                params=params)


def run_binary_log_reg(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    model = LogisticRegression(metrics=metrics, **params)
    pipe = RiverPipeline(storage_path=storage_path, reader=reader, model=model)
    pipe.fit(no_subdirs=True)
    pipe.test()


def run_multioutput_log_reg(reader: SplitSetReader, storage_path: Path, metrics: list,
                            params: dict):
    model = MultiOutputLogisticRegression(metrics=metrics, **params)
    pipe = RiverPipeline(storage_path=storage_path, reader=reader, model=model)
    pipe.fit(no_subdirs=True)
    pipe.test()


def run_linear_reg(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    model = LinearRegression(metrics=metrics, **params)
    pipe = RiverPipeline(storage_path=storage_path, reader=reader, model=model)
    pipe.fit(no_subdirs=True)
    pipe.test()


### FILE: .\examples\yerva_nn_benchmark\scripts\lstm.py ###
from models.pytorch.lstm import LSTMNetwork
from tests.settings import *
from pipelines.pytorch import TorchPipeline
from pathlib import Path
from datasets.readers import SplitSetReader
from copy import deepcopy


def run_standard_lstm(task_name: str, reader: SplitSetReader, storage_path: Path, metrics: list,
                      params: dict):

    params = deepcopy(params)

    model = LSTMNetwork(**params.pop("model"))
    training_params = params.pop("training")
    params["compile_options"].update({"metrics": metrics})
    pipe = TorchPipeline(storage_path=storage_path, reader=reader, model=model,
                         **params).fit(no_subdirs=True, **training_params)


def run_binary_lstm(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    params = deepcopy(params)
    model = LSTMNetwork(**params.pop("model"))
    training_params = params.pop("training")
    if "compile_options" in params:
        params["compile_options"].update(params.pop("compile_options"))
    else:
        params["compile_options"] = params.pop("compile_options")
    pipe = TorchPipeline(storage_path=storage_path, reader=reader, model=model,
                         **params).fit(no_subdirs=True, **training_params)


def run_multilabel_lstm(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    ...


def run_regression_lstm(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    ...


### FILE: .\examples\yerva_nn_benchmark\scripts\lstm_channel_wise.py ###


### FILE: .\examples\yerva_nn_benchmark\scripts\river_models.py ###


### FILE: .\examples\yerva_nn_benchmark\scripts\__init__.py ###


### FILE: .\model_templates\__init__.py ###


### FILE: .\src\handlers.py ###

import datasets
from datasets.readers import ProcessedSetReader
from pathlib import Path
from utils import load_json, write_json
from utils.IO import info_io, error_io
from sklearn.linear_model import LogisticRegression, SGDClassifier
from models.sklearn.standard.linear_models import StandardLogReg
from sklearn.ensemble import RandomForestClassifier
from models.tf2.lstm import LSTMNetwork
from models.tf2.logistic_regression import IncrementalLogReg as IncrementalLogRegTF2
from models.sklearn.incremental.linear_models import IncrementalLogRegSKLearn
from pipelines.nn import MIMICPipeline as MIMICNNPipeline
from pipelines.regression import MIMICPipeline as MIMICRegPipeline
from tensorflow.keras.metrics import AUC
from copy import deepcopy


class MultiCaseHandler(object):
    """_summary_
    """

    def __init__(self):
        """_summary_
        """
        pass

    def run(path: Path):
        """_summary_

        Args:
            path (Path): _description_
        """
        directories = list(path.iterdir())
        for directory in directories:
            case = CaseHandler()
            case.run(directory)


class AbstractCaseHandler(object):
    """_summary_
    """

    def __init__(self) -> None:
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        #TODO! unused
        self.regression_models = ["sgd_classifier", "logistic_regression", "random_forest"]
        self.neural_network_models = ["lstm"]
        self.subcase_configs = list()

    def read_case_config(self, case_config: dict, case_folder: Path):
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        # Constants
        name = case_config["name"]
        task = case_config["task"]
        frame_work = case_config["pipeline_config"]["framework"]
        model_type = case_config["model_type"]

        # Compose default pipeline config
        case_config["pipeline_config"].update({
            "model_name": name,
            "root_path": case_folder,
        })
        if task in ["IHM", "DECOMP"]:
            case_config["pipeline_config"]["output_type"] = "sparse"

        # Compose default data config
        case_config["data_config"] = deepcopy(case_config["data_config"])
        if not "extracted" == Path(case_config["data_config"]["storage_path"]).name:
            case_config["data_config"].update({
                "storage_path": str(Path(case_config["data_config"]["storage_path"], "extracted")),
                "task": task
            })

        # Compose defualt config
        default_config = {
            "name": name,
            "model_type": model_type,
            "data_config": case_config["data_config"],
            "model_config": case_config["model_config"],
            "pipeline_config": case_config["pipeline_config"],
        }

        case_config = self.custom_configs(default_config, case_config)

        return default_config, model_type, frame_work, task, name, case_folder

    def write_case_folder(self):
        """_summary_
        """

    def run(self, case_folder: Path):
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        case_config_data = load_json(Path(case_folder, "config.json"))
        if "subcases" in case_config_data:
            for data in case_config_data["subcases"]:
                (config, \
                model_type, \
                framework, \
                task, \
                name, \
                root_path) = self.read_case_config(data, case_folder)

                info_io(f"Loading model config from: {str(Path(root_path, name))}")

                case_path = Path(root_path, name, task)
                case_path.mkdir(parents=True, exist_ok=True)

                write_json(Path(case_path, "config.json"), config)
                if model_type in self.regression_models:
                    self.regression(config, framework, model_type, task)
                elif model_type in self.neural_network_models:
                    self.neural_network(config, framework, model_type, task)
                else:
                    raise ValueError(
                        f"Model type needs to be in {', '.join(str(x) for x in list([*self.regression_models, *self.neural_network_models]))}"
                    )
        else:
            (config, \
            model_type, \
            framework, \
            task, \
            name, \
            root_path) = self.read_case_config(case_config_data, case_folder)

            info_io(f"Loading model config from: {str(Path(root_path))}")

            case_path = Path(root_path)
            case_path.mkdir(parents=True, exist_ok=True)

            write_json(Path(case_path, "config.json"), config)
            print(config)
            if model_type in self.regression_models:
                self.regression(config, framework, model_type, task)
            elif model_type in self.neural_network_models:
                self.neural_network(config, framework, model_type, task)
            else:
                raise ValueError(
                    f"Model type needs to be in {', '.join(str(x) for x in list([*self.regression_models, *self.neural_network_models]))}"
                )

            # except Exception as e:
            #    print(e)

    def regression(self, config, framework, model_type):
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        raise NotImplementedError()

    def neural_network(self, config, framework, model_type):
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        raise NotImplementedError()

    def custom_configs(self, config: dict) -> dict:
        return config


class CaseHandler(AbstractCaseHandler):
    """_summary_

    Args:
        AbstractCaseHandler (_type_): _description_
    """

    def __init__(self) -> None:
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        super().__init__()

    def custom_configs(self, config: dict, config_json: dict) -> dict:
        """_summary_

        Args:
            config (dict): _description_
            config_json (dict): _description_

        Returns:
            dict: _description_
        """
        config["pipeline_config"]["task"] = config_json["task"]
        config["task"] = config_json["task"]

        return config

    def regression(self, config, framework, model_type, task):
        """_summary_
        """
        regression_models = {
            "tf2": {
                "logistic_regression": IncrementalLogRegTF2
            },
            "sklearn": {
                "incremental": {
                    "sgd_classifier": SGDClassifier,
                    "logistic_regression": IncrementalLogRegSKLearn,
                },
                "standard": {
                    "random_forest": RandomForestClassifier,
                    "logistic_regression": StandardLogReg
                }
            }
        }

        if framework == "sklearn":
            model_switch = regression_models["sklearn"]
            if task in ["IHM", "PHENO"]:
                model_switch = model_switch["standard"]
            else:
                model_switch = model_switch["incremental"]
            try:
                model = model_switch[model_type](task=task,
                                                 random_state=42,
                                                 **config["model_config"])
            except KeyError:
                raise ValueError(
                    f"For the framework {framework}, only the models \"{', '.join(str(x) for x in model_switch)}\" are available."
                )

        else:
            model_switch = regression_models["tf2"]
            try:
                model = model_switch[model_type](task=task,
                                                 input_dim=714,
                                                 random_state=42,
                                                 **config["model_config"])
            except KeyError:
                raise ValueError(
                    f"For the framework {framework}, only the models \"{', '.join(str(x) for x in model_switch)}\" are available."
                )
        config["data_config"].update({"preprocess": True, "engineer": True})

        self.run_case(MIMICRegPipeline(model, **config["pipeline_config"]), config["data_config"])

    def neural_network(self, config, framework, model_type):
        """_summary_
        """
        custom_objects = {"auc_2": AUC(curve='ROC'), "auc_3": AUC(curve='PR')}
        neural_network_models = {"lstm": LSTMNetwork}

        model = neural_network_models[model_type](**config["model_config"])
        config["data_config"].update({"preprocess": True})
        self.run_case(
            MIMICNNPipeline(model, custom_objects=custom_objects, **config["pipeline_config"]),
            config["data_config"])

    def run_case(self, pipeline, data_config):
        """_summary_

        Args:
            pipeline (_type_): _description_
        """
        try:
            if not "chunksize" in data_config.keys():
                timeseries, \
                episodic_data, \
                subject_diagnoses, \
                subject_icu_history = datasets.load_data(**data_config)

                pipeline.fit(timeseries=timeseries,
                             episodic_data=episodic_data,
                             subject_diagnoses=subject_diagnoses,
                             subject_icu_history=subject_icu_history)
            else:
                data_path = datasets.load_data(**data_config)
                pipeline.fit(data_path=data_path)
        except Exception as e:
            error_io("Encountered exception:")
            error_io(e)
            error_io("Case is finalized and shut down!")



### FILE: .\src\managers.py ###
import shutil
import numpy as np
import tensorflow as tf
from pathlib import Path
from utils.IO import *
from utils import load_json, update_json
from tensorflow.keras import Model


class AbstractCheckpointManager(object):
    """_summary_
    """

    def __init__(self, directory):
        """_summary_
        """
        if isinstance(directory, str):
            self.directory = Path(directory)
        else:
            self.directory = directory

        self.custom_objects = []

    @property
    def latest(self):
        """_summary_
        """
        return self.latest_epoch()

    def load_model(self):
        """_summary_
        """
        model_path = Path(self.directory, f"cp-{self.latest:04d}.ckpt")
        info_io(f"Loading model from epoch {self.latest}.")
        model = tf.keras.models.load_model(model_path, self.custom_objects)

        return model

    def load_weights(self, model: Model):
        """_summary_

        Args:
            model (_type_): _description_
        """
        latest_cp_name = tf.train.latest_checkpoint(self.directory)
        model.load_weights(latest_cp_name)
        return model

    def latest_epoch(self):
        """_summary_

        Returns:
            _type_: _description_
        """
        raise NotImplementedError("This is an abstract class!")

    def clean_directory(self, best_epoch: int, keep_latest: bool = True):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """
        raise NotImplementedError("This is an abstract class!")

    def is_empty(self):
        """_summary_
        """
        if self.latest == 0:
            return True
        return False


class CheckpointManager(AbstractCheckpointManager):
    """_summary_
    """

    def __init__(self, directory, train_epochs, custom_objects):
        """_summary_

        Args:
            directory (_type_): _description_
            train_epochs (_type_): _description_
            custom_objects (_type_): _description_
        """
        super().__init__(directory)
        self.epochs = train_epochs
        self.custom_objects = custom_objects

    def latest_epoch(self):
        """_summary_

        Returns:
            _type_: _description_
        """

        check_point_epochs = [
            i for i in range(self.epochs + 1) for folder in self.directory.iterdir()
            if f"{i:04d}" in folder.name
        ]

        if check_point_epochs:
            return max(check_point_epochs)

        return 0

    def clean_directory(self, best_epoch: int, keep_latest: bool = True):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """

        [
            shutil.rmtree(folder)
            for i in range(self.epochs + 1)
            for folder in self.directory.iterdir()
            if f"{i:04d}" in folder.name and ((i != self.epochs) or not keep_latest) and
            (i != best_epoch) and (".ckpt" in folder.name)
        ]


class ReducedCheckpointManager(AbstractCheckpointManager):

    def __init__(self, directory):
        """_summary_

        Args:
            directory (_type_): _description_
        """
        super().__init__(directory)

    def latest_epoch(self):
        """_summary_

        Returns:
            _type_: _description_
        """
        items = [item for item in self.directory.iterdir()]
        check_point_epochs = [
            i for i in range(len(items)) for folder in items if f"{i:04d}" in folder.name
        ]

        if check_point_epochs:
            return max(check_point_epochs)

        return 0

    def clean_directory(self, best_epoch: int):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """
        items = [item for item in self.directory.iterdir()]
        [
            shutil.rmtree(folder)
            for i in range(len(items))
            for folder in items
            if f"{i:04d}" in folder.name and (i != best_epoch) and (".ckpt" in folder.name)
        ]


class HistoryManager():
    """_summary_
    """

    def __init__(self, directory):
        """_summary_

        Args:
            directory (_type_): _description_
        """
        self.directory = directory
        self.history_file = Path(directory, "history.json")

    @property
    def history(self):
        self._history = load_json(self.history_file)
        return self._history

    @history.setter
    def history(self, value):
        self._history = value

    @property
    def best(self):
        """_summary_
        """
        if "val_loss" in self.history.keys():
            return min(self.history["val_loss"]), np.argmin(self.history["val_loss"]) + 1
        return None, None

    def update(self, items: dict):
        """_summary_v

        Args:
            items (dict): _description_
        """
        self._history = update_json(self.history_file, items)

    def is_finished(self):
        if "finished" in self.history.keys():
            return True
        return False

    def finished(self):
        """_summary_
        """
        self.update({'finished': True})

### FILE: .\src\settings.py ###
import json
import os
import bisect
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
from utils.IO import *

load_dotenv(verbose=False)

__all__ = [
    'TASK_NAMES', 'DATASET_SETTINGS', 'DECOMP_SETTINGS', 'LOS_SETTINGS', 'PHENOT_SETTINGS',
    'IHM_SETTINGS', 'TEXT_METRICS'
]

TASK_NAMES = ["DECOMP", "LOS", "PHENO", "IHM"]
TEXT_METRICS = ["classification_report", "confusion_matrix"]

with Path(os.getenv("CONFIG"), "datasets.json").open() as file:
    DATASET_SETTINGS = json.load(file)
    DECOMP_SETTINGS = DATASET_SETTINGS["DECOMP"]
    LOS_SETTINGS = DATASET_SETTINGS["LOS"]
    PHENOT_SETTINGS = DATASET_SETTINGS["PHENO"]
    IHM_SETTINGS = DATASET_SETTINGS["IHM"]


### FILE: .\src\storable.py ###
"""Provides a storable dataclass with some custom dictionary operations for trackers.
Attempted thread safety but didn't work out too well. Use an external lock. 
Designed to work multiple processes.
"""
import shelve
import multiprocess as mp
from copy import deepcopy
from pathlib import Path
from typing import Any


class SimpleProperty(object):
    """
    A simple descriptor class for storing and retrieving property values.
    """

    def __init__(self, name: str, default: Any = 0):
        self._name = name
        self._default = default

    def __get__(self, instance, owner) -> Any:
        return instance._progress.get(self._name, self._default)

    def __set__(self, instance, value: Any):
        instance._progress[self._name] = value
        instance._write({self._name: value})


class FloatPropert(SimpleProperty):
    """
    A descriptor class for storing and retrieving float property values.
    """

    def __init__(self, name: str, default: float):
        super().__init__(name, default)

    def __get__(self, instance, owner) -> float:
        return super().__get__(instance, owner)

    def __set__(self, instance, value: float):
        super().__set__(instance, value)


class BoolProperty(SimpleProperty):
    """
    A descriptor class for storing and retrieving bool property values.
    """

    def __init__(self, name: str, default: bool):
        super().__init__(name, default)

    def __get__(self, instance, owner) -> bool:
        return super().__get__(instance, owner)

    def __set__(self, instance, value: bool):
        super().__set__(instance, value)


class IntProperty(SimpleProperty):
    """
    A descriptor class for storing and retrieving int property values.
    """

    def __init__(self, name: str, default: int):
        super().__init__(name, default)

    def __get__(self, instance, owner) -> int:
        return super().__get__(instance, owner)

    def __set__(self, instance, value: int):
        super().__set__(instance, value)


class ProxyDictionary(dict):
    """
    A dictionary subclass that allows for callback on modification.
    """

    def __init__(self,
                 name: str,
                 default: dict = None,
                 store_total: bool = False,
                 write_callback=None,
                 *args,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self._name = name
        self._store_total = store_total
        self._on_modified = write_callback
        if store_total:
            if "total" not in default:
                # If the dict is created from scratch, total is created in the first update
                # After that it is assumed that total is correctly updated
                self._is_initial = False
                self["total"] = 0
            else:
                self._is_initial = True

            if not self._is_initial:
                self._on_modified(self._name, self, *args, **kwargs)
        self.update(default)
        self._is_initial = False
        return

    def _update_total(self, key, value):
        if self._store_total:
            if isinstance(value, dict):
                if not self._is_initial:
                    if key not in self or isinstance(self[key], (int, float)):
                        if len(value):
                            orig_value = self.get(key, 0)
                            super().__setitem__(key, dict())
                            self[key]["total"] = sum(
                                [stay_data for stay, stay_data in value.items() if stay != "total"])
                            self["total"] += self[key]["total"] - orig_value
                    elif len(value):
                        length = self[key].get("total", 0)
                        new_length = sum([
                            stay_data for stay, stay_data in self[key].items() if stay != "total"
                        ]) + length
                        self[key]["total"] = new_length
                        if not self._is_initial:
                            self["total"] += new_length - length

            elif isinstance(value, (int, float)):
                if not (key == "total" or self._is_initial):
                    orig_value = self.get(key, 0)
                    self["total"] += value - orig_value

    def __iadd__(self, other, *args, **kwargs):
        if set(other.keys()) - set(self.keys()):
            raise ValueError(
                f"Keys in the dictionary must match to __iadd__! stored_keys: {list(self.keys())}; added_keys: {list(other.keys())}"
            )
        for key, value in other.items():
            self._update_total(key, value)
        for key in other.keys():
            self[key] += other[key]
        if self._on_modified:
            self._on_modified(self._name, self, *args, **kwargs)
        return self

    def __setitem__(self, key, value):
        self._update_total(key, value)
        if self._store_total and isinstance(value, dict) and key in self:
            self[key].update(value)
        else:
            super().__setitem__(key, value)
        if self._on_modified:
            self._on_modified(self._name, self)
        return

    def __delitem__(self, key):
        super().__delitem__(key)
        if self._on_modified:
            self._on_modified(self._name, self)

    def update(self, other, *args, **kwargs):
        """
        Update the dictionary with the values from another dictionary.

        Args:
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.
        """
        for key, value in other.items():
            if key in self and isinstance(self[key], dict) and isinstance(value, dict):
                self._update_total(key, value)
                self._recursive_update(self[key], value)
            else:
                self[key] = value

        if self._on_modified:
            self._on_modified(self._name, self)
        return

    def _recursive_update(self, dictionary, update):
        """
        Recursively update a dictionary with the values from another dictionary.

        Args:
            d (dict): The dictionary to update.
            u (dict): The dictionary with the new values.
        """
        for key, value in update.items():
            if isinstance(dictionary.get(key), dict) and isinstance(update[key], dict):
                self._update_total(key, value)
                self._recursive_update(dictionary[key], update[key])
            else:
                dictionary[key] = update[key]


class ProxyList(list):
    """
    A list subclass that allows for callback on modification.
    """

    def __init__(self, name, initial=None, write_callback=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._name = name
        if initial is None:
            initial = []
        self._on_modified = write_callback
        self.extend(initial)

    def append(self, item):
        super().append(item)
        if self._on_modified:
            self._on_modified(self._name, self)

    def extend(self, items):
        super().extend(items)
        if self._on_modified:
            self._on_modified(self._name, self)

    def insert(self, index, item):
        super().insert(index, item)
        if self._on_modified:
            self._on_modified(self._name, self)

    def remove(self, item):
        super().remove(item)
        if self._on_modified:
            self._on_modified(self._name, self)

    def pop(self, index=-1):
        item = super().pop(index)
        if self._on_modified:
            self._on_modified(self._name, self)
        return item

    def clear(self):
        super().clear()
        if self._on_modified:
            self._on_modified(self._name, self)

    def __setitem__(self, index, value):
        super().__setitem__(index, value)
        if self._on_modified:
            self._on_modified(self._name, self)

    def __delitem__(self, index):
        super().__delitem__(index)
        if self._on_modified:
            self._on_modified(self._name, self)


class ListProperty:
    """
    A descriptor class for storing and retrieving list property values.
    """

    def __init__(self, name: str, default=None, write_callback=None):
        self._name = name
        self._write_callback = write_callback
        self._default = ProxyList(name=name, initial=default, write_callback=write_callback)

    def __get__(self, instance, owner):
        if instance is not None:
            if self._name not in instance.__dict__:
                instance.__dict__[self._name] = ProxyList(
                    name=self._name,
                    initial=self._default,
                    write_callback=lambda n, x: self.__set__(instance, x))
            return instance.__dict__[self._name]
        else:
            return self._default

    def __set__(self, instance, value):
        if isinstance(value, list):
            value = ProxyList(name=self._name, initial=value, write_callback=self._write_callback)
        instance.__dict__[self._name] = value
        if self._write_callback:
            self._write_callback(self._name, value)


class DictionaryProperty(object):
    """
    A descriptor class for storing and retrieving dictionary property values.
    """

    def __init__(self, name: str, default: dict = {}, store_total=False, write_callback=None):
        self._name = name
        self._store_total = store_total
        self._write_callback = (write_callback if store_total else None)
        self._default = ProxyDictionary(self._name,
                                        default,
                                        write_callback=write_callback,
                                        store_total=self._store_total)

    def __get__(self, instance, owner) -> dict:
        if instance is not None:
            instance._progress[self._name] = instance._read(self._name)
            return ProxyDictionary(self._name,
                                   instance._progress.get(self._name, self._default),
                                   write_callback=lambda n, x: self.__set__(instance, x),
                                   store_total=self._store_total)
        elif owner is not None:
            return deepcopy(owner._originals.get(self._name, self._default))

    def __set__(self, instance, value: dict):
        if isinstance(value, ProxyDictionary):
            value = dict(value)
        elif self._store_total:
            value = dict(
                ProxyDictionary(self._name,
                                value,
                                store_total=True,
                                write_callback=self._write_callback))
        instance._progress[self._name] = value
        instance._write({self._name: value})


def storable(cls):
    """
    A class decorator that adds persistence functionality to a class.

    Args:
        cls: The class to decorate.

    Returns:
        The decorated class.
    """
    # When you think about it this is highly illegal but the only way of keeping them cls attributes unchanged between instantiations.
    # Find a better way if you can, you are the real hero.
    originals = {
        name: deepcopy(attr)
        for name, attr in vars(cls).items()
        if not name.startswith("_") and isinstance(attr, (int, float, bool, dict, list, type(None)))
    }
    setattr(cls, '_originals', originals)
    original_init = cls.__init__

    def __init__(self, *args, **kwargs):
        """
        Initialize the storable class.

        Args:
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.
        """
        self._lock = mp.Lock()
        self._progress = {}
        for name, original_value in cls._originals.items():
            setattr(cls, name, deepcopy(original_value))

        if "storage_path" in kwargs:
            self._path = Path(kwargs.pop('storage_path'))
        elif args:
            self._path = Path(args[0])
            args = tuple(args[1:])
        else:
            raise ValueError(
                "No storage path provided to storable. Either provide as first positional argument or with the storage_path keyword argument."
            )

        self._path.parent.mkdir(exist_ok=True, parents=True)

        self._save_frequency = kwargs.pop('save_frequency', 1)
        self._access_count = 0

        # Load or initialize progress
        if Path(self._path.parent, f"{self._path.name}.dat").is_file():
            self._progress = self._read()
            self._wrap_attributes()
        else:
            self._wrap_attributes()
            self._write(self._progress)

        original_init(self, *args, **kwargs)

    def _write(self, items: dict):
        """
        Write the current state of the progress to the file.

        Args:
            items (dict): The dictionary containing the progress items.
        """
        if self._lock is not None:
            self._lock.acquire()
        self._access_count += 1
        with shelve.open(str(self._path)) as db:
            for key, value in items.items():
                # Make sure no Property types are written back
                if isinstance(value, ProxyDictionary):
                    db[key] = dict(value)
                elif isinstance(value, ProxyList):
                    db[key] = list(value)
                else:
                    db[key] = value
        if self._lock is not None:
            self._lock.release()

    def _read(self, key=None):
        """
        Read the progress from the file.

        Returns:
            dict: The progress dictionary.
        """
        if self._lock is not None:
            self._lock.acquire()

        with shelve.open(str(self._path)) as db:

            def _read_value(key):
                value = db[key]
                default_value = getattr(cls, key)
                # These types are miscast by shelve
                if isinstance(default_value, bool):
                    return bool(value)
                elif isinstance(default_value, int):
                    return int(value)
                else:
                    return value

            if key is None:
                ret = dict()
                for key in db.keys():
                    ret[key] = _read_value(key)
            else:
                ret = _read_value(key)

        if self._lock is not None:
            self._lock.release()

        return ret

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._wrap_attributes()

    cls.__setstate__ = __setstate__
    cls.__init__ = __init__
    cls._write = _write
    cls._read = _read

    # Attribute wrapping logic
    def _wrap_attributes(self):
        """
        Wrap the attributes of the class with the appropriate descriptors.
        """

        for name, attr in vars(cls).items():
            if (isinstance(attr, (int, float, bool, dict, list)) or
                    attr is None) and not name.startswith("_"):
                attr = self._progress.get(name, attr)

                def _write_callback(name, value):
                    # Read before you _write like github
                    self._progress[name] = value
                    self._write({name: value})

                if isinstance(attr, dict):
                    # Store total is a mess but necessary for preprocessing trackers
                    store_total = getattr(cls, "_store_total", False)
                    setattr(
                        cls, name,
                        DictionaryProperty(name, attr, store_total, write_callback=_write_callback))
                elif isinstance(attr, list):
                    setattr(cls, name, ListProperty(name, attr, write_callback=_write_callback))
                elif isinstance(attr, bool):
                    setattr(cls, name, BoolProperty(name, attr))
                elif isinstance(attr, float):
                    setattr(cls, name, FloatPropert(name, attr))
                else:
                    setattr(cls, name, IntProperty(name, attr))
                self._progress[name] = attr

    cls._wrap_attributes = _wrap_attributes

    return cls


### FILE: .\src\visualization.py ###
import matplotlib.pyplot as plt
import os
import numpy as np
import pandas as pd
import pdb
import seaborn as sn
import tensorflow as tf
from pathlib import Path
from utils import make_prediction_vector


def plot_fourier_transform(series, years):
    """_summary_

    Args:
        series (_type_): _description_
    """
    fast_fourier_transform = tf.signal.rfft(series)
    f_per_dataset = np.arange(0, len(fast_fourier_transform))

    f_per_year = f_per_dataset / years
    print(max(np.abs(fast_fourier_transform)))
    # pdb.set_trace()
    plt.step(f_per_year, np.abs(fast_fourier_transform))
    plt.xscale('log')
    plt.ylim(0, max(np.abs(fast_fourier_transform)) / 2)
    plt.xlim([0.1, max(plt.xlim())])
    plt.xticks([1, 12, 52.2, 365.2524, 365.2524 * 2, 365.2524 * 4, 365.2524 * 24],
               labels=['1/Year', '1/Month', '1/Week', '1/day', '1/12', '1/6', '1/hour'])
    _ = plt.xlabel('Frequency (log scale)')
    plt.title("Fast Fourier Transform")


def _subplot_generator(data_df, name, features, layout, type, save=False):
    """
    This function generates subplots of the specified type with the specified parameters.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
        type:       plot type
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """

    if layout[0] * layout[1] < len(features):
        print(
            f"Layout not valid, there are {len(features)} columns within the data frame and only {layout[0] * layout[1]} subplots!"
        )
        return

    if not name:
        name = "Subplots"
    if layout[1] > len(features):
        ncols = len(features)
    else:
        ncols = layout[1]
    nrows = int(np.ceil(len(features) / layout[1]))
    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 7, nrows * 3.6))
    for index, feature in enumerate(features):
        rows = int(np.floor(index / layout[1]))
        cols = index % layout[1]
        if nrows == ncols == 1:
            axs.plot(data_df[feature])
            axs.set_title(feature)
        elif nrows == 1:
            getattr(axs[cols], type)(data_df[feature])
            axs[cols].set_title(feature)
        else:
            getattr(axs[rows, cols], type)(data_df[feature])
            axs[rows, cols].set_title(feature)

    if save:
        fig.savefig(Path(plot_location, f"{name}.png"), dpi=200)

    return fig, axs


def plot(data_df, name="", features=None, subplots=True, layout=(27, 2), save=False):
    """
    Plots against index.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """
    plt.clf()
    # implement something to adjust layout ratio if fewer features where passed
    if isinstance(data_df, pd.core.series.Series):
        features = [data_df.name]
        is_series = True
        subplots = False
    elif not features:
        features = data_df.columns

    if subplots:
        return _subplot_generator(data_df, name, features, layout, 'plot')

    else:
        for feature in features:
            plt.clf()
            if is_series:
                time_serie = data_df
            else:
                time_serie = data_df[feature].copy()
            time_serie.plot(figsize=(15, 10))
            if save:
                plt.savefig(Path(plot_location, f"{name}_{feature}.png"), dpi=200)

    return plt.gcf()


def acorr(data_df, name="", features=None, subplots=True, layout=(27, 2), maxlags=100, save=False):
    """
    Autocorrelation plots.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """

    plt.clf()

    if subplots:
        if not features:
            features = data_df.columns
        return _subplot_generator(data_df, name, features, layout, 'acorr')
    else:
        if not features:
            features = data_df.columns

        for feature in features:
            plt.acorr(data_df[feature], maxlags=maxlags)

            if save:
                plt.savefig(Path(plot_location, f"{feature}.png"), dpi=200)

            plt.clf()

        return None


def hist(data_df, name="", features=None, subplots=True, layout=(27, 2), save=False):
    """
    Histogram plots.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """
    plt.clf()
    if not features:
        features = data_df.columns
    if subplots:
        return _subplot_generator(data_df, name, features, layout, 'hist')
    else:
        if not features:
            features = data_df.columns

        for feature in features:
            plt.hist(data_df[feature])
            if save:
                plt.savefig(Path(plot_location, f"{feature}.png"), dpi=200)
            plt.clf()

        return None


def corrMatrix(data_df, features=None):
    """
    Correlation plots.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        features:   columns from the frame which are to be plotted
    
    Returns:
        plt:    matplotlib generated pyplot obj
    """
    plt.clf()
    if not features:
        features = data_df.columns

    data = data_df[features]
    corrMatrix = data.corr()
    size = (np.max([0.4 * len(features), 5]), np.max([0.4 * len(features), 5]))
    fig, axs = plt.subplots(1, 1, figsize=size)
    axs = sn.heatmap(corrMatrix, annot=False)
    plt.subplots_adjust(bottom=0.35)
    plt.subplots_adjust(left=0.45)
    plt.savefig(Path(plot_location, "corrMatrix.png"), dpi=200)
    return plt.gcf()


def make_sample_plot(model, generator, folder=None, batches=20, title="", bin_averages=None):
    """
    """
    y_pred, y_true = make_prediction_vector(model=model,
                                            generator=generator,
                                            batches=batches,
                                            bin_averages=bin_averages)

    fig, ax = plt.subplots()
    pd.DataFrame(y_true, columns=['y_true']).plot(ax=ax, ylabel="load")
    pd.DataFrame(y_pred.reshape(-1, 1), columns=['y_pred']).plot(color="r", ax=ax)

    if title:
        plt.title(title)

    if folder:
        plt.savefig(Path(folder, f"{title}_sample.png"))

    return fig


def make_sample_subplot(model,
                        generators,
                        titles=None,
                        folder=None,
                        batches=20,
                        title="",
                        bin_averages=None,
                        layout=None):
    """
    """
    if not isinstance(generators, list):
        generators = [generators]

    if not layout:
        layout = (len(generators), 1)

    if not titles:
        titles = [None] * len(generators)

    fig, ax = plt.subplots(*layout)
    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)

    for index, (generator, title) in enumerate(zip(generators, titles)):
        cur_ax = get_ax(ax, index, layout)

        y_pred, y_true = make_prediction_vector(model=model,
                                                generator=generator,
                                                batches=batches,
                                                bin_averages=bin_averages)

        pd.DataFrame(y_true.reshape(-1, 1), columns=['y_true']).plot(ax=cur_ax, ylabel="load")
        pd.DataFrame(y_pred.reshape(-1, 1), columns=['y_pred']).plot(color="r", ax=cur_ax)

        if title:
            cur_ax.set_title(title)

    if folder:
        plt.savefig(Path(folder, f"{title}_sample.png"))

    return fig


def get_ax(ax, index, layout):
    """_summary_

    Args:
        ax (_type_): _description_
        index (_type_): _description_
        layout (_type_): _description_
    """
    if layout[1] == 1:
        rows = int(np.floor(index / layout[1]))
        return ax[rows]
    else:
        rows = int(np.floor(index / layout[1]))
        cols = index % layout[1]
        return ax[rows, cols]


def make_history_plot(history, folder=None, title=None, train_key="loss", val_key="val_loss"):
    """
    """
    fig, ax = plt.subplots()
    pd.DataFrame(history[train_key], columns=[train_key]).plot(ax=ax, ylabel="y_true")
    if val_key in history.keys():
        pd.DataFrame(history[val_key], columns=[val_key]).plot(color="r", ax=ax, ylabel="y_pred")

    if title:
        plt.title(title)

    if folder:
        plt.savefig(Path(folder, "loss.png"))

    return plt


def make_history_plots(history, train_keys, val_keys, folder=None, titles=None):
    """
    """
    layout = (len(train_keys), 1)

    fig, ax = plt.subplots(*layout)
    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)

    if not titles:
        titles = [None] * len(train_keys)

    for index, (train_key, val_key, title) in enumerate(zip(train_keys, val_keys, titles)):
        cur_ax = get_ax(ax, index, layout)
        pd.DataFrame(history[train_key], columns=[train_key]).plot(ax=cur_ax, ylabel="y_true")
        pd.DataFrame(history[val_key], columns=[val_key]).plot(color="r",
                                                               ax=cur_ax,
                                                               ylabel="y_pred")

        if title:
            cur_ax.set_title(title)

    if folder:
        plt.savefig(Path(folder, "loss.png"))

    return fig


### FILE: .\src\datasets\discretizing.py ###
import random
import os
import pandas as pd
from copy import deepcopy
from itertools import chain
from typing import Dict
from pathlib import Path
from utils.IO import *
from utils import dict_subset
from pathos.multiprocessing import cpu_count, Pool
from preprocessing.discretizers import MIMICDiscretizer
from .trackers import PreprocessingTracker
from .readers import ExtractedSetReader, ProcessedSetReader
from .mimic_utils import copy_subject_info

__all__ = ["compact_discretization", "iterative_discretization"]


def compact_discretization(X_subject: Dict[str, Dict[str, pd.DataFrame]],
                           y_subject: Dict[str, Dict[str, pd.DataFrame]],
                           task: str,
                           subject_ids: list = None,
                           num_subjects: int = None,
                           time_step_size: float = 1.0,
                           impute_strategy: str = "previous",
                           mode: str = "legacy",
                           start_at_zero: bool = True,
                           eps: float = 1e-6,
                           storage_path: Path = None,
                           source_path: Path = None) -> Dict[str, Dict[str, pd.DataFrame]]:
    """_summary_

    Args:
        timeseries (pd.DataFrame): _description_
        episodic_data (pd.DataFrame): _description_
        subject_diagnoses (pd.DataFrame): _description_
        subject_icu_history (pd.DataFrame): _description_

    Returns:
        _type_: _description_
    """

    tracker = PreprocessingTracker(storage_path=Path(storage_path, "progress"),
                                   num_subjects=num_subjects,
                                   subject_ids=subject_ids,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode)

    copy_subject_info(source_path, storage_path)

    if tracker.is_finished:
        info_io(f"Compact discretization finalized in directory:\n{str(storage_path)}")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(root_path=storage_path, subject_ids=subject_ids,
                                  set_index=False).read_samples(read_ids=True)

    info_io(f"Compact Discretization: {task}", level=0)
    discretizer = MIMICDiscretizer(task=task,
                                   storage_path=storage_path,
                                   tracker=tracker,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode,
                                   eps=eps,
                                   verbose=True)

    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=X_subject.keys())
    missing_subjects = 0
    if num_subjects is not None:
        X_discretized = dict()
        y_discretized = dict()
        while not len(X_discretized) == num_subjects:
            curr_X_subject = dict_subset(X_subject, subject_ids)
            curr_y_subject = dict_subset(y_subject, subject_ids)

            X, y = discretizer.transform(curr_X_subject, curr_y_subject)
            X_discretized.update(X)
            y_discretized.update(y)
            it_missing_subjects = set(X.keys()) - set(subject_ids)
            subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects -
                                                                len(X_discretized),
                                                                subject_ids=None,
                                                                all_subjects=excluded_subject_ids)
            if it_missing_subjects:
                missing_subjects += len(it_missing_subjects)
                debug_io(f"Missing subjects are: {*it_missing_subjects,}")
            if not subject_ids:
                break
            if len(X_discretized) == num_subjects:
                debug_io(f"Missing {len(X_discretized) - num_subjects} subjects.")
                debug_io(f"Unprocessable subjects are: {*it_missing_subjects,}")

    else:
        X_subject = dict_subset(X_subject, subject_ids)
        y_subject = dict_subset(y_subject, subject_ids)

        (X_discretized, y_discretized) = discretizer.transform(X_subject, y_subject)
    if storage_path is not None:
        discretizer.save_data()
        info_io(f"Finalized discretization for {task} in directory:\n{str(storage_path)}")
    else:
        info_io(f"Finalized discretization for {task}.")
    # TODO! this doesn't work, reimagine
    # if missing_subjects:
    #     warn_io(f"The subject target was not reached, missing {missing_subjects} subjects.")
    tracker.is_finished = True
    return {"X": X_discretized, "y": y_discretized}


def iterative_discretization(reader: ExtractedSetReader,
                             task: str,
                             subject_ids: list = None,
                             num_subjects: int = None,
                             time_step_size: float = 1.0,
                             impute_strategy: str = "previous",
                             mode: str = "legacy",
                             start_at_zero: bool = True,
                             eps: float = 1e-6,
                             storage_path: Path = None) -> ProcessedSetReader:
    """_summary_

    Args:
        timeseries (pd.DataFrame): _description_
        episodic_data (pd.DataFrame): _description_
        subject_diagnoses (pd.DataFrame): _description_
        subject_icu_history (pd.DataFrame): _description_

    Returns:
        _type_: _description_
    """
    original_subject_ids = deepcopy(subject_ids)
    tracker = PreprocessingTracker(storage_path=Path(storage_path, "progress"),
                                   num_subjects=num_subjects,
                                   subject_ids=subject_ids,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode)

    copy_subject_info(reader.root_path, storage_path)

    discretizer = MIMICDiscretizer(reader=reader,
                                   task=task,
                                   storage_path=storage_path,
                                   tracker=tracker,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode,
                                   eps=eps,
                                   verbose=False)

    if tracker.is_finished:
        info_io(f"Data discretization for {task} is already in directory:\n{str(storage_path)}.")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(storage_path, subject_ids=subject_ids, set_index=False)

    info_io(f"Iterative Discretization: {task}", level=0)
    info_io(f"Discretizing for task {task}.")

    # Tracking info
    n_discretizer_subjects = len(tracker.subject_ids)
    n_discretizer_stays = len(tracker.stay_ids)
    n_discretizer_samples = tracker.samples

    # Parallel processing logic
    # discretizer_pr = discretizer

    def discretize_subject(subject_id: str):
        """_summary_
        """
        _, tracking_infos = discretizer_pr.transform_subject(subject_id)

        if tracking_infos:
            discretizer_pr.save_data([subject_id])
            return subject_id, tracking_infos

        return subject_id, None

    def init(discretizer: MIMICDiscretizer):
        global discretizer_pr
        discretizer_pr = discretizer

    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=discretizer.subjects,
                                                        discretizer_subjects=tracker.subject_ids)
    # for subject_id in subject_ids:
    #     discretize_subject(subject_id)
    info_io(f"Discretizing task data:\n"
            f"Discretize subjects: {n_discretizer_subjects}\n"
            f"Discretize stays: {n_discretizer_stays}\n"
            f"Discretize samples: {n_discretizer_samples}\n"
            f"Skipped subjects: {0}")

    # Start the run
    with Pool(cpu_count() - 1, initializer=init, initargs=(discretizer,)) as pool:
        res = pool.imap_unordered(discretize_subject, subject_ids, chunksize=500)

        empty_subjects = 0
        missing_subjects = 0
        while True:
            try:
                subject_id, tracker_data = next(res)
                if tracker_data is None:
                    empty_subjects += 1
                    # Add new samples if to meet the num subjects target
                    if num_subjects is None:
                        continue
                    debug_io(f"Missing subject is: {subject_id}")
                    try:
                        subj = excluded_subject_ids.pop()
                        res = chain(res, [pool.apply_async(discretize_subject, args=(subj,)).get()])
                    except IndexError:
                        missing_subjects += 1
                        debug_io(
                            f"Could not replace missing subject. Excluded subjects is: {excluded_subject_ids}"
                        )
                else:
                    n_discretizer_subjects += 1
                    n_discretizer_stays += len(tracker_data) - 1
                    n_discretizer_samples += tracker_data["total"]

                info_io(
                    f"Discretizing timeseries data:\n"
                    f"Discretized subjects: {n_discretizer_subjects}\n"
                    f"Discretized stays: {n_discretizer_stays}\n"
                    f"Discretized samples: {n_discretizer_samples}\n"
                    f"Skipped subjects: {empty_subjects}",
                    flush_block=(True and not int(os.getenv("DEBUG", 0))))
            except StopIteration as e:
                tracker.is_finished = True
                info_io(f"Finalized for task {task} in directory:\n{str(storage_path)}")
                if num_subjects is not None and missing_subjects:
                    warn_io(
                        f"The subject target was not reached, missing {missing_subjects} subjects.")
                break

    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return ProcessedSetReader(storage_path, subject_ids=original_subject_ids, set_index=False)


def get_subject_ids(num_subjects: int,
                    subject_ids: list,
                    all_subjects: list,
                    discretizer_subjects: list = list()):
    remaining_subject_ids = list(set(all_subjects) - set(discretizer_subjects))
    # Select subjects to process logic
    n_processed_subjects = len(discretizer_subjects)
    if num_subjects is not None:
        num_subjects = max(num_subjects - n_processed_subjects, 0)
        selected_subjects_ids = random.sample(remaining_subject_ids, k=num_subjects)
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
        random.shuffle(remaining_subject_ids)
    elif subject_ids is not None:
        unknown_subjects = set(subject_ids) - set(all_subjects)
        if unknown_subjects:
            warn_io(f"Unknown subjects: {*unknown_subjects,}")
        selected_subjects_ids = list(set(subject_ids) & set(all_subjects))
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
    else:
        selected_subjects_ids = remaining_subject_ids
    return selected_subjects_ids, remaining_subject_ids


### FILE: .\src\datasets\feature_engineering.py ###
import os
import random
import numpy as np
import pandas as pd
from copy import deepcopy
from typing import Dict, List
from pathlib import Path
from itertools import chain
from pathos.multiprocessing import cpu_count, Pool
from pathos.helpers import mp
from utils.IO import *
from utils import dict_subset
from preprocessing.feature_engines import MIMICFeatureEngine
from .trackers import PreprocessingTracker
from .readers import ProcessedSetReader
from .mimic_utils import copy_subject_info

__all__ = ["iterative_fengineering", "compact_fengineering"]


def compact_fengineering(X_subjects: Dict[str, Dict[str, pd.DataFrame]],
                         y_subjects: Dict[str, Dict[str, pd.DataFrame]],
                         task: str,
                         storage_path=None,
                         source_path=None,
                         subject_ids=None,
                         num_subjects=None) -> Dict[str, Dict[str, pd.DataFrame]]:
    """_summary_

    Args:
        X_subjects (_type_): _description_
        y_subjects (_type_): _description_

    Returns:
        _type_: _description_
    """
    tracker = PreprocessingTracker(num_subjects=num_subjects,
                                   subject_ids=subject_ids,
                                   storage_path=Path(storage_path, "progress"))
    copy_subject_info(source_path, storage_path)

    if tracker.is_finished:
        info_io(f"Compact feature engineering already finalized in directory:\n{str(storage_path)}")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(root_path=storage_path,
                                  subject_ids=subject_ids).read_samples(read_ids=True)

    info_io(f"Compact Feature Engineering: {task}", level=0)
    engine = MIMICFeatureEngine(config_dict=Path(os.getenv("CONFIG"), "engineering_config.json"),
                                storage_path=storage_path,
                                task=task,
                                tracker=tracker,
                                verbose=True)

    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=X_subjects.keys())

    X_subjects = dict_subset(X_subjects, subject_ids)
    y_subjects = dict_subset(y_subjects, subject_ids)


    X_processed, \
    y_processed, \
    _ = engine.transform(X_subjects, y_subjects) # Omitting timestamps

    if storage_path:
        engine.save_data()
        info_io(f"Finalized feature engineering for {task} in directory:\n{str(storage_path)}")
    else:
        info_io(f"Finalized feature engineering for {task}.")
    tracker.is_finished = True
    return {"X": X_processed, "y": y_processed}


def iterative_fengineering(reader: ProcessedSetReader,
                           task: str,
                           storage_path: Path,
                           subject_ids: List[int] = None,
                           num_subjects: int = None):
    """_summary_
    """
    original_subject_ids = deepcopy(subject_ids)
    tracker = PreprocessingTracker(storage_path=Path(storage_path, "progress"),
                                   num_subjects=num_subjects,
                                   subject_ids=subject_ids)

    copy_subject_info(reader.root_path, storage_path)

    engine = MIMICFeatureEngine(reader=reader,
                                config_dict=Path(os.getenv("CONFIG"), "engineering_config.json"),
                                storage_path=storage_path,
                                task=task,
                                tracker=tracker)

    if tracker.is_finished:
        info_io(f"Data engineering for {task} is already in directory:\n{str(storage_path)}.")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(storage_path, subject_ids=subject_ids)

    info_io(f"Iterative Feature Engineering: {task}", level=0)
    info_io(f"Engineering data and saving at:\n{storage_path}.")

    # Tracking info
    n_engineered_subjects = len(tracker.subject_ids)
    n_engineered_stays = len(tracker.stay_ids)
    n_engineered_samples = tracker.samples

    def engineer_subject(subject_id: int):
        """"""

        _, tracking_infos = engine_pr.transform_subject(subject_id)

        if tracking_infos is not None:
            engine_pr.save_data([subject_id])
            return subject_id, tracking_infos

        return subject_id, None

    def init(engine: MIMICFeatureEngine):
        global engine_pr
        engine_pr = engine

    # Select subjects to process logic
    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=engine.subjects,
                                                        engineered_subjects=tracker.subject_ids)

    info_io(f"Engineering processed data:\n"
            f"Engineered subjects: {n_engineered_subjects}\n"
            f"Engineered stays: {n_engineered_stays}\n"
            f"Engineered samples: {n_engineered_samples}")
    with Pool(cpu_count() - 1, initializer=init, initargs=(engine,)) as pool:
        chunksize = min(1000, int(np.ceil(len(subject_ids) / (cpu_count() - 1))) + 1)
        res = pool.imap_unordered(engineer_subject, subject_ids, chunksize=chunksize)

        empty_subjects = 0
        missing_subjects = 0
        while True:
            try:
                subject_id, tracker_data = next(res)
                if tracker_data is None:
                    empty_subjects += 1
                    # Add new samples if to meet the num subjects target
                    if num_subjects is None:
                        continue
                    try:
                        subj = excluded_subject_ids.pop()
                        res = chain(res, [pool.apply_async(engineer_subject, args=(subj,)).get()])
                    except IndexError:
                        debug_io(f"Missing subject is: {subject_id}")
                        missing_subjects += 1
                else:
                    n_engineered_subjects += 1
                    n_engineered_stays += len(tracker_data) - 1
                    n_engineered_samples += tracker_data["total"]

                info_io(
                    f"Engineering processed data:\n"
                    f"Engineered subjects: {n_engineered_subjects}\n"
                    f"Engineered stays: {n_engineered_stays}\n"
                    f"Engineered samples: {n_engineered_samples}\n"
                    f"Skipped subjects: {empty_subjects}",
                    flush_block=True)
            except StopIteration:
                tracker.is_finished = True
                info_io(
                    f"Finalized feature engineering for {task} in directory:\n{str(storage_path)}")
                if num_subjects is not None and missing_subjects:
                    info_io(
                        f"The subject target was not reached, missing {missing_subjects} subjects.")
                break
    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return ProcessedSetReader(storage_path, original_subject_ids)


def get_subject_ids(num_subjects: int,
                    subject_ids: list,
                    all_subjects: list,
                    engineered_subjects: list = list()):
    """_summary_

    Args:
        num_subjects (_type_): _description_
        subject_ids (_type_): _description_
        all_subjects (_type_): _description_

    Returns:
        _type_: _description_
    """
    remaining_subject_ids = list(set(all_subjects) - set(engineered_subjects))
    n_engineered_subjects = len(engineered_subjects)
    if num_subjects is not None:
        num_subjects = max(num_subjects - n_engineered_subjects, 0)
        selected_subjects_ids = random.sample(remaining_subject_ids, k=num_subjects)
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
        random.shuffle(remaining_subject_ids)
    elif subject_ids is not None:
        unknown_subjects = set(subject_ids) - set(all_subjects)
        if unknown_subjects:
            warn_io(f"Unknown subjects: {*unknown_subjects,}")
        selected_subjects_ids = list(set(subject_ids) & set(all_subjects))
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
    else:
        selected_subjects_ids = remaining_subject_ids
    return selected_subjects_ids, remaining_subject_ids


### FILE: .\src\datasets\mimic_utils.py ###
"""Dataset file

This file allows access to the dataset as specified.

Todo:


YerevaNN/mimic3-benchmarks
"""
import numpy as np
import pandas as pd
import os
import re
import shutil
from pathlib import Path
from typing import Dict
from utils.IO import *
from settings import *


def copy_subject_info(source_path: Path, storage_path: Path):
    """
    Copy subject information from source path to storage path
    """
    if source_path is None:
        if storage_path is not None:
            warn_io("No source path provided for subject information. Skipping copy.")
        return
    if not storage_path.is_dir():
        storage_path.mkdir(parents=True, exist_ok=True)
    source_file = Path(source_path, "subject_info.csv")
    target_file = Path(storage_path, "subject_info.csv")
    shutil.copy(str(source_file), str(target_file))


def get_samples_per_df(event_frames: Dict[str, pd.DataFrame], num_samples: int):
    """_summary_

    Args:
        event_frames (Dict[str, pd.DataFrame]): _description_
        num_samples (int): _description_
    """
    total_length = sum(len(df) for df in event_frames.values())
    samples_per_df = {
        event_types: int((len(df) / total_length) * num_samples)
        for event_types, df in event_frames.items()
    }

    # Adjust for rounding errors if necessary (simple method shown here)
    samples_adjusted = num_samples - sum(samples_per_df.values())
    for name in samples_per_df:
        if samples_adjusted <= 0:
            break
        samples_per_df[name] += 1
        samples_adjusted -= 1

    sampled_dfs = {
        event_types: event_frames[event_types][event_frames[event_types]["CHARTTIME"].isin(
            event_frames[event_types]["CHARTTIME"].unique()[:samples])]
        if len(event_frames[event_types]) >= samples else event_frames[event_types]
        for event_types, samples in samples_per_df.items()
    }

    subject_events_per_df = {
        event_types: len(samples) for event_types, samples in sampled_dfs.items()
    }

    if not sum([len(frames) for frames in sampled_dfs.values()]):
        raise RuntimeError(
            "Sample limit compliance subsampling produced empty dataframe. Source code is erroneous!"
        )

    return sampled_dfs, subject_events_per_df, samples_per_df


def convert_dtype_value(value, dtype: str):
    dtype_mapping = {
        "Int8": np.int8,
        "Int16": np.int16,
        "Int32": np.int32,
        "Int64": np.int64,
        "str": str,
        "float": float,
        "float64": np.float64,
        "float32": np.float32,
        "object": lambda x: x
    }
    return dtype_mapping[dtype](value)


def convert_dtype_dict(dtypes: dict, add_lower=True) -> dict:
    """_summary_

    Args:
        dtypes (Dict[str, str]): column name to dtype maping. Dtype can be one of Int8, Int16, Int32, Int64, str, float" 

    Returns:
        dict: Returns dictionary with column name to dtype object mapping.
    """
    dtype_mapping = {
        "Int8": pd.Int8Dtype(),
        "Int16": pd.Int16Dtype(),
        "Int32": pd.Int32Dtype(),
        "Int64": pd.Int64Dtype(),
        "str": pd.StringDtype(),
        "float": float,
        "float64": pd.Float64Dtype(),
        "float32": pd.Float32Dtype(),
        "object": "object"
    }
    dtype_dict = {
        column: dtype_mapping[type_identifyer] for column, type_identifyer in dtypes.items()
    }
    if add_lower:
        dtype_dict.update({
            column.lower(): dtype_mapping[type_identifyer]
            for column, type_identifyer in dtypes.items()
        })
    return dtype_dict


def make_writeboolean(storage_path: Path) -> bool:
    """_summary_

    Args:
        storage_path (Path): _description_

    Returns:
        bool: _description_
    """
    # TODO this is because i am unable to store and load a list within a dataframe
    # solve this
    if storage_path == None:
        return True

    if storage_path.name == "PHENO":
        return True

    if not storage_path.is_dir():
        os.mkdir(storage_path)
        return True
    subject_dirs = os.listdir(storage_path)
    if not subject_dirs:
        return True
    if not "episodic_data.csv" in os.listdir(Path(storage_path, subject_dirs.pop())):
        return True
    return False


def make_episodic_data(subject_icu_history: pd.DataFrame) -> pd.DataFrame:
    """_summary_

    Args:
        subject_icu_history (pd.DataFrame): _description_

    Returns:
        pd.DataFrame: _description_
    """
    # Episodic data contains: Icustay, Age, Length of Stay, Mortality, Gender, Ethnicity, Height and Weight
    episodic_data = subject_icu_history[["ICUSTAY_ID", "AGE", "LOS",
                                         "MORTALITY"]].rename(columns={"ICUSTAY_ID": "Icustay"})

    def imputeby_map(string, map):
        """
        """
        if string in map:
            return map[string]
        return map['OTHER']

    # Impute gender
    episodic_data['GENDER'] = subject_icu_history.GENDER.fillna('').apply(
        imputeby_map, args=([DATASET_SETTINGS["gender_map"]]))

    # Impute ethnicity
    ethnicity_series = subject_icu_history.ETHNICITY.apply(
        lambda x: x.replace(' OR ', '/').split(' - ')[0].split('/')[0])
    episodic_data['ETHNICITY'] = ethnicity_series.fillna('').apply(
        imputeby_map, args=([DATASET_SETTINGS["ethnicity_map"]]))

    # Empty values
    episodic_data['Height'] = np.nan
    episodic_data['Weight'] = np.nan

    episodic_data = episodic_data.set_index('Icustay')

    return episodic_data


def make_diagnoses_util(diagnoses: pd.DataFrame) -> pd.DataFrame:
    """_summary_

    Args:
        diagnoses (pd.DataFrame): _description_

    Returns:
        pd.DataFrame: _description_
    """
    # Diagnoese from each ICU stay with diagnose code in the column and stay ID as index
    diagnoses['VALUE'] = 1
    diagnoses = diagnoses[['ICUSTAY_ID', 'ICD9_CODE', 'VALUE']].drop_duplicates()
    labels = diagnoses.pivot(index='ICUSTAY_ID', columns='ICD9_CODE', values='VALUE')
    labels = labels.fillna(0).astype(int)
    labels = labels.reindex(columns=DATASET_SETTINGS["diagnosis_labels"])
    labels = labels.fillna(0).astype(int)

    return labels


def make_timeseries_util(chartevents: pd.DataFrame, variables) -> pd.DataFrame:
    """
    """
    # Lets create the time series
    metadata = chartevents[['CHARTTIME', 'ICUSTAY_ID']]
    metadata = metadata.sort_values(by=['CHARTTIME', 'ICUSTAY_ID'])
    metadata = metadata.drop_duplicates(keep='first').set_index('CHARTTIME')

    # Timeseries contains only the following. Subject_id and personal information in episodic data
    timeseries_df = chartevents[['CHARTTIME', 'VARIABLE', 'VALUE']]
    timeseries_df = timeseries_df.sort_values(by=['CHARTTIME', 'VARIABLE', 'VALUE'], axis=0)
    timeseries_df = timeseries_df.drop_duplicates(subset=['CHARTTIME', 'VARIABLE'], keep='last')
    timeseries_df = timeseries_df.pivot(index='CHARTTIME', columns='VARIABLE', values='VALUE')
    timeseries_df = timeseries_df.merge(metadata, left_index=True, right_index=True)
    timeseries_df = timeseries_df.sort_index(axis=0).reset_index()

    timeseries_df = timeseries_df.reindex(columns=np.append(variables, ['ICUSTAY_ID', 'CHARTTIME']))

    return timeseries_df


def make_episode(timeseries_df: pd.DataFrame,
                 stay_id: int,
                 intime=None,
                 outtime=None) -> pd.DataFrame:
    """
    """
    # All events with ID
    indices = (timeseries_df.ICUSTAY_ID == stay_id)

    # Plus all events int time frame
    if intime is not None and outtime is not None:
        indices = indices | ((timeseries_df.CHARTTIME >= intime) &
                             (timeseries_df.CHARTTIME <= outtime))

    # Filter out and remove ID (ID already in episodic data)
    timeseries_df = timeseries_df.loc[indices]
    del timeseries_df['ICUSTAY_ID']

    return timeseries_df


def make_hour_index(episode_df: pd.DataFrame,
                    intime,
                    remove_charttime: bool = True) -> pd.DataFrame:
    """
    """
    # Get difference and convert to hours
    episode_df = episode_df.copy()
    episode_df['hours'] = (episode_df.CHARTTIME -
                           intime).apply(lambda s: s / np.timedelta64(1, 's'))
    episode_df['hours'] = episode_df.hours / 60. / 60

    # Set index
    episode_df = episode_df.set_index('hours').sort_index(axis=0)

    if remove_charttime:
        del episode_df['CHARTTIME']

    return episode_df


def clean_chartevents_util(chartevents: pd.DataFrame):
    """
    """
    function_switch = DATASET_SETTINGS["CHARTEVENTS"]["clean"]
    for variable_name, function_identifier in function_switch.items():
        index = (chartevents.VARIABLE == variable_name)
        try:
            chartevents.loc[index, 'VALUE'] = globals()[function_identifier](chartevents.loc[index])
        except Exception as exp:
            print("Exception in clean_events function", function_identifier, ": ", exp)
            print("number of rows:", np.sum(index))
            print("values:", chartevents.loc[index])
            raise exp

    return chartevents.loc[chartevents.VALUE.notnull()]


def get_static_value(timeseries: pd.DataFrame, variable: str):
    """
    """
    index = timeseries[variable].notnull()

    if index.any():
        loc = np.where(index)[0][0]
        return timeseries[variable].iloc[loc]

    return np.nan


def upper_case_column_names(frame: pd.DataFrame) -> pd.DataFrame:
    """Converts the column names to upper case for consistency.

    Args:
        frame (pd.DataFrame): Target dataframe.

    Returns:
        pd.DataFrame: output dataframe.
    """
    frame.columns = frame.columns.str.upper()

    return frame


def convert_to_numpy_types(frame: pd.DataFrame) -> pd.DataFrame:
    """Converts the dtypes to numpy types for consistency.

    Args:
        frame (pd.DataFrame): Target dataframe.

    Returns:
        pd.DataFrame: output dataframe.
    """
    for col in frame.columns:
        # Convert pandas "Int64" (or similar) types to "int64"
        if pd.api.types.is_integer_dtype(frame[col]):
            frame[col] = frame[col].astype('int64', errors='ignore')
        # Convert pandas "boolean" type to NumPy "bool"
        elif pd.api.types.is_bool_dtype(frame[col]):
            frame[col] = frame[col].astype('bool', errors='ignore')
        # Convert pandas "string" type to NumPy "object"
        elif pd.api.types.is_string_dtype(frame[col]):
            frame[col] = frame[col].astype('object', errors='ignore')
        # Convert pandas "Float64" (or similar) types to "float64"
        elif pd.api.types.is_float_dtype(frame[col]):
            frame[col] = frame[col].astype('float64', errors='ignore')
    return frame


def _clean_height(df: pd.DataFrame) -> pd.Series:
    """
    Convert inch to centimeter
    """
    value = df.VALUE.astype(float).copy()

    def get_measurment_type(string):
        """
        """
        return 'in' in string.lower()

    index = df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type)
    value.loc[index] = np.round(value[index] * 2.54)
    return value


def _clean_systolic_bp(df: pd.DataFrame) -> pd.Series:
    """
    Filter out systolic blood preasure only. 
    """
    value = df.VALUE.astype(str).copy()
    index = value.apply(lambda string: '/' in string)
    value.loc[index] = value[index].apply(lambda string: re.match('^(\d+)/(\d+)$', string).group(1))
    return value.astype(float)


def _clean_diastolic_bp(df: pd.DataFrame) -> pd.Series:
    """
    Filter out diastolic blood preasure only. 
    """
    value = df.VALUE.astype(str).copy()
    index = value.apply(lambda string: '/' in string)
    value.loc[index] = value[index].apply(lambda string: re.match('^(\d+)/(\d+)$', string).group(2))
    return value.astype(float)


def _clean_capilary_rr(df: pd.DataFrame) -> pd.Series:
    """
    Categorize: Normal or Brisk: 0
                Abnormal or Delayed: 1
    """
    df = df.copy()
    value = pd.Series(np.zeros(df.shape[0]), index=df.index).copy()
    value.loc[:] = np.nan

    df['VALUE'] = df.VALUE.astype(str)

    value.loc[(df.VALUE == 'Normal <3 secs') | (df.VALUE == 'Brisk')] = 0
    value.loc[(df.VALUE == 'Abnormal >3 secs') | (df.VALUE == 'Delayed')] = 1
    return value


def _clean_fraction_inspired_o2(df: pd.DataFrame) -> pd.Series:
    """
    many 0s, mapping 1<x<20 to 0<x<0.2 
    """
    value = df.VALUE.astype(float).copy()

    # Check wheather value is string
    is_str = np.array(map(lambda x: type(x) == str, list(df.VALUE)), dtype=bool)

    def get_measurment_type(string):
        """
        torr is equal to mmHg
        """
        return 'torr' not in string.lower()

    index = df.VALUEUOM.fillna('').apply(get_measurment_type) & (is_str | (~is_str & (value > 1.0)))

    value.loc[index] = value[index] / 100.

    return value


def _clean_laboratory_values(df: pd.DataFrame) -> pd.Series:
    """
    GLUCOSE, PH: sometimes have ERROR as value
    """
    value = df.VALUE.copy()
    index = value.apply(
        lambda string: type(string) is str and not re.match('^(\d+(\.\d*)?|\.\d+)$', string))
    value.loc[index] = np.nan
    return value.astype(float)


def _clean_o2sat(df: pd.DataFrame) -> pd.Series:
    """
    small number of 0<x<=1 that should be mapped to 0-100 scale
    """
    # change "ERROR" to NaN
    value = df.VALUE.copy()
    index = value.apply(
        lambda string: type(string) is str and not re.match('^(\d+(\.\d*)?|\.\d+)$', string))
    value.loc[index] = np.nan
    value = value.astype(float)

    # Scale values
    index = (value <= 1)
    value.loc[index] = value[index] * 100.
    return value


def _clean_temperature(df: pd.DataFrame) -> pd.Series:
    """
    map Farenheit to Celsius, some ambiguous 50<x<80
    """
    value = df.VALUE.astype(float).copy()

    def get_measurment_type(string):
        """
        """
        return 'F' in string

    index = df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type) | (value >= 79)
    value.loc[index] = (value[index] - 32) * 5. / 9
    return value


def _clean_weight(df: pd.DataFrame) -> pd.Series:
    """
    Weight: some really light/heavy adults: <50 lb, >450 lb, ambiguous oz/lb
    Children are tough for height, weight
    """
    value = df.VALUE.astype(float).copy()

    def get_measurment_type(string):
        """
        """
        return 'oz' in string

    # ounces
    index = df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type)
    value.loc[index] = value[index] / 16.

    def get_measurment_type(string):
        """
        """
        return 'lb' in string

    # pounds
    index = index | df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type)
    value.loc[index] = value[index] * 0.453592
    return value


def _clean_respiratory_rate(df: pd.DataFrame) -> pd.Series:
    """_summary_

    Args:
        df (_type_): _description_

    Returns:
        _type_: _description_
    """
    value = df.VALUE
    value = value.replace('>60/min retracts', 60)
    value = value.replace('>60/minute', 60)

    return value


def read_varmap_csv(resource_folder: Path):
    """
    Parameters:
        resource_folder:    If not default dataset path at data/mimic-iii-demo/resources/

    Returns:
        varmap_df:          Variable map containing relevant feature variables as provided by the benchmark code
    """
    csv_settings = DATASET_SETTINGS["varmap"]
    # Load the resource map
    varmap_df = pd.read_csv(Path(resource_folder, "itemid_to_variable_map.csv"),
                            index_col=None,
                            dtype=convert_dtype_dict(csv_settings["dtype"]))

    # Impute empty to string
    varmap_df = varmap_df.fillna('').astype(str)

    # Cast columns
    varmap_df['COUNT'] = varmap_df.COUNT.astype(int)
    varmap_df['ITEMID'] = varmap_df.ITEMID.astype(int)

    # Remove unlabeled and not occuring phenotypes and make sure only variables with ready status
    varmap_df = varmap_df.loc[(varmap_df['LEVEL2'] != '') & (varmap_df['COUNT'] > 0)]
    varmap_df = varmap_df.loc[(varmap_df.STATUS == 'ready')]

    # Get subdf
    varmap_df = varmap_df[['LEVEL2', 'ITEMID', 'MIMIC LABEL']].set_index('ITEMID')
    name_equivalences = {'LEVEL2': 'VARIABLE', 'MIMIC LABEL': 'MIMIC_LABEL'}
    varmap_df = varmap_df.rename(columns=name_equivalences)

    return varmap_df


### FILE: .\src\datasets\preprocessing.py ###
import random
import os
import pandas as pd
from copy import deepcopy
from itertools import chain
from typing import Dict
from pathlib import Path
from utils.IO import *
from utils import dict_subset
from pathos.multiprocessing import cpu_count, Pool
from preprocessing.preprocessors import MIMICPreprocessor
from .trackers import PreprocessingTracker
from .readers import ExtractedSetReader, ProcessedSetReader
from .mimic_utils import copy_subject_info

__all__ = ["compact_processing", "iterative_processing"]


def compact_processing(dataset: dict,
                       task: str,
                       phenotypes_yaml: dict,
                       subject_ids: list = None,
                       num_subjects: int = None,
                       storage_path: Path = None,
                       source_path: Path = None) -> Dict[str, Dict[str, pd.DataFrame]]:
    """_summary_

    Args:
        timeseries (pd.DataFrame): _description_
        episodic_data (pd.DataFrame): _description_
        subject_diagnoses (pd.DataFrame): _description_
        subject_icu_history (pd.DataFrame): _description_

    Returns:
        _type_: _description_
    """
    tracker = PreprocessingTracker(storage_path=Path(storage_path, "progress"),
                                   num_subjects=num_subjects,
                                   subject_ids=subject_ids)

    copy_subject_info(source_path, storage_path)

    if tracker.is_finished:
        info_io(f"Compact data processing already finalized in directory:\n{str(storage_path)}")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(root_path=storage_path,
                                  subject_ids=subject_ids).read_samples(read_ids=True)

    info_io(f"Compact Preprocessing: {task}", level=0)
    preprocessor = MIMICPreprocessor(task=task,
                                     storage_path=storage_path,
                                     phenotypes_yaml=phenotypes_yaml,
                                     tracker=tracker,
                                     label_type="one-hot",
                                     verbose=True)

    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=dataset.keys())
    assert all([len(subject) for subject in dataset.values()])
    missing_subjects = 0
    if num_subjects is not None:
        X_subjects = dict()
        y_subjects = dict()
        while not len(X_subjects) == num_subjects:
            curr_dataset = dict_subset(dataset, subject_ids)
            X, y = preprocessor.transform(dataset=curr_dataset)
            X_subjects.update(X)
            y_subjects.update(y)
            it_missing_subjects = set(X.keys()) - set(subject_ids)
            subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects -
                                                                len(X_subjects),
                                                                subject_ids=None,
                                                                all_subjects=excluded_subject_ids)
            if it_missing_subjects:
                missing_subjects += len(it_missing_subjects)
                debug_io(f"Missing subjects are: {*it_missing_subjects,}")
            if not subject_ids:
                break
            if len(X_subjects) == num_subjects:
                debug_io(f"Missing {len(X_subjects) - num_subjects} subjects.")
                debug_io(f"Unprocessable subjects are: {*it_missing_subjects,}")

    else:
        assert all([len(subject) for subject in dataset.values()])
        dataset = dict_subset(dataset, subject_ids)
        assert all([len(subject) for subject in dataset.values()])
        (X_subjects, y_subjects) = preprocessor.transform(dataset=dataset)
    if storage_path is not None:
        preprocessor.save_data()
        info_io(f"Finalized data preprocessing for {task} in directory:\n{str(storage_path)}")
    else:
        info_io(f"Finalized data preprocessing for {task}.")
    # TODO! this doesn't work, reimagine
    # if missing_subjects:
    #     warn_io(f"The subject target was not reached, missing {missing_subjects} subjects.")
    tracker.is_finished = True
    return {"X": X_subjects, "y": y_subjects}


def iterative_processing(reader: ExtractedSetReader,
                         task: str,
                         phenotypes_yaml: dict,
                         subject_ids: list = None,
                         num_subjects: int = None,
                         storage_path: Path = None) -> ProcessedSetReader:
    """_summary_

    Args:
        timeseries (pd.DataFrame): _description_
        episodic_data (pd.DataFrame): _description_
        subject_diagnoses (pd.DataFrame): _description_
        subject_icu_history (pd.DataFrame): _description_

    Returns:
        _type_: _description_
    """
    original_subject_ids = deepcopy(subject_ids)

    tracker = PreprocessingTracker(storage_path=Path(storage_path, "progress"),
                                   num_subjects=num_subjects,
                                   subject_ids=subject_ids)
    copy_subject_info(reader.root_path, storage_path)

    preprocessor = MIMICPreprocessor(task=task,
                                     reader=reader,
                                     storage_path=storage_path,
                                     tracker=tracker,
                                     phenotypes_yaml=phenotypes_yaml,
                                     label_type="one-hot")

    if tracker.is_finished:
        info_io(f"Data preprocessing for {task} is already in directory:\n{str(storage_path)}.")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(storage_path, subject_ids=subject_ids)

    info_io(f"Iterative Preprocessing: {task}", level=0)
    info_io(f"Preprocessing data for task {task}.")

    # Tracking info
    n_processed_subjects = len(tracker.subject_ids)
    n_processed_stays = len(tracker.stay_ids)
    n_processed_samples = tracker.samples

    # Parallel processing logic
    def process_subject(subject_id: str):
        """_summary_
        """
        _, tracking_infos = preprocessor_pr.transform_subject(subject_id)

        if tracking_infos:
            preprocessor_pr.save_data([subject_id])
            return subject_id, tracking_infos

        return subject_id, None

    def init(preprocessor: MIMICPreprocessor):
        global preprocessor_pr
        preprocessor_pr = preprocessor

    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=preprocessor.subjects,
                                                        processed_subjects=tracker.subject_ids)

    info_io(f"Processing timeseries data:\n"
            f"Processed subjects: {n_processed_subjects}\n"
            f"Processed stays: {n_processed_stays}\n"
            f"Processed samples: {n_processed_samples}\n"
            f"Skipped subjects: {0}")

    # Start the run
    with Pool(cpu_count() - 1, initializer=init, initargs=(preprocessor,)) as pool:
        res = pool.imap_unordered(process_subject, subject_ids, chunksize=500)

        empty_subjects = 0
        missing_subjects = 0
        while True:
            try:
                subject_id, tracker_data = next(res)
                if tracker_data is None:
                    empty_subjects += 1
                    # Add new samples if to meet the num subjects target
                    if num_subjects is None:
                        continue
                    debug_io(f"Missing subject is: {subject_id}")
                    try:
                        subj = excluded_subject_ids.pop()
                        res = chain(res, [pool.apply_async(process_subject, args=(subj,)).get()])
                    except IndexError:
                        missing_subjects += 1
                        debug_io(
                            f"Could not replace missing subject. Excluded subjects is: {excluded_subject_ids}"
                        )
                else:
                    n_processed_subjects += 1
                    n_processed_stays += len(tracker_data) - 1
                    n_processed_samples += tracker_data["total"]

                info_io(
                    f"Processing timeseries data:\n"
                    f"Processed subjects: {n_processed_subjects}\n"
                    f"Processed stays: {n_processed_stays}\n"
                    f"Processed samples: {n_processed_samples}\n"
                    f"Skipped subjects: {empty_subjects}",
                    flush_block=(True and not int(os.getenv("DEBUG", 0))))
            except StopIteration as e:
                tracker.is_finished = True
                info_io(f"Finalized for task {task} in directory:\n{str(storage_path)}")
                if num_subjects is not None and missing_subjects:
                    warn_io(
                        f"The subject target was not reached, missing {missing_subjects} subjects.")
                break
    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return ProcessedSetReader(storage_path, subject_ids=original_subject_ids)


def get_subject_ids(num_subjects: int,
                    subject_ids: list,
                    all_subjects: list,
                    processed_subjects: list = list()):
    remaining_subject_ids = list(set(all_subjects) - set(processed_subjects))
    # Select subjects to process logic
    n_processed_subjects = len(processed_subjects)
    if num_subjects is not None:
        num_subjects = max(num_subjects - n_processed_subjects, 0)
        selected_subjects_ids = random.sample(remaining_subject_ids, k=num_subjects)
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
        random.shuffle(remaining_subject_ids)
    elif subject_ids is not None:
        unknown_subjects = set(subject_ids) - set(all_subjects)
        if unknown_subjects:
            warn_io(f"Unknown subjects: {*unknown_subjects,}")
        selected_subjects_ids = list(set(subject_ids) & set(all_subjects))
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
    else:
        selected_subjects_ids = remaining_subject_ids
    return selected_subjects_ids, remaining_subject_ids


### FILE: .\src\datasets\readers.py ###
"""
Dataset Reader Module
=====================

This module provides classes and methods for reading and handling dataset files. 

Classes
-------

- AbstractReader: A base reader class for datasets, providing methods to handle and sample subject directories.
- ExtractedSetReader: A reader for extracted datasets, providing methods to read various types of data including timeseries, episodic data, events, diagnoses, and ICU history.
- ProcessedSetReader: A reader for processed datasets, providing methods to read samples and individual subject data.
- EventReader: A reader for event data from CHARTEVENTS, OUTPUTEVENTS, LABEVENTS, providing methods to read data either in chunks or in a single shot.
- SplitSetReader: A reader for datasets split into training, validation, and test sets, providing access to each split.

References
----------
- YerevaNN/mimic3-benchmarks: https://github.com/YerevaNN/mimic3-benchmarks
"""
import random
import re
import os
import threading
import pandas as pd
import numpy as np
from pathlib import Path
from collections.abc import Iterable
from copy import deepcopy
from utils.IO import *
from settings import *
from .mimic_utils import upper_case_column_names, convert_dtype_dict, read_varmap_csv
from .trackers import ExtractionTracker
from typing import List, Union, Dict

__all__ = ["ExtractedSetReader", "ProcessedSetReader", "EventReader", "SplitSetReader"]


class AbstractReader(object):
    """
    A base reader class for datasets, providing methods to handle and sample subject directories.

    Parameters
    ----------
    root_path : Path
        The root directory path containing subject folders.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subject directories in the root_path.

    Raises
    ------
    ValueError
        If the specified subject IDs do not have existing directories.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> reader = AbstractReader(root_path, subject_ids=[10006, 10011, 10019])
    >>> reader.subject_ids
    [10006, 10011, 10019]
    """

    def __init__(self, root_path: Path, subject_ids: List[int] = None) -> None:
        self._root_path = (root_path if isinstance(root_path, Path) else Path(root_path))

        if subject_ids is None:
            self._subject_folders = [
                folder for folder in self._root_path.iterdir()
                if folder.is_dir() and folder.name.isnumeric()
            ]
            self._update_self = True
        elif not subject_ids:
            warn_io("List of subjects passed to mimic dataset reader is empty!")
            self._update_self = False
            self._subject_folders = []
        else:
            self._update_self = False
            if all([Path(str(folder)).is_dir() for folder in subject_ids]):
                self._subject_folders = subject_ids
            elif all([Path(self._root_path, str(folder)).is_dir() for folder in subject_ids]):
                self._subject_folders = [
                    Path(self._root_path, str(folder)) for folder in subject_ids
                ]
            else:
                raise ValueError(
                    f"The following subject do not have existing directories: "
                    f"{*[ Path(str(folder)).name for folder in subject_ids if not (Path(self._root_path, str(folder)).is_dir() or Path(str(folder)).is_dir())],}"
                )

    def _update(self):
        """
        Update the list of subject folders. Does not update if subject IDs were specified on creation.
        """
        # Doesn't update if subject_ids specified on creation
        if self._update_self:
            self._subject_folders = [
                folder for folder in self._root_path.iterdir()
                if folder.is_dir() and folder.name.isnumeric()
            ]

    def _cast_dir_path(self, dir_path: Union[Path, str, int]) -> Path:
        """
        Cast the directory path to a Path object and ensure it is relative to the root path.
        """
        if isinstance(dir_path, int):
            dir_path = Path(str(dir_path))
        elif isinstance(dir_path, str):
            dir_path = Path(dir_path)
        if not dir_path.is_relative_to(self._root_path):
            dir_path = Path(self._root_path, dir_path)
        return dir_path

    def _cast_subject_ids(self, subject_ids: Union[List[str], List[int], np.ndarray]) -> List[int]:
        """
        Cast the subject IDs to a list of integers.
        """
        if subject_ids is None:
            return None
        return [int(subject_id) for subject_id in subject_ids]

    def _sample_ids(self, subject_ids: list, num_subjects: int, seed: int = 42):
        """
        Sample a specified number of subject IDs.
        """
        # Subject ids overwrites num subjects
        random.seed(seed)
        self._update()
        if subject_ids is not None:
            return subject_ids
        if num_subjects is not None:
            random.seed(seed)
            return random.sample(self.subject_ids, num_subjects)
        return self._subject_folders

    @property
    def root_path(self) -> Path:
        """
        Get the root directory path.

        Returns
        -------
        Path
            The root directory path.

        Examples
        --------
        >>> reader.root_path
        PosixPath('/path/to/data')
        """
        return self._root_path

    @property
    def subject_ids(self) -> List[int]:
        """
        Get the list of subject IDs either past as parameter or located in the directory.

        Returns
        -------
        List[int]
            The list of subject IDs.

        Examples
        --------
        >>> reader.subject_ids
        [10006, 10011, 10019]
        """
        return [int(folder.name) for folder in self._subject_folders]

    def _init_returns(self, file_types: tuple, read_ids: bool = True):
        """
        Initialize a dictionary or list to store the data to be read, depending on read IDs.
        """
        return {file_type: {} if read_ids else [] for file_type in file_types}


class ExtractedSetReader(AbstractReader):
    """
    A reader for extracted datasets, providing methods to read various types of data including
    timeseries, episodic data, events, diagnoses, and ICU history.

    Parameters
    ----------
    root_path : Path
        The root directory path containing subject folders.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subjects in the root_path.
    num_samples : int, optional
        Number of samples to read. If None, reads all available samples.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> reader = ExtractedSetReader(root_path, subject_ids=[10006, 10011, 10019])
    >>> timeseries = reader.read_timeseries(num_subjects=2)
    >>> episodic_data = reader.read_episodic_data(subject_ids=[10006, 10011])
    """

    convert_datetime = ["INTIME", "CHARTTIME", "OUTTIME", "ADMITTIME", "DISCHTIME", "DEATHTIME"]

    def __init__(self, root_path: Path, subject_ids: list = None, num_samples: int = None) -> None:
        """_summary_

        Args:
            root_path (Path): _description_
            subject_folders (list, optional): _description_. Defaults to None.
        """

        self._file_types = ("timeseries", "episodic_data", "subject_events", "subject_diagnoses",
                            "subject_icu_history")
        # Maps from file type to expected index name
        self._index_name_mapping = dict(zip(self._file_types[1:], ["Icustay", None, None, None]))
        # Maps from file type to dtypes
        self._dtypes = {
            file_type: DATASET_SETTINGS[file_index]["dtype"]
            for file_type, file_index in zip(self._file_types, [
                "timeseries",
                "episodic_data",
                "subject_events",
                "diagnosis",
                "icu_history",
            ])
        }
        self._convert_datetime = {
            "subject_icu_history": DATASET_SETTINGS["icu_history"]["convert_datetime"],
            "subject_events": DATASET_SETTINGS["subject_events"]["convert_datetime"]
        }
        super().__init__(root_path, subject_ids)

    def read_csv(self, path: Path, dtypes: tuple = None) -> pd.DataFrame:
        """
        Read a CSV file into a pandas DataFrame, converting specified columns to datetime.

        Parameters
        ----------
        path : Path
            Absolute or relative path to the CSV file.
        dtypes : tuple, optional
            Data type(s) to apply to either the whole dataset or individual columns.

        Returns
        -------
        pd.DataFrame
            The dataframe read from the specified location.

        Examples
        --------
        >>> df = reader.read_csv(Path("/path/to/file.csv"))
        >>> df.head()
        """
        file_path = Path(path)
        if not file_path.is_relative_to(self._root_path):
            file_path = Path(self._root_path, file_path)

        if not file_path.is_file():
            warn_io(f"File path {str(file_path)} does not exist!")
            return pd.DataFrame()
        try:
            df = pd.read_csv(file_path, dtype=dtypes, low_memory=False)
        except TypeError as error:
            error_io(f"Can't fit the integer range into requested dtype. Pandas error: {error}",
                     TypeError)

        df = upper_case_column_names(df)

        for column in set(df.columns) & set(self.convert_datetime):
            df[column] = pd.to_datetime(df[column], errors="coerce")

        return df

    def read_subject(self,
                     dir_path: Union[Path, int, str],
                     read_ids: bool = False,
                     file_type_keys: bool = True,
                     file_types: tuple = None):
        """
        Read data for a single subject for specified directory or subject ID.

        Parameters
        ----------
        dir_path : Union[Path, int, str]
            The directory path to read the subject data from.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        file_type_keys : bool, optional
            Whether to use file type keys in the returned dictionary. Defaults to True.
        file_types : tuple, optional
            The types of files to read. If None, reads all file types.

        Returns
        -------
        dict
            Dictionary containing the data read for the subject.

        Examples
        --------
        >>> subject_data = reader.read_subject(dir_path="10019", read_ids=True)
        >>> subject_data["timeseries"].head()
        """
        dir_path = self._cast_dir_path(dir_path)

        if file_types is None:
            file_types = self._file_types
        else:
            if not (isinstance(file_types, Iterable) and not isinstance(file_types, str)):
                raise ValueError(f'file_types must be a iterable but is {type(file_types)}')

        return_data = dict() if file_type_keys else list()

        if not self._check_subject_dir(dir_path,
                                       [file for file in file_types if not file == "timeseries"]):
            return {}

        for filename in file_types:
            if filename == "timeseries":
                if file_type_keys:
                    return_data["timeseries"] = self._get_timeseries(dir_path, read_ids)
                else:
                    return_data.append(self._get_timeseries(dir_path, read_ids))
            else:
                if file_type_keys:
                    return_data[filename] = self._read_file(filename, dir_path)
                else:
                    return_data.append(self._read_file(filename, dir_path))
        if not len(return_data):
            warn_io(f"Directory {str(dir_path)} does not exist!")
        return return_data

    def read_subjects(self,
                      subject_ids: Union[List[str], List[int], None] = None,
                      num_subjects: int = None,
                      read_ids: bool = False,
                      file_type_keys: bool = True,
                      seed: int = 42):
        """
        Read data for multiple subjects, with file keys being one of timeseries, episodic_data, subject_events,
        diagnosis or icu_history.

        Parameters
        ----------
        subject_ids : Union[List[str], List[int]], optional
            List of subject IDs to read. If None, reads all subjects.
        num_subjects : int, optional
            Number of subjects to read. If None, reads all available subjects.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        file_type_keys : bool, optional
            Whether to use file type keys in the returned dictionary. Defaults to True.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the data read for the subjects.

        Examples
        --------
        >>> subjects_data = reader.read_subjects(subject_ids=[10006, 10011], read_ids=True)
        >>> subjects_data[10006]["episodic_data"].head()
        >>> subjects_data[10006]["timeseries"][2246713].head()
        >>> # Or as list
        >>> subjects_data = reader.read_subjects(num_subjects=2)
        >>> subjects_data[0]["timeseries"][0].head()
        """
        subject_ids = self._cast_subject_ids(subject_ids)

        if subject_ids is not None and num_subjects is not None:
            raise ValueError("Only one of subject_ids or num_subjects can be specified!")

        subject_ids = self._sample_ids(subject_ids, num_subjects, seed)

        if read_ids:
            return_data = dict()
            for subject_id in subject_ids:
                subject_path = Path(self._root_path, str(subject_id))
                if not subject_path.is_dir():
                    continue
                subject_id = int(subject_path.name)
                return_data[subject_id] = self.read_subject(dir_path=Path(subject_path),
                                                            file_type_keys=file_type_keys,
                                                            file_types=self._file_types,
                                                            read_ids=read_ids)
            assert all([len(subject) for subject in return_data.values()])
            return return_data

        # without ids
        return_data = list()
        for subject_path in subject_ids:
            return_data.append(
                self.read_subject(dir_path=Path(subject_path),
                                  file_types=self._file_types,
                                  file_type_keys=file_type_keys,
                                  read_ids=read_ids))
        return return_data

    def read_timeseries(self,
                        num_subjects: int = None,
                        subject_ids: int = None,
                        read_ids: bool = False,
                        seed: int = 42):
        """
        Read timeseries data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the timeseries data for the subjects.

        Examples
        --------
        >>> timeseries_data = reader.read_timeseries(num_subjects=2, read_ids=True)
        >>> timeseries_data[10006][244351].head()
        >>> # Or as list
        >>> timeseries_data = reader.read_timeseries(subject_ids=[10006, 10011])
        >>> timeseries_data[0].head()
        """
        return self._read_filetype("timeseries", num_subjects, subject_ids, read_ids, seed)

    def read_episodic_data(self,
                           num_subjects: int = None,
                           subject_ids: int = None,
                           read_ids: bool = False,
                           seed: int = 42):
        """
        Read episodic data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the episodic data for the subjects.

        Examples
        --------
        >>> episodic_data = reader.read_episodic_data(subject_ids=[10006, 10011])
        >>> episodic_data[10006].head()
        """
        return self._read_filetype("episodic_data", num_subjects, subject_ids, read_ids, seed)

    def read_events(self,
                    num_subjects: int = None,
                    subject_ids: int = None,
                    read_ids: bool = False,
                    seed: int = 42):
        """
        Read event data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the event data for the subjects.

        Examples
        --------
        >>> events_data = reader.read_events(subject_ids=[10006, 10011])
        >>> events_data[10006].head()
        """
        return self._read_filetype("subject_events", num_subjects, subject_ids, read_ids, seed)

    def read_diagnoses(self,
                       num_subjects: int = None,
                       subject_ids: int = None,
                       read_ids: bool = False,
                       seed: int = 42):
        """
        Read diagnosis data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the diagnosis data for the subjects.

        Examples
        --------
        >>> diagnoses_data = reader.read_diagnoses(subject_ids=[10006, 10011])
        >>> diagnoses_data[10006].head()
        """
        return self._read_filetype("subject_diagnoses", num_subjects, subject_ids, read_ids, seed)

    def read_icu_history(self,
                         num_subjects: int = None,
                         subject_ids: int = None,
                         read_ids: bool = False,
                         seed: int = 42):
        """
        Read ICU history data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the ICU history data for the subjects.

        Examples
        --------
        >>> icu_history_data = reader.read_icu_history(subject_ids=[10006, 10011])
        >>> icu_history_data[10006].head()
        """
        return self._read_filetype("subject_icu_history", num_subjects, subject_ids, read_ids, seed)

    def _read_filetype(
        self,
        file_type: str,
        num_subjects: int,
        subject_ids: Union[List[int], List[str], np.ndarray],
        read_ids: bool,
        seed: int,
    ):

        subject_ids = self._cast_subject_ids(subject_ids)

        if subject_ids is not None and num_subjects is not None:
            raise ValueError("Only one of subject_ids or num_subjects can be specified!")

        subject_ids = self._sample_ids(subject_ids, num_subjects, seed)
        subject_folders = [Path(self.root_path, str(subject_id)) for subject_id in subject_ids]
        if read_ids:
            return self._read_data_with_ids(subject_folders, file_type)
        return self._read_data_without_ids(subject_folders, file_type)

    def _read_data_with_ids(self, subject_folders: Path, file_type: str):
        return_data = dict()
        for subject_path in subject_folders:
            subject_id = int(subject_path.name)
            return_data.update({
                subject_id:
                    self.read_subject(dir_path=Path(subject_path),
                                      file_types=[file_type],
                                      file_type_keys=False,
                                      read_ids=True).pop()
            })
        return return_data

    def _read_data_without_ids(self, subject_folders: Path, file_type: str):
        return_data = list()
        for subject_id in subject_folders:
            if file_type == 'timeseries':
                return_data.extend(
                    self.read_subject(dir_path=Path(subject_id),
                                      file_types=[file_type],
                                      file_type_keys=False,
                                      read_ids=False).pop())
            else:
                return_data.append(
                    self.read_subject(dir_path=Path(subject_id),
                                      file_types=[file_type],
                                      file_type_keys=False,
                                      read_ids=False).pop())
        return return_data

    def _check_subject_dir(self, subject_folder: Path, file_types: tuple):
        """_summary_

        Args:
            dir_path (Path): _description_
            file_types (tuple): _description_

        Returns:
            _type_: _description_
        """
        if os.getenv("DEBUG"):
            for filename in file_types:
                if not Path(subject_folder, f"{filename}.csv").is_file():
                    debug_io(f"Directory {subject_folder} does not have file {filename}.csv")
        return all([
            True if Path(subject_folder, f"{filename}.csv").is_file() else False
            for filename in file_types
        ])

    def _read_file(self, filename: str, dir_path: Path):  # , return_data: dict):
        """_summary_

        Args:
            filename (str): _description_
            dir_path (Path): _description_
            subject_id (int): _description_
        """
        file_df = pd.read_csv(Path(dir_path, f"{filename}.csv"),
                              dtype=self._dtypes[filename],
                              index_col=self._index_name_mapping[filename],
                              low_memory=False)

        if filename in self._convert_datetime:
            for column in self._convert_datetime[filename]:
                file_df[column] = pd.to_datetime(file_df[column])
        return file_df

    def _get_timeseries(self, dir_path: Path, read_ids: bool):
        """_summary_

        Args:
            dir_path (Path): _description_
            subject_id (int): _description_
        """
        subject_files = os.listdir(dir_path)
        if read_ids:
            timeseries = dict()
        else:
            timeseries = list()

        for file in subject_files:
            stay_id = re.findall('[0-9]+', file)
            if not stay_id:
                continue

            stay_id = stay_id.pop()
            if file.replace(stay_id, "") == "timeseries_.csv":
                if read_ids:
                    timeseries[int(stay_id)] = pd.read_csv(
                        Path(dir_path, file), dtype=self._dtypes["timeseries"]).set_index('hours')
                else:
                    timeseries.append(
                        pd.read_csv(Path(dir_path, file),
                                    dtype=self._dtypes["timeseries"]).set_index('hours'))

        return timeseries


class ProcessedSetReader(AbstractReader):
    """
    A reader for processed datasets, providing methods to read samples and individual subject data.

    Parameters
    ----------
    root_path : Path
        The root directory path containing subject folders.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subjects in the root_path.
    set_index : bool, optional
        Whether to set the index for the dataframes. Defaults to True.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> reader = ProcessedSetReader(root_path, subject_ids=[10006, 10011, 10019])
    >>> X, y = reader.random_samples(n_samples=2).values()
    >>> X[10006][244351].head()
    """

    def __init__(self, root_path: Path, subject_ids: list = None, set_index: bool = True) -> None:
        """_summary_

        Args:
            root_path (Path): _description_
            subject_folders (list, optional): _description_. Defaults to None.
        """
        self._reader_switch_Xy = {
            "csv": {
                "X": (lambda x: self._read_csv(x, dtypes=DATASET_SETTINGS["timeseries"]["dtype"])),
                "y": lambda x: self._read_csv(x)
            },
            "npy": {
                "X": np.load,
                "y": np.load,
                "t": np.load
            },
            "h5": {
                "X": pd.read_hdf,
                "y": pd.read_hdf
            }
        }
        super().__init__(root_path, subject_ids)
        self._random_ids = deepcopy(self.subject_ids)
        self._convert_datetime = ["INTIME", "CHARTTIME", "OUTTIME"]
        self._possibgle_datatypes = [pd.DataFrame, np.ndarray, np.array, None]

    @staticmethod
    def _read_csv(path: Path, dtypes: tuple = None) -> pd.DataFrame:
        df = pd.read_csv(path, dtype=dtypes)
        if 'hours' in df.columns:
            df = df.set_index('hours')
        if 'Timestamp' in df.columns:
            df = df.set_index('Timestamp')
        return df

    def read_samples(self,
                     subject_ids: Union[List[str], List[int]] = None,
                     read_ids: bool = False,
                     read_timestamps: bool = False,
                     data_type=None):
        """
        Read samples for the specified subject IDs, either as dictionary with ID keys or as list.

        Parameters
        ----------
        subject_ids : Union[List[str], List[int]], optional
            List of subject IDs to read. If None, reads all subjects.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        read_timestamps : bool, optional
            Whether to read timestamps. Defaults to False.
        data_type : type, optional
            Data type to cast the read data to. Can be one of [pd.DataFrame, np.ndarray, None]. Defaults to None.

        Returns
        -------
        dict
            Dictionary containing the samples read.

        Examples
        --------
        >>> # Reading samples for subject ID 10006 and stay 244351
        >>> samples = reader.read_samples(subject_ids=[10006], read_ids=True)
        >>> samples["X"][10006][244351].head()
        """

        dataset = {"X": {}, "y": {}} if read_ids else {"X": [], "y": []}

        if read_timestamps:
            dataset.update({"t": {} if read_ids else []})

        if subject_ids is None:
            subject_ids = self.subject_ids

        subject_ids = self._cast_subject_ids(subject_ids)

        for subject_id in subject_ids:
            sample = self.read_sample(subject_id,
                                      read_ids=read_ids,
                                      read_timestamps=read_timestamps,
                                      data_type=data_type)
            for prefix in sample:
                if not len(sample[prefix]):
                    warn_io(f"Subject {subject_id} does not exist!")
                if read_ids:
                    dataset[prefix].update({subject_id: sample[prefix]})
                else:
                    dataset[prefix].extend(sample[prefix])

        return dataset

    def read_sample(self,
                    subject_id: Union[int, str],
                    read_ids: bool = False,
                    read_timestamps: bool = False,
                    data_type=None) -> dict:
        """
        Read data for a single subject.

        Parameters
        ----------
        subject_id : Union[int, str]
            The subject ID to read.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        read_timestamps : bool, optional
            Whether to read timestamps. Defaults to False.
        data_type : type, optional
            Data type to cast the read data to. Can be one of [pd.DataFrame, np.ndarray, None]. Defaults to None.

        Returns
        -------
        dict
            Dictionary containing the data read for the subject.

        Raises
        ------
        ValueError
            If the data_type is not one of the possible data types.

        Examples
        --------
        >>> sample = reader.read_sample(subject_id=10006, read_ids=False)
        >>> sample["X"][0].head()
        """
        subject_id = int(subject_id)
        if not data_type in self._possibgle_datatypes:
            raise ValueError(
                f"Parameter data_type must be one of {self._possibgle_datatypes} but is {data_type}."
            )
        if data_type == np.array:
            data_type = np.ndarray

        dir_path = Path(self._root_path, str(subject_id))

        def _extract_number(string: str) -> int:
            stripper = f"abcdefghijklmnopqrstuvwxyzABZDEFGHIJKLMNOPQRSTUVWXYZ."
            return int(string.replace(".h5", "").replace(".csv", "").strip(stripper).strip("_"))

        def _convert_file_data(X):
            if data_type is None:
                return X
            if not isinstance(X, data_type) and data_type == np.ndarray:
                return X.to_numpy()
            elif not isinstance(X, data_type) and data_type == pd.DataFrame:
                return pd.DataFrame(X)
            return X

        dataset = {"X": {}, "y": {}} if read_ids else {"X": [], "y": []}

        if read_timestamps:
            dataset.update({"t": {} if read_ids else []})

        stay_id_stack = list()
        for file in dir_path.iterdir():
            stay_id = _extract_number(file.name)
            file_extension = file.suffix.strip(".")
            reader = self._reader_switch_Xy[file_extension]
            reader_kwargs = ({"allow_pickle": True} if file_extension == "npy" else {})

            if stay_id in stay_id_stack:
                continue

            stay_id_stack.append(stay_id)
            for prefix in dataset.keys():
                file_path = Path(file.parent, f"{prefix}_{stay_id}{file.suffix}")
                if not file_path.is_file():
                    continue
                file_data = reader[prefix](file_path, **reader_kwargs)
                file_data = _convert_file_data(file_data)

                if prefix == "t" and read_timestamps:
                    continue

                if read_ids:
                    if subject_id not in dataset[prefix]:
                        dataset[prefix].update({_extract_number(file.name): file_data})
                    else:
                        dataset[prefix][_extract_number(file.name)] = file_data
                else:
                    dataset[prefix].append(file_data)

        return dataset

    def random_samples(
            self,
            n_samples: int = 1,
            read_ids: bool = False,  # This is for debugging
            read_timestamps: bool = False,
            data_type=None,
            return_ids: bool = False,
            seed: int = 42):
        """
        Sample subjects randomly without replacement until subject list is exhauasted.

        Parameters
        ----------
        n_samples : int, optional
            Number of samples to read. Default is 1.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        read_timestamps : bool, optional
            Whether to read timestamps. Default is False.
        data_type : type, optional
            Data type to cast the read data to. Can be one of [pd.DataFrame, np.ndarray, None]. Default is None.
        return_ids : bool, optional
            Whether to return the sampled IDs along with the data. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the sampled data.
        list, optional
            List of sampled subject IDs if return_ids is True.

        Examples
        --------
        >>> samples = reader.random_samples(n_samples=3, read_ids=True)
        >>> samples["X"].keys()
        >>> data, ids = reader.random_samples(n_samples=3, return_ids=True)
        >>> ids
        >>> [10006, 10011, 10019]
        """
        random.seed(seed)
        sample_ids = list()
        n_samples_needed = n_samples

        while n_samples_needed > 0:
            if not self._random_ids:
                self._random_ids = list(set(self.subject_ids) - set(sample_ids))
                random.shuffle(self._random_ids)

            n_samples_curr = min(len(self._random_ids), n_samples_needed)
            sample_ids.extend(self._random_ids[:n_samples_curr])
            self._random_ids = self._random_ids[n_samples_curr:]
            n_samples_needed -= n_samples_curr

            if len(sample_ids) >= len(self.subject_ids):
                if len(sample_ids) > len(self.subject_ids):
                    warn_io(
                        f"Maximum number of samples in dataset reached! Requested {n_samples}, but dataset size is {len(self.subject_ids)}."
                    )
                break
        if return_ids:
            return self.read_samples(sample_ids,
                                     read_ids=read_ids,
                                     read_timestamps=read_timestamps,
                                     data_type=data_type), sample_ids
        return self.read_samples(sample_ids,
                                 read_ids=read_ids,
                                 read_timestamps=read_timestamps,
                                 data_type=data_type)


class EventReader():
    """
    A reader for event data from CHARTEVENTS, OUTPUTEVENTS, LABEVENTS, providing methods to read data either in chunks
    or in a single shot.

    Parameters
    ----------
    dataset_folder : Path
        The path to the dataset folder containing event data files.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subjects.
    chunksize : int, optional
        The size of chunks to read at a time. Defaults to None.
    tracker : ExtractionTracker, optional
        An object to track the extraction progress. Defaults to None.

    Examples
    --------
    >>> dataset_folder = Path("/path/to/dataset")
    >>> event_reader = EventReader(dataset_folder, subject_ids=[10006, 10011, 10019], chunksize=1000)
    >>> event_reader.get_chunk()["CHARTEVENTS.csv"].head()
    """

    def __init__(self,
                 dataset_folder: Path,
                 subject_ids: list = None,
                 chunksize: int = None,
                 tracker: ExtractionTracker = None) -> None:
        """_summary_

        Args:
            dataset_folder (Path): _description_
            chunksize (int, optional): _description_. Defaults to None.
            tracker (object, optional): _description_. Defaults to None.
        """
        self.dataset_folder = dataset_folder
        self._done = False

        # Logic to early terminate if none of the subjects are in the remaining dataset
        if subject_ids is not None and len(subject_ids):
            self._last_occurrence = {
                "CHARTEVENTS.csv": {},
                "LABEVENTS.csv": {},
                "OUTPUTEVENTS.csv": {}
            }
            subject_target = dict(
                zip([str(subject_id) for subject_id in subject_ids],
                    ["SUBJECT_ID"] * len(subject_ids)))
            self._subject_ids = [int(subject_id) for subject_id in subject_ids]
            self._lo_thread_response = {}  # Last occurence thread response
            self._lo_thread_done = {}
            for csv in self._last_occurrence:
                self._lo_thread_response[csv] = threading.Event()
                thread = threading.Thread(target=self._csv_find_last,
                                          args=(Path(self.dataset_folder, csv), subject_target,
                                                self._last_occurrence[csv],
                                                self._lo_thread_response[csv]))
                thread.start()
                self._lo_thread_done[csv] = False

        else:
            self._lo_thread_response = None
            self._lo_thread_done = {
                "CHARTEVENTS.csv": False,
                "LABEVENTS.csv": False,
                "OUTPUTEVENTS.csv": False
            }
            self._subject_ids = None

        self._chunksize = chunksize
        self._tracker = tracker
        self._csv_settings = DATASET_SETTINGS["CHARTEVENTS"]
        self._event_csv_kwargs = {
            "CHARTEVENTS.csv": {
                "dtype": convert_dtype_dict(DATASET_SETTINGS["CHARTEVENTS"]["dtype"])
            },
            "LABEVENTS.csv": {
                "dtype": convert_dtype_dict(DATASET_SETTINGS["LABEVENTS"]["dtype"])
            },
            "OUTPUTEVENTS.csv": {
                "dtype": convert_dtype_dict(DATASET_SETTINGS["OUTPUTEVENTS"]["dtype"])
            }
        }
        self.event_csv_skip_rows = dict(zip(self._event_csv_kwargs.keys(), [0, 0, 0]))

        if chunksize:
            # Readers from which to get chunk
            # Pandas is leaky when get chunk is not carried out to the EOF so we
            # have to manage the file handle ourselves
            self._csv_reader = dict()
            self._csv_handle = dict()
            for csv_name, kwargs in self._event_csv_kwargs.items():
                file_handle = Path(dataset_folder, csv_name).open("rb")
                self._csv_reader[csv_name] = pd.read_csv(file_handle,
                                                         iterator=True,
                                                         chunksize=chunksize,
                                                         low_memory=False,
                                                         **kwargs)
                self._csv_handle[csv_name] = file_handle
            if subject_ids is None:
                # If subject ids is specified we need to start from the begining again
                # Since the new subjects might be in the first chunks
                self._init_reader()

        self._convert_datetime = ["INTIME", "CHARTTIME", "OUTTIME"]
        resource_folder = Path(dataset_folder, "resources")
        assert resource_folder.is_dir(), FileNotFoundError(
            f"Folder {str(resource_folder)} does not contain resources folder, which in turn should contain"
            f" hcup_ccs_2015_definitions.yaml and itemid_to_variable_map.csv.")
        self._varmap_df = read_varmap_csv(resource_folder)

    @property
    def done_reading(self):
        """
        Check if all chunks have been read.

        Returns
        -------
        bool
            True if all chunks have been read, False otherwise.

        Examples
        --------
        >>> event_reader.done_reading
        False
        """
        return self._done

    def _init_reader(self):
        """
        Initialize the reader by skipping rows according to the tracker if it exists.
        """
        info_io(f"Starting reader initialization.")
        header = "Initializing reader and starting at row:\n"
        msg = list()
        for csv in self._event_csv_kwargs:
            try:
                n_chunks = self._tracker.count_subject_events[csv] // self._chunksize
                skip_rows = self._tracker.count_subject_events[csv] % self._chunksize
                [len(self._csv_reader[csv].get_chunk()) for _ in range(n_chunks)]  # skipping chunks
                self.event_csv_skip_rows[csv] = skip_rows  # rows to skip in first chunk
                msg.append(f"{csv}: {self._tracker.count_subject_events[csv]}")
            except:
                self._csv_handle[csv].close()
        info_io(header + " - ".join(msg))

    def get_chunk(self) -> tuple:
        """
        Get the next chunk of event data.

        Returns
        -------
        tuple
            A tuple containing:
            - event_frames: dict of pd.DataFrame
                A dictionary where keys are CSV file names and values are dataframes of the read chunk.
            - frame_lengths: dict of int
                A dictionary where keys are CSV file names and values are the number of events read in the chunk.

        Examples
        --------
        >>> event_frames, frame_lengths = event_reader.get_chunk()
        >>> event_frames["CHARTEVENTS.csv"].head()
        """
        def read_frame(csv_name):
            # Read a frame
            if self._lo_thread_response is not None:
                if not self._lo_thread_done[csv_name] and self._lo_thread_response[csv_name].is_set(
                ):
                    last_occurences = max(self._last_occurrence[csv_name].values())
                    self._last_occurrence[csv_name] = last_occurences
                    debug_io(f"Last occurence for {csv_name}: {last_occurences}")
                    self._lo_thread_done[csv_name] = True

            events_df = self._csv_reader[csv_name].get_chunk()

            # If start index exceeds last occurence of any subject, stop reader
            if self._lo_thread_done[csv_name] and events_df.index[0] >= self._last_occurrence[
                    csv_name]:
                debug_io(
                    f"Reader for {csv_name} is done on last occurence at line {events_df.index[0]}."
                )
                self._csv_handle[csv_name].close()
                if all([handle.closed for handle in self._csv_handle.values()]):
                    self._done = True

            # Uppercase column names for consistency
            events_df = upper_case_column_names(events_df)

            if self._subject_ids is not None:
                events_df = events_df[events_df["SUBJECT_ID"].isin(self._subject_ids)]

            if not 'ICUSTAY_ID' in events_df:
                events_df['ICUSTAY_ID'] = pd.NA
                events_df['ICUSTAY_ID'] = events_df['ICUSTAY_ID'].astype(
                    self._event_csv_kwargs[csv_name]["dtype"]["ICUSTAY_ID"])

            # Drop specified columns and NAN rows, merge onto varmap for variable definitions
            drop_cols = set(events_df.columns) - set(self._csv_settings["columns"])
            events_df = events_df.drop(drop_cols, axis=1)

            # Skip start rows if directory already existed
            if self.event_csv_skip_rows[csv_name]:
                events_df = events_df.iloc[self.event_csv_skip_rows[csv_name]:]
                self.event_csv_skip_rows[csv_name] = 0

            # Convert to datetime
            for column in self._csv_settings["convert_datetime"]:
                events_df[column] = pd.to_datetime(events_df[column])
            if not events_df.empty and self._lo_thread_response is not None:
                debug_io(
                    f"Csv: {csv_name}\nRead chunk of size: {len(events_df)}\nLast idx: {events_df.index[-1]}"
                )

            return events_df

        event_frames = dict()

        for csv_name in self._event_csv_kwargs:
            # Read frame, if reader is done, it will raise
            try:
                if not self._csv_handle[csv_name].closed:
                    event_frames[csv_name] = read_frame(csv_name)
            except StopIteration as error:
                debug_io(f"Reader finished on {error}")
                self._csv_handle[csv_name].close()
                if all([handle.closed for handle in self._csv_handle.values()]):
                    self._done = True

            # Readers are done, return empty frame if queried
            if all([handle.closed for handle in self._csv_handle.values()]):
                return {csv_name: pd.DataFrame() for csv_name in self._event_csv_kwargs}, dict()

        # Number of subject events per CVS type
        frame_lengths = {csv_name: 0 for csv_name in self._event_csv_kwargs}
        frame_lengths.update({csv_name: len(frame) for csv_name, frame in event_frames.items()})

        return event_frames, frame_lengths

    def get_all(self):
        """
        Get all event data from the dataset.

        Returns
        -------
        pd.DataFrame
            Dataframe containing all event data.

        Examples
        --------
        >>> all_events = event_reader.get_all()
        >>> all_events.head()
        """
        event_csv = ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]
        event_frames = list()

        for csv in event_csv:
            events_df = pd.read_csv(Path(self.dataset_folder, csv),
                                    low_memory=False,
                                    **self._event_csv_kwargs[csv])
            events_df = upper_case_column_names(events_df)

            if not 'ICUSTAY_ID' in events_df:
                events_df['ICUSTAY_ID'] = pd.NA

            # Drop specified columns and NAN rows, merge onto varmap for variable definitions
            drop_cols = set(events_df.columns) - set(self._csv_settings["columns"])
            events_df = events_df.drop(drop_cols, axis=1)

            for column in self._csv_settings["convert_datetime"]:
                events_df[column] = pd.to_datetime(events_df[column])

            event_frames.append(events_df)

        full_df = pd.concat(event_frames, ignore_index=True)

        return full_df

    @staticmethod
    def _csv_find_last(csv_path: Path,
                       target_dict: dict,
                       last_occurences: dict,
                       done_event: threading.Event = None):
        """Finds the last line where the values in the target_dict are found in the csv file.
        Target_dict should be a dictionary with the target value as key and the target column name as value.
        """
        with open(csv_path, 'r', encoding='utf-8') as file:
            # Check if the given column name is valid
            headers = file.readline().strip().split(',')
            column_indices = {name.strip("\'\""): index for index, name in enumerate(headers)}

            missing_columns = [
                column_name for column_name in target_dict.values()
                if column_name not in column_indices
            ]
            if missing_columns:
                raise ValueError(f"Column name '{*missing_columns,}' does not exist in the file")

            # Column->idx and idx->column mapping
            column_indices = {
                column_name: column_indices[column_name] for column_name in target_dict.values()
            }
            value_to_idx = {value: column_indices[target_dict[value]] for value in target_dict}
            # Init counts
            line_number = 0
            last_occurences.update(dict(zip(target_dict.keys(), [0] * len(target_dict))))

            columns_of_interest = list(set(value_to_idx.values()))
            max_column_of_interest = max(columns_of_interest) + 1

            # Run through the file
            for line in file:
                columns = line.strip().split(',', maxsplit=max_column_of_interest)
                # Check if the target column has the target value
                line_number += 1
                for target_value in last_occurences.keys():
                    if target_value == columns[value_to_idx[target_value]]:
                        last_occurences[target_value] = line_number

        done_event.set()
        debug_io(
            f"Thread for {csv_path.name} has and found last occurence {max(last_occurences.values())}."
        )
        return last_occurences


class SplitSetReader(object):

    def __init__(self, root_path: Path, split_sets: Dict[str, List[int]]) -> None:
        """
        A reader for datasets split into training, validation, and test sets, providing access to each split.

        Parameters
        ----------
        root_path : Path
            The root directory path containing the dataset.
        split_sets : dict of {str: list of int}
            A dictionary where keys are split names (e.g., "train", "val", "test") and values are lists of subject IDs.

        Examples
        --------
        >>> root_path = Path("/path/to/data")
        >>> split_sets = {
        ...     "train": [1, 2, 3],
        ...     "val": [4, 5],
        ...     "test": [6, 7]
        ... }
        >>> split_reader = SplitSetReader(root_path, split_sets)
        >>> train_reader = split_reader.train
        >>> val_reader = split_reader.val
        >>> test_reader = split_reader.test
        """
        self._root_path = Path(root_path)
        self._subject_ids = split_sets
        self._readers = {
            split: ProcessedSetReader(self._root_path, subject_ids=split_sets[split])
            for split in split_sets
            if split_sets[split]
        }

        self._splits = list(self._readers.keys())
        cum_length = sum([len(split) for split in split_sets.values()])
        self._ratios = {split: len(split_sets[split]) / cum_length for split in split_sets}

    @property
    def split_names(self) -> list:
        """
        Get the names of the dataset splits.

        Returns
        -------
        list
            List of split names.

        Examples
        --------
        >>> split_reader.split_names
        ['train', 'val', 'test']
        """
        return self._splits

    @property
    def root_path(self):
        """
        Get the root path.

        Returns
        -------
        Path
            The root directory path.

        Examples
        --------
        >>> split_reader.root_path
        PosixPath('/path/to/data')
        """
        return self._root_path

    @property
    def train(self) -> ProcessedSetReader:
        """
        Get the reader for the training set.

        Returns
        -------
        ProcessedSetReader
            The reader for the training set, or None if not available.

        Examples
        --------
        >>> train_reader = split_reader.train
        >>> train_reader.read_samples()
        """
        if "train" in self._readers:
            return self._readers["train"]
        return

    @property
    def val(self) -> ProcessedSetReader:
        """
        Get the reader for the validation set.

        Returns
        -------
        ProcessedSetReader
            The reader for the validation set, or None if not available.

        Examples
        --------
        >>> val_reader = split_reader.val
        >>> val_reader.read_samples()
        """
        if "val" in self._readers:
            return self._readers["val"]
        return

    @property
    def test(self) -> ProcessedSetReader:
        """
        Get the reader for the test set.

        Returns
        -------
        ProcessedSetReader
            The reader for the test set, or None if not available.

        Examples
        --------
        >>> test_reader = split_reader.test
        >>> test_reader.read_samples()
        """
        if "test" in self._readers:
            return self._readers["test"]
        return


### FILE: .\src\datasets\trackers.py ###
from pathlib import Path
from utils.IO import *
from storable import storable


@storable
class ExtractionTracker():
    """_summary_
    """
    count_subject_events: dict = {"OUTPUTEVENTS.csv": 0, "LABEVENTS.csv": 0, "CHARTEVENTS.csv": 0}
    count_total_samples: int = 0
    has_bysubject_info: bool = False
    has_episodic_data: bool = False
    has_timeseries: bool = False
    has_subject_events: bool = False
    has_icu_history: bool = False
    has_diagnoses: bool = False
    is_finished: bool = False
    subject_ids: list = list()  # Not extraction target but tracking
    num_samples: int = None  # Extraction target
    num_subjects: int = None  # Extraction target

    def __init__(self,
                 num_samples: int = None,
                 num_subjects: int = None,
                 subject_ids: list = None,
                 *args,
                 **kwargs) -> None:

        # If num samples has increase more samples need to be raised, if decreased value error
        if self.num_samples is not None and num_samples is None:
            # If changed to None extraction is carried out for all samples
            self.reset(flags_only=True)

        if num_samples is not None and self.count_total_samples < num_samples:
            self.reset(flags_only=True)
            self.num_samples = num_samples
        elif self.num_samples is not None and num_samples is None:
            self.reset(flags_only=True)
            self.num_samples = num_samples

        if num_subjects is not None and len(self.subject_ids) < num_subjects:
            self.reset(flags_only=True)
            self.num_subjects = num_subjects
        # Continue processing if num subjects switche to None
        elif self.num_subjects is not None and num_subjects is None:
            self.reset(flags_only=True)
            self.num_subjects = num_subjects

        if subject_ids is not None:
            unprocessed_subjects = set(subject_ids) - set(self.subject_ids)
            if unprocessed_subjects:
                self.reset(flags_only=True)

    def reset(self, flags_only: bool = False):
        if not flags_only:
            self.count_subject_events = {
                "OUTPUTEVENTS.csv": 0,
                "LABEVENTS.csv": 0,
                "CHARTEVENTS.csv": 0
            }
            self.count_total_samples = 0
            self.num_samples = None
            self.num_subjects = None
            self.subject_ids = list()
        # The other dfs are light weight and computed for all subjects
        self.has_episodic_data = False
        self.has_timeseries = False
        self.has_bysubject_info = False
        self.has_subject_events = False
        self.is_finished = False


@storable
class PreprocessingTracker():
    """_summary_
    """
    subjects: dict = {}
    num_subjects: int = None
    is_finished: bool = False
    _store_total: bool = True
    # These are discretizer only settings
    time_step_size: int = None
    start_at_zero: bool = None
    impute_strategy: str = None
    mode: str = None

    def __init__(self, num_subjects: int = None, subject_ids: list = None, **kwargs):
        self._lock = None
        # Continue processing if num subjects is not reached
        if num_subjects is not None and len(self.subjects) - 1 < num_subjects:
            self.is_finished = False
            self.num_subjects = num_subjects
        # Continue processing if num subjects switche to None
        elif self.num_subjects is not None and num_subjects is None:
            self.is_finished = False
            self.num_subjects = num_subjects

        if subject_ids is not None:
            unprocessed_subjects = set(subject_ids) - set(self.subjects.keys())
            if unprocessed_subjects:
                self.is_finished = False

        # The impute startegies of the discretizer might change
        # In this case we rediscretize the data
        if kwargs:
            for attribute in ["time_step_size", "start_at_zero", "impute_strategy", "mode"]:
                if attribute in kwargs:
                    if getattr(self, attribute) is not None and getattr(
                            self, attribute) != kwargs[attribute]:
                        self.reset()
                    setattr(self, attribute, kwargs[attribute])

    @property
    def subject_ids(self) -> list:
        if hasattr(self, "_progress"):
            return [
                subject_id for subject_id in self._read("subjects").keys() if subject_id != "total"
            ]
        return list()

    @property
    def stay_ids(self) -> list:
        if hasattr(self, "_progress"):
            return [
                stay_id for subject_id, subject_data in self._read("subjects").items()
                if subject_id != "total" for stay_id in subject_data.keys() if stay_id != "total"
            ]
        return list()

    @property
    def samples(self) -> int:
        if hasattr(self, "_progress"):
            return sum([
                subject_data["total"]
                for subject_id, subject_data in self._read("subjects").items()
                if subject_id != "total"
            ])
        return 0

    def reset(self):
        self.subjects = {}
        self.is_finished = False
        self.num_subjects = None


@storable
class DataSplitTracker():
    # Targets
    test_size: float = None
    val_size: float = None
    train_size: float = None
    subjects: dict = {}
    # Demographic settings
    demographic_filter: dict = None
    demographic_split: dict = None
    # Results
    split: dict = {}
    ratios: dict = {}
    is_finished: bool = False

    def __init__(self,
                 tracker: PreprocessingTracker,
                 test_size: float = 0.0,
                 val_size: float = 0.0,
                 demographic_filter: dict = None,
                 demographic_split: dict = None):
        """_summary_

        Args:
            tracker (PreprocessingTracker): _description_
            test_size (float, optional): _description_. Defaults to 0.0.
            val_size (float, optional): _description_. Defaults to 0.0.
            demographic_filter (dict, optional): _description_. Defaults to None.
            demographic_split (dict, optional): _description_. Defaults to None.
        """
        if self.is_finished:
            # Reset if changed
            if self.test_size != test_size:
                self.reset()
            elif self.val_size != val_size:
                self.reset()
            elif self.demographic_filter != demographic_filter:
                self.reset()
            elif self.demographic_split != demographic_split:
                self.reset()
        # Apply settings
        self.test_size = test_size
        self.val_size = val_size
        self.demographic_filter = demographic_filter
        self.demographic_split = demographic_split
        self.subjects = tracker.subjects

    def reset(self) -> None:
        # Reset the results
        self.is_finished = False
        self.ratios = {}
        self.split = {}

    @property
    def subject_ids(self) -> list:
        if hasattr(self, "_progress"):
            return [
                subject_id for subject_id in self._read("subjects").keys() if subject_id != "total"
            ]
        return list()

    @property
    def split_sets(self):
        if hasattr(self, "_progress"):
            return list(self.ratios.keys())
        return list()


### FILE: .\src\datasets\writers.py ###
"""
Dataset Writers
===============

This module provides classes and methods for writing dataset files, and creating the subject directories named with the respecitve subject ID. The main class `DataSetWriter`
is used to write the subject data either as .npy, .csv, or .hdf5 files. 
and ICU history.

Classes
-------
- DataSetWriter: A writer class for datasets, providing methods to write data to files.

Todo
----
- Expand functionality as needed.

References
----------
- YerevaNN/mimic3-benchmarks: https://github.com/YerevaNN/mimic3-benchmarks
"""
import pandas as pd
import shutil
import numpy as np
from pathos.helpers import mp
import operator
from pathlib import Path
from utils.IO import *
from functools import reduce

__all__ = ["DataSetWriter"]


class DataSetWriter():
    """
    A writer class for datasets, providing methods to write data to files and create subject ID labeld directories.

    Parameters
    ----------
    root_path : Path
        The root directory path where the dataset files will be written.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> writer = DataSetWriter(root_path)
    >>> data = {
    ...     "subject_events": {10006: pd.DataFrame(...), 10011: pd.DataFrame(...)},
    ...     "episodic_data": {10006: pd.DataFrame(...), 10011: pd.DataFrame(...)}
    ... }
    >>> writer.write_bysubject(data, file_type="csv")
    """

    def __init__(self, root_path: Path) -> None:
        self.root_path = root_path

    def _check_filename(self, filename: str):
        """
        Check if the filename is valid.
        """
        possible_filenames = [
            "episodic_data", "timeseries", "subject_events", "subject_diagnoses",
            "subject_icu_history", "X", "y", "t", "header"
        ]

        if filename not in possible_filenames:
            raise (f"choose a filename from {possible_filenames}")

    def _get_subject_ids(self, data: dict):
        """
        Get the subject IDs from the data dictionary.
        """

        id_sets = [set(dictionary.keys()) for dictionary in data.values()]
        subject_ids = list(reduce(operator.and_, id_sets))
        return subject_ids

    def write_bysubject(self,
                        data: dict,
                        index: bool = True,
                        exists_ok: bool = False,
                        file_type: str = "csv"):
        """
        Write data of file type by subject and create subject ID labeled directories.

        Parameters
        ----------
        data : dict
            The data to write.
        index : bool, optional
            Whether to write the index. Default is True.
        exists_ok : bool, optional
            Whether to overwrite existing files. Default is False.
        file_type : str, optional
            The file type to write. Must be one of ['csv', 'npy', 'hdf5']. Default is 'csv'.

        Raises
        ------
        ValueError
            If the file_type is not supported.
        """
        if self.root_path is None:
            return

        if not file_type in ["csv", "npy", "hdf5"]:
            raise ValueError(
                f"file_type {file_type} not supported. Must be one of ['csv', 'npy', 'hdf5']")

        for subject_id in self._get_subject_ids(data):

            self._write_subject(subject_id=subject_id,
                            data={filename: data[filename][subject_id] for filename in data.keys()},
                            index=index,
                            exists_ok=exists_ok,
                            file_type=file_type)

        return

    def _write_subject(self,
                   subject_id: int,
                   data: dict,
                   index: bool = True,
                   exists_ok: bool = False,
                   file_type: str = "csv"):
        """
        Write all files for a single subject
        """

        def save_df(df: pd.DataFrame,
                    path: Path,
                    index: str = True,
                    file_type: str = "csv",
                    exists_ok: bool = False) -> None:
            if exists_ok and path.is_file() and not file_type == "hd5f":
                mode = "a"
                header = False
            else:
                mode = "w"
                header = True
            if file_type == "hdf5":
                pd.DataFrame(df).to_hdf(Path(path.parent, f"{path.stem}.h5"),
                                        key="data",
                                        mode=mode,
                                        index=index)
            elif file_type == "csv":
                pd.DataFrame(df).to_csv(Path(path.parent, f"{path.stem}.csv"),
                                        mode=mode,
                                        index=index,
                                        header=header)
            elif file_type == "npy":
                if isinstance(df, (pd.DataFrame, pd.Series)):
                    df = df.to_numpy()
                np.save(Path(path.parent, f"{path.stem}.npy"), df)

        if file_type in ["npy", "hdf5"] and exists_ok:
            raise ValueError("Append mode not supported for numpy files!")

        if not file_type in ["csv", "npy", "hdf5"]:
            raise ValueError(
                f"file_type {file_type} not supported. Must be one of ['csv', 'npy', 'hdf5']")
        for filename, item in data.items():
            delet_flag = False
            self._check_filename(filename)

            subject_path = Path(self.root_path, str(subject_id))

            if not subject_path.is_dir():
                subject_path.mkdir(parents=True, exist_ok=True)
            if isinstance(item, (pd.DataFrame, pd.Series, np.ndarray)):
                if not len(item):
                    continue
                csv_path = Path(subject_path, f"{filename}")
                save_df(df=item,
                        path=csv_path,
                        index=index,
                        file_type=file_type,
                        exists_ok=exists_ok)
            elif isinstance(item, dict):
                for icustay_id, data in item.items():
                    if not len(data):
                        continue
                    csv_path = Path(subject_path, f"{filename}_{icustay_id}")
                    save_df(df=data,
                            path=csv_path,
                            index=index,
                            file_type=file_type,
                            exists_ok=exists_ok)

            # do not create empty or incomplete folders
            if not [folder for folder in subject_path.iterdir()] or delet_flag:
                debug_io(
                    f"Removing folder {subject_path}, because a file is missing or the folder is empty!"
                )
                shutil.rmtree(str(subject_path))

    def write_subject_events(self, data: dict, lock: mp.Lock = None, dtypes: dict = None):
        """
        Write subject events data to files by creating a new file or appending to existing file and create subject ID labeled directories.

        Parameters
        ----------
        data : dict
            The subject events data to write.
        lock : mp.Lock, optional
            A lock object to synchronize writing. Default is None.
        dtypes : dict, optional
            Data types to cast the dataframe to. Default is None.
        """
        if self.root_path is None:
            return

        def write_csv(dataframe: pd.DataFrame, path: Path, lock: mp.Lock):
            if dataframe.empty:
                return
            if lock is not None:
                lock.acquire()
            if dtypes is not None:
                dataframe = dataframe.astype(dtypes)
            if not path.is_file():
                dataframe.to_csv(path, index=False)
            else:
                dataframe.to_csv(path, mode='a', index=False, header=False)

            if lock is not None:
                lock.release()
            return

        for subject_id, subject_data in data.items():
            subject_path = Path(self.root_path, str(subject_id))
            subject_path.mkdir(parents=True, exist_ok=True)
            subject_event_path = Path(subject_path, "subject_events.csv")

            write_csv(subject_data, subject_event_path, lock)

        return


### FILE: .\src\datasets\__init__.py ###
"""Dataset file

This file allows access to the dataset as specified.
All function in this file are used by the main interface function load_data.
Subfunctions used within private functions are located in the datasets.utils module.

TODOS
- Use a settings.json
- This is a construction site, see what you can bring in here
- Provid link to kaggle in load_data doc string
- Expand function to utils

YerevaNN/mimic3-benchmarks
"""
import yaml
from typing import Union
from pathlib import Path
from utils.IO import *
from settings import *
from . import extraction
from . import preprocessing
from . import feature_engineering
from . import discretizing
from .readers import ProcessedSetReader, ExtractedSetReader
from .split import train_test_split

# global settings

__all__ = ["load_data", "train_test_split"]


def load_data(source_path: str,
              storage_path: str = None,
              chunksize: int = None,
              subject_ids: list = None,
              num_subjects: int = None,
              time_step_size: float = 1.0,
              impute_strategy: str = "previous",
              mode: str = "legacy",
              start_at_zero=True,
              extract: bool = True,
              preprocess: bool = False,
              engineer: bool = False,
              discretize: bool = False,
              task: str = None) -> Union[ProcessedSetReader, ExtractedSetReader, dict]:
    """_summary_

    Args:
        stoarge_path (str, optional): Location where the processed dataset is to be stored. Defaults to None.
        source_path (str, optional): Location form which the unprocessed dataset is to be loaded. Defaults to None.
        ehr (str, optional): _description_. Defaults to None.
        from_storage (bool, optional): _description_. Defaults to True.
        chunksize (int, optional): _description_. Defaults to None.
        num_subjects (int, optional): _description_. Defaults to None.

    Raises:
        ValueError: _description_

    Returns:
        _type_: _description_
    """
    storage_path = Path(storage_path)
    source_path = Path(source_path)

    subject_ids = _check_inputs(storage_path=storage_path,
                                source_path=source_path,
                                chunksize=chunksize,
                                subject_ids=subject_ids,
                                num_subjects=num_subjects,
                                extract=extract,
                                preprocess=preprocess,
                                engineer=engineer,
                                discretize=discretize,
                                task=task)

    # Iterative generation if a chunk size is specified
    if storage_path is not None and chunksize is not None:
        if extract or preprocess or engineer or discretize:
            extracted_storage_path = Path(storage_path, "extracted")
            # Account for missing subjects
            reader = extraction.iterative_extraction(storage_path=extracted_storage_path,
                                                     source_path=source_path,
                                                     chunksize=chunksize,
                                                     subject_ids=subject_ids,
                                                     num_subjects=num_subjects,
                                                     task=task)

        if preprocess or engineer or discretize:
            # Contains phenotypes and a list of codes referring to the phenotype
            with Path(source_path, "resources", "hcup_ccs_2015_definitions.yaml").open("r") as file:
                phenotypes_yaml = yaml.full_load(file)

            processed_storage_path = Path(storage_path, "processed", task)
            reader = preprocessing.iterative_processing(reader=reader,
                                                        task=task,
                                                        subject_ids=subject_ids,
                                                        num_subjects=num_subjects,
                                                        storage_path=processed_storage_path,
                                                        phenotypes_yaml=phenotypes_yaml)

        if engineer:
            engineered_storage_path = Path(storage_path, "engineered", task)
            reader = feature_engineering.iterative_fengineering(
                subject_ids=subject_ids,
                num_subjects=num_subjects,
                reader=reader,
                task=task,
                storage_path=engineered_storage_path)

        if discretize:
            discretized_storage_path = Path(storage_path, "discretized", task)
            reader = discretizing.iterative_discretization(reader=reader,
                                                           task=task,
                                                           storage_path=discretized_storage_path,
                                                           time_step_size=time_step_size,
                                                           impute_strategy=impute_strategy,
                                                           start_at_zero=start_at_zero,
                                                           mode=mode)

        return reader

    elif chunksize is not None:
        raise ValueError("To run iterative iteration, specify storage path!")

    # Compact generation otherwise
    if extract or preprocess or engineer or discretize:
        extracted_storage_path = Path(storage_path, "extracted")
        dataset = extraction.compact_extraction(storage_path=extracted_storage_path,
                                                source_path=source_path,
                                                num_subjects=num_subjects,
                                                subject_ids=subject_ids,
                                                task=task)
    if preprocess or engineer or discretize:
        processed_storage_path = Path(storage_path, "processed", task)
        # Contains phenotypes and a list of codes referring to the phenotype
        with Path(source_path, "resources", "hcup_ccs_2015_definitions.yaml").open("r") as file:
            phenotypes_yaml = yaml.full_load(file)
        dataset = preprocessing.compact_processing(dataset=dataset,
                                                   task=task,
                                                   subject_ids=subject_ids,
                                                   num_subjects=num_subjects,
                                                   storage_path=processed_storage_path,
                                                   source_path=extracted_storage_path,
                                                   phenotypes_yaml=phenotypes_yaml)

    if engineer:
        engineered_storage_path = Path(storage_path, "engineered", task)
        dataset = feature_engineering.compact_fengineering(dataset["X"],
                                                           dataset["y"],
                                                           task=task,
                                                           storage_path=engineered_storage_path,
                                                           source_path=processed_storage_path,
                                                           subject_ids=subject_ids,
                                                           num_subjects=num_subjects)

    if discretize:
        discretized_storage_path = Path(storage_path, "discretized", task)
        dataset = discretizing.compact_discretization(dataset["X"],
                                                      dataset["y"],
                                                      task=task,
                                                      storage_path=discretized_storage_path,
                                                      source_path=processed_storage_path,
                                                      time_step_size=time_step_size,
                                                      impute_strategy=impute_strategy,
                                                      start_at_zero=start_at_zero,
                                                      mode=mode)

    # TODO: make dependent from return reader (can also return reader)
    # TODO: write some tests for comparct generation
    return dataset


def _check_inputs(storage_path: str, source_path: str, chunksize: int, subject_ids: list,
                  num_subjects: int, extract: bool, preprocess: bool, engineer: bool,
                  discretize: bool, task: str):
    if chunksize and not storage_path:
        raise ValueError(f"Specify storage path if using iterative processing!"
                         f"Storage path is '{storage_path}' and chunksize is '{chunksize}'")
    if (preprocess or engineer) and not task:
        raise ValueError(
            "Specify the 'task' parameter for which to preprocess or engineer the data!"
            " Possible values for task are: DECOMP, LOS, IHM, PHENO")
    if task and not (engineer or preprocess or discretize):
        warn_io(f"Specified  task '{task}' for data extraction only, despite "
                "data extraction being task agnostic. Parameter is ignored.")
    if subject_ids and num_subjects:
        raise ValueError("Specify either subject_ids or num_subjects, not both!")
    if not any([extract, preprocess, engineer]):
        raise ValueError("One of extract, preprocess or engineer must be set to load the dataset.")
    if subject_ids is not None:
        return [int(subject_id) for subject_id in subject_ids]
    return None


if __name__ == "__main__":
    resource_folder = Path(os.getenv("WORKINGDIR"), "datalab", "mimic", "data_splits", "resources")
    handler = SplitHandler(Path(resource_folder, "subject_info_df.csv"),
                           Path(resource_folder, "progress.json"))
    assert len(handler.get_subjects("ETHNICITY", "WHITE")) == 30019
    assert len(handler.get_subjects("ETHNICITY", "BLACK")) == 3631
    assert len(handler.get_subjects("ETHNICITY", "UNKNOWN/NOT SPECIFIED")) == 3861
    assert len(handler.get_subjects("ETHNICITY", "HISPANIC")) == 1538
    assert len(handler.get_subjects("ETHNICITY", "ASIAN")) == 1623
    assert len(handler.get_subjects("ETHNICITY", "OTHER")) == 1902
    assert len(handler.get_subjects("ETHNICITY", "UNABLE TO OBTAIN")) == 703

    assert len(handler.get_subjects("ETHNICITY", ["WHITE", "BLACK"])) == 30019 + 3631
    assert len(handler.get_subjects("ETHNICITY", ["HISPANIC", "ASIAN"])) == 1538 + 1623

    dataset, ratios = handler.split("ETHNICITY", test="WHITE", train="BLACK")
    assert len(dataset["train"]) == 3631
    assert len(dataset["test"]) == 30019

    dataset, ratios = handler.split("ETHNICITY", test="WHITE")
    assert sum(list(ratios.values())) == 1
    assert len(dataset["train"]) == 3631 + 3861 + 1538 + 1623 + 1902 + 703
    assert len(dataset["test"]) == 30019

    dataset, ratios = handler.split("ETHNICITY", train="WHITE")
    assert sum(list(ratios.values())) == 1
    assert len(dataset["train"]) == 30019
    assert len(dataset["test"]) == 3631 + 3861 + 1538 + 1623 + 1902 + 703

    dataset, ratios = handler.split("LANGUAGE", train=["ENGL", "SPAN"])
    assert sum(list(ratios.values())) == 1
    assert len(dataset["train"]) == 19436 + 728


### FILE: .\src\datasets\extraction\event_consumer.py ###
import pandas as pd
from pathlib import Path
from utils.IO import *
from multiprocess import Process, JoinableQueue, Lock
from .extraction_functions import make_subject_events
from ..writers import DataSetWriter


class EventConsumer(Process):

    def __init__(self, storage_path: Path, in_q: JoinableQueue, out_q: JoinableQueue,
                 icu_history_df: pd.DataFrame, lock: Lock):
        """_summary_

        Args:
            in_q (JoinableQueue): _description_
            out_q (JoinableQueue): _description_
            icu_history_df (pd.DataFrame): _description_
            storage_path (Path): _description_
            lock (Lock): _description_
            subject_ids (list, optional): _description_. Defaults to None.
        """
        super().__init__()
        self._in_q = in_q
        self._out_q = out_q
        self._icu_history_df = icu_history_df
        self._dataset_writer = DataSetWriter(storage_path)
        self._lock = lock

    def run(self):
        """_summary_
        """
        count = 0  # count read data chunks
        while True:
            # Draw from queue
            chartevents_df, frame_lengths = self._in_q.get()

            if chartevents_df is None:
                # Join the consumer and update the queue
                self._in_q.task_done()
                self._out_q.put((frame_lengths, True))
                debug_io(f"Consumer finished on empty df and consumed {count} event chunks.")
                break
            # Process events
            subject_events = self._make_subject_events(chartevents_df, self._icu_history_df)
            self._dataset_writer.write_subject_events(subject_events, self._lock)

            # Update queue and send tracking data to publisher
            self._in_q.task_done()
            self._out_q.put((frame_lengths, False))
            # Update tracking
            count += 1
        return

    @staticmethod
    def _make_subject_events(chartevents_df: pd.DataFrame, icu_history_df: pd.DataFrame):
        """_summary_

        Args:
            chartevents_df (pd.DataFrame): _description_
            icu_history_df (pd.DataFrame): _description_

        Returns:
            _type_: _description_
        """
        return make_subject_events(chartevents_df, icu_history_df)

    def start(self):
        """_summary_
        """
        super().start()
        debug_io("Started consumers")

    def join(self):
        """_summary_
        """
        super().join()
        debug_io("Joined in consumers")


### FILE: .\src\datasets\extraction\event_producer.py ###
import pandas as pd
import pathos, multiprocess
from pathlib import Path
from copy import deepcopy
from multiprocess import Manager
from pathos.multiprocessing import cpu_count
from utils.IO import *
from .event_consumer import EventConsumer
from .progress_publisher import ProgressPublisher
from ..mimic_utils import *
from ..trackers import ExtractionTracker
from ..readers import EventReader


class EventProducer(object):

    def __init__(self,
                 source_path: Path,
                 storage_path: Path,
                 num_samples: int,
                 chunksize: int,
                 tracker: ExtractionTracker,
                 icu_history_df: pd.DataFrame,
                 subject_ids: list = None):
        """_summary_

        Args:
            source_path (Path): _description_
            storage_path (Path): _description_
            num_samples (int): _description_
            chunksize (int): _description_
            tracker (MIMICExtractionTracker): _description_
            icu_history_df (pd.DataFrame): _description_
            subject_ids (list, optional): _description_. Defaults to None.
        """
        super().__init__()
        self._source_path = source_path
        self._storage_path = storage_path
        self._tracker = tracker
        self._icu_history_df = icu_history_df
        self._num_samples = num_samples
        self._chunksize = chunksize
        self._subject_ids = subject_ids

        # Counting variables
        self._count = 0  # count read data chunks
        self._total_length = self._tracker.count_total_samples
        event_csv = ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]
        self._ts_total_lengths = dict(zip(event_csv, [0] * 3))

        # Queues to connect process stages
        manager = Manager()
        self._in_q = manager.Queue()
        self._out_q = manager.Queue()

        # Number of cpus is adjusted depending on sample size
        if num_samples is not None:
            self._cpus = min(
                cpu_count() - 2,
                int(np.ceil((num_samples - tracker.count_total_samples) / (chunksize))))
        else:
            self._cpus = cpu_count() - 2
        debug_io(f"Using {self._cpus+2} CPUs!")

    def run(self):
        """_summary_
        """
        # Start event consumers (processing and storing)
        consumers = [
            EventConsumer(storage_path=self._storage_path,
                          in_q=self._in_q,
                          out_q=self._out_q,
                          icu_history_df=self._icu_history_df,
                          lock=self._tracker._lock)
            # lock=multiprocess.Lock())
        ]

        consumers[0].start()
        # Starting publisher (publish processing progress)
        progress_publisher = ProgressPublisher(n_consumers=self._cpus,
                                               source_path=self._source_path,
                                               in_q=self._out_q,
                                               tracker=self._tracker)
        progress_publisher.start()
        event_reader = EventReader(dataset_folder=self._source_path,
                                   chunksize=self._chunksize,
                                   subject_ids=self._subject_ids,
                                   tracker=self._tracker)
        while True:
            # Create more consumers if needed
            if len(consumers) < self._cpus and not self._in_q.empty():
                consumers.append(
                    EventConsumer(storage_path=self._storage_path,
                                  in_q=self._in_q,
                                  out_q=self._out_q,
                                  icu_history_df=self._icu_history_df,
                                  lock=self._tracker._lock))
                consumers[-1].start()
            # Read data chunk from CSVs through readers
            event_frames, frame_lengths = event_reader.get_chunk()

            # Number of timeseries samples after preprocessing
            # TODO! This shouldn't be computed if num_samples is unspecified
            ts_lengths = {
                csv_name: self._count_timeseries_sample(event_reader._varmap_df, frame)
                for csv_name, frame in event_frames.items()
            }

            # If sample limit defined
            if self._num_samples is not None:
                # If remaining samples until sample limit is reached is smaller than sum of remaining samples
                if self._num_samples - self._total_length < sum(ts_lengths.values()):
                    # Get the correct number of samples
                    event_frames, frame_lengths, ts_lengths = get_samples_per_df(
                        event_frames, self._num_samples - self._total_length)
                    # Let consumers know about sample limit finish, so that read chunks is not incremented
                    frame_lengths["sample_limit"] = True
                    # Queue up data frame
                    events_df = pd.concat(event_frames.values(), ignore_index=True)
                    self._in_q.put((events_df, frame_lengths))

                    # Close consumers on empty dfs
                    for _ in range(self._cpus):
                        self._in_q.put((None, {}))
                    self._update_total_lengths(ts_lengths)

                    # Record total length
                    self._total_length += sum(ts_lengths.values())
                    self._tracker.count_total_samples = self._total_length
                    self._count += 1
                    debug_io(
                        f"Event producer finished on sample size restriction and produced {self._count} event chunks."
                    )
                    break

            # Queue up data frame
            events_df = pd.concat(event_frames.values(), ignore_index=True)

            # Close consumers on empty df
            if event_reader.done_reading:
                for _ in range(len(consumers)):
                    self._in_q.put((None, {}))
                debug_io(
                    f"Event producer finished on empty data frame and produced {self._count} event chunks."
                )
                break
            else:
                self._in_q.put((events_df, frame_lengths))

            # Update tracking
            self._count += 1
            self._total_length += sum(ts_lengths.values())
            self._update_total_lengths(ts_lengths)
            self._tracker.count_total_samples = self._total_length

        # Join processes and queues when done reading
        debug_io(f"Joining in queue")
        self._in_q.join()
        debug_io(f"In queue joined")
        [consumer.join() for consumer in consumers]
        # signal the publisher that we're done
        debug_io(f"Sending consumer finishes for uninitialized consumers: {frame_lengths}")
        [self._out_q.put(({}, True)) for _ in range(self._cpus - len(consumers))]
        debug_io(f"Joining out queue")
        self._out_q.join()
        debug_io(f"Out queue joined")
        progress_publisher.join()

        return

    def _count_timeseries_sample(self, varmap_df: pd.DataFrame, frame: pd.DataFrame):
        """_summary_

        Args:
            varmap_df (pd.DataFrame): _description_
            frame (pd.DataFrame): _description_

        Returns:
            _type_: _description_
        """
        frame = deepcopy(frame)
        if frame.empty:
            return 0
        frame = frame.merge(varmap_df, left_on='ITEMID', right_index=True)
        frame = frame.dropna(subset=['HADM_ID'])
        frame = frame[['HADM_ID', 'ICUSTAY_ID',
                       'CHARTTIME']].merge(self._icu_history_df[['HADM_ID', 'ICUSTAY_ID']],
                                           left_on=['HADM_ID'],
                                           right_on=['HADM_ID'],
                                           suffixes=['', '_r'],
                                           how='inner')
        frame['ICUSTAY_ID'] = frame['ICUSTAY_ID'].fillna(frame['ICUSTAY_ID_r'])
        frame = frame.dropna(subset=['ICUSTAY_ID'])
        frame = frame[frame['ICUSTAY_ID'] == frame['ICUSTAY_ID_r']]

        return int(frame[["CHARTTIME"]].nunique())

    def _update_total_lengths(self, ts_lengths: dict):
        """_summary_

        Args:
            ts_lengths (dict): _description_
        """
        for csv_name, value in ts_lengths.items():
            self._ts_total_lengths[csv_name] += value
        return


### FILE: .\src\datasets\extraction\extraction_functions.py ###
import pandas as pd
from utils.IO import *
from ..mimic_utils import *


def make_subject_events(chartevents_df: pd.DataFrame, icu_history_df: pd.DataFrame):
    """
    Parameters:
        chartevents_df:     Chartevent data from ICU bed
        icu_history_df:     ICU stay data

    Returns:
        subject_events:     Dictionary containing chartevents per subject ID
    """
    chartevents_df = chartevents_df.dropna(subset=['HADM_ID'])
    recovered_df = chartevents_df.merge(icu_history_df,
                                        left_on=['HADM_ID'],
                                        right_on=['HADM_ID'],
                                        how='left',
                                        suffixes=['', '_r'],
                                        indicator=True)
    recovered_df = recovered_df[recovered_df['_merge'] == 'both']
    recovered_df['ICUSTAY_ID'] = recovered_df['ICUSTAY_ID'].fillna(recovered_df['ICUSTAY_ID_r'])
    recovered_df = recovered_df.dropna(subset=['ICUSTAY_ID'])
    recovered_df = recovered_df[(recovered_df['ICUSTAY_ID'] == recovered_df['ICUSTAY_ID_r'])]
    recovered_df = recovered_df[[
        'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM'
    ]]

    return {id: x for id, x in recovered_df.groupby('SUBJECT_ID') if not x.empty}


def make_timeseries(subject_events, subject_diagnoses, subject_icu_history, varmap_df):
    """
    Parameters:
        subject_events:         Chart and laboratory events per subject
        subject_diagnoses:      Diagnoses per ICU stay per patient 
        subject_icu_history:    ICU history per subject
        varmap_df:

    Returns:
        episodic_data:          Data describing each ICU stay 
        timeseries:             Timeseries of events over time for single ICU stay
    """
    variables = varmap_df.VARIABLE.unique()
    timeseries = dict()
    episodic_data = dict()

    for subject_id in subject_icu_history.keys():
        try:
            current_icu_history_df: pd.DataFrame = subject_icu_history[subject_id]
            current_diagnoses_df: pd.DataFrame = subject_diagnoses[subject_id]
            current_events_df: pd.DataFrame = subject_events[subject_id]
        except:
            continue

        # Adjust current events
        current_events_df = current_events_df.merge(varmap_df, left_on='ITEMID', right_index=True)
        current_events_df = current_events_df.loc[current_events_df.VALUE.notnull()]
        current_events_df.loc[:, 'VALUEUOM'] = current_events_df['VALUEUOM'].fillna('').astype(str)

        # Sort stays
        current_icu_history_df = current_icu_history_df.sort_values(by=['INTIME', 'OUTTIME'])

        # General patient data belonging to each ICU stay
        diagnosis_labels = make_diagnoses_util(current_diagnoses_df.reset_index(drop=True))
        episodic_data_df = make_episodic_data(current_icu_history_df)

        # Reset index before merge, so that we can keep it
        episodic_data_df = episodic_data_df.merge(diagnosis_labels,
                                                  left_index=True,
                                                  right_index=True)
        episodic_data_df.index.names = ["Icustay"]
        current_events_df = clean_chartevents_util(current_events_df)

        timeseries_df = make_timeseries_util(current_events_df, variables)

        timeseries[subject_id] = dict()

        for index in range(current_icu_history_df.shape[0]):
            stay_id = current_icu_history_df.ICUSTAY_ID.iloc[index]
            intime = current_icu_history_df.INTIME.iloc[index]
            outtime = current_icu_history_df.OUTTIME.iloc[index]
            icu_episode_df = make_episode(timeseries_df, stay_id, intime, outtime)

            if icu_episode_df.shape[0] == 0:
                continue

            icu_episode_df = make_hour_index(icu_episode_df, intime)

            episodic_data_df.loc[stay_id, 'Weight'] = get_static_value(icu_episode_df, 'Weight')
            episodic_data_df.loc[stay_id, 'Height'] = get_static_value(icu_episode_df, 'Height')

            timeseries[subject_id][stay_id] = icu_episode_df

        episodic_data[subject_id] = episodic_data_df

    return episodic_data, timeseries


### FILE: .\src\datasets\extraction\progress_publisher.py ###
import os
from utils import count_csv_size
from utils.IO import *
from pathlib import Path
from multiprocess import JoinableQueue, Process
from ..trackers import ExtractionTracker


class ProgressPublisher(Process):

    def __init__(self, n_consumers: int, source_path: Path, in_q: JoinableQueue,
                 tracker: ExtractionTracker):
        super().__init__()
        self._in_q = in_q
        self._tracker = tracker
        self._n_consumers = n_consumers
        self._event_file_lengths = {
            name: count_csv_size(Path(source_path, name))
            for name in ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]
        }

    def run(self):
        done_count = 0  # Track consumer finishes

        # Print initial state
        msg = [f"Processed event rows: "]
        for csv_name in self._event_file_lengths.keys():
            csv_event_count = self._tracker.count_subject_events[csv_name]
            total_event_count = self._event_file_lengths[csv_name]
            print_name = csv_name.strip('.csv') + ": "
            msg.append(
                f"{print_name} {csv_event_count:>{len(str(total_event_count))}}/{total_event_count}"
            )
        info_io("\n".join(msg))

        while True:
            # Draw tracking information from queue
            frame_lengths, finished = self._in_q.get()
            # TODO! real crash resilience can only be achieved by updating in the event consumer
            self._tracker.count_subject_events += frame_lengths
            # Track consumer finishes
            if finished:
                done_count += 1
                debug_io(f"Publisher received consumer finished: {done_count}/{self._n_consumers}")
            # Print current state
            msg = [f"Processed event rows: "]
            for csv_name in self._event_file_lengths.keys():
                csv_event_count = self._tracker.count_subject_events[csv_name]
                total_event_count = self._event_file_lengths[csv_name]
                print_name = csv_name.strip('.csv') + ": "
                msg.append(
                    f"{print_name} {csv_event_count:>{len(str(total_event_count))}}/{total_event_count}"
                )

            info_io("\n".join(msg), flush_block=not (int(os.getenv("DEBUG", 0))))
            # Join publisher
            if done_count == self._n_consumers:
                self._tracker.has_subject_events = True
                debug_io("All consumers finished, publisher finishes now.")
                self._in_q.task_done()
                break
            self._in_q.task_done()
        return

    def start(self):
        super().start()
        debug_io("Started publisher")

    def join(self):
        super().join()
        debug_io("Joined publisher")


### FILE: .\src\datasets\extraction\timeseries_processor.py ###
import pandas as pd
from utils.IO import *
from settings import *
from pathlib import Path
from pathos.multiprocessing import cpu_count, Pool
from .extraction_functions import make_timeseries
from ..trackers import ExtractionTracker
from ..writers import DataSetWriter
from ..readers import ExtractedSetReader


class TimeseriesProcessor(object):

    def __init__(self,
                 storage_path: Path,
                 source_path: Path,
                 tracker: ExtractionTracker,
                 subject_ids: list,
                 diagnoses_df: pd.DataFrame,
                 icu_history_df: pd.DataFrame,
                 varmap_df: pd.DataFrame,
                 num_samples: int = None):
        """_summary_

        Args:
            storage_path (Path): _description_
            source_path (Path): _description_
            tracker (ExtractionTracker): _description_
            subject_ids (list): _description_
            diagnoses_df (pd.DataFrame): _description_
            icu_history_df (pd.DataFrame): _description_
            varmap_df (pd.DataFrame): _description_
            num_samples (int, optional): _description_. Defaults to None.
        """
        self._storage_path = storage_path
        self._tracker = tracker
        self._dataset_reader = ExtractedSetReader(source_path)
        self._dataset_writer = DataSetWriter(storage_path)
        if subject_ids is None:
            self._subject_ids = [
                int(folder.name)
                for folder in storage_path.iterdir()
                if folder.is_dir() and folder.name.isnumeric()
            ]
            self._subject_ids.sort()
        else:
            self._subject_ids = subject_ids
        self._num_samples = num_samples
        self._subject_diagnoses = diagnoses_df
        self._subject_icu_history = icu_history_df
        self._varmap_df = varmap_df

    def _store_df_chunk(self, episodic_info_df):
        """_summary_

        Args:
            episodic_info_df (_type_): _description_
        """
        file_path = Path(self._storage_path, "episodic_info_df.csv")
        if not file_path.is_file():
            episodic_info_df.to_csv(file_path, index=False)
        else:
            episodic_info_df.to_csv(file_path, mode='a', header=False, index=False)

    @staticmethod
    def _process_subject(subject_id):
        """_summary_

        Args:
            subject_id (_type_): _description_

        Returns:
            _type_: _description_
        """
        subject_event_df_path = Path(storage_path_pr, str(subject_id), "subject_events.csv")
        subject_event_df = dataset_reader_pr.read_csv(
            subject_event_df_path, dtypes=DATASET_SETTINGS["subject_events"]["dtype"])

        if subject_ids_pr is not None:
            subject_event_df = subject_event_df[subject_event_df["SUBJECT_ID"].isin(subject_ids_pr)]

        if subject_event_df.empty or \
           not subject_id in subject_diagnoses_pr or \
           not subject_id in subject_icu_history_pr:
            return pd.DataFrame(columns=["SUBJECT_ID", "ICUSTAY_ID", "Height", "Weight"])

        curr_subject_event = {subject_id: subject_event_df}
        curr_subject_diagnoses = {subject_id: subject_diagnoses_pr[subject_id]}
        curr_icu_history_pr = {subject_id: subject_icu_history_pr[subject_id]}

        episodic_data, timeseries = make_timeseries(curr_subject_event, curr_subject_diagnoses,
                                                    curr_icu_history_pr, varmap_df_pr)

        # Store processed subject events
        name_data_pairs = {
            "episodic_data": episodic_data,
            "timeseries": timeseries,
        }
        dataset_writer_pr.write_bysubject(name_data_pairs, exists_ok=True)

        info_dfs = list()
        # Return episodic information for compact storage as it is still needed for subsequent processing
        for subject_id, df in episodic_data.items():
            df["SUBJECT_ID"] = subject_id
            df = df.reset_index()
            df = df.rename(columns={"Icustay": "ICUSTAY_ID"})
            info_dfs.append(df[["SUBJECT_ID", "ICUSTAY_ID", "Height", "Weight"]])

        if not info_dfs:
            return pd.DataFrame(columns=["SUBJECT_ID", "ICUSTAY_ID", "Height", "Weight"])

        return pd.concat(info_dfs)

    @staticmethod
    def _init(storage_path: Path, subject_ids: list, diagnoses: dict, icu_history: dict,
              varmap: pd.DataFrame, dataset_reader: ExtractedSetReader,
              dataset_writer: DataSetWriter):
        """_summary_

        Args:
            storage_path (Path): _description_
            subject_ids (list): _description_
            diagnoses (dict): _description_
            icu_history (dict): _description_
            varmap (pd.DataFrame): _description_
            dataset_reader (ExtractedSetReader): _description_
            dataset_writer (DataSetWriter): _description_
        """
        global storage_path_pr
        global subject_ids_pr
        global subject_diagnoses_pr
        global subject_icu_history_pr
        global varmap_df_pr
        global dataset_writer_pr
        global dataset_reader_pr
        storage_path_pr = storage_path
        subject_ids_pr = subject_ids
        subject_diagnoses_pr = diagnoses
        subject_icu_history_pr = icu_history
        varmap_df_pr = varmap
        dataset_reader_pr = dataset_reader
        dataset_writer_pr = dataset_writer

    def run(self):
        """_summary_
        """
        with Pool(cpu_count() - 1,
                  initializer=self._init,
                  initargs=(self._storage_path, self._subject_ids, self._subject_diagnoses,
                            self._subject_icu_history, self._varmap_df, self._dataset_reader,
                            self._dataset_writer)) as pool:
            res = pool.imap_unordered(self._process_subject, self._subject_ids, chunksize=500)

            episodic_info_df = None
            for index, info_df in enumerate(res):
                if episodic_info_df is None:
                    episodic_info_df = info_df
                else:
                    episodic_info_df = pd.concat([episodic_info_df, info_df])

                self._tracker.subject_ids.extend(info_df["SUBJECT_ID"].unique())
                info_io(f"Subject directories extracted: {len(self._tracker.subject_ids)}",
                        end="\r",
                        flush=True)

                if index % 100 == 0 and index != 0:
                    self._store_df_chunk(episodic_info_df)
                    episodic_info_df = None

            if episodic_info_df is not None:
                self._store_df_chunk(episodic_info_df)

            self._tracker.has_episodic_data = True
            self._tracker.has_timeseries = True
        return


### FILE: .\src\datasets\extraction\__init__.py ###
"""
Dataset Extraction Module
=========================

This module provides functions and classes for extracting and processing dataset files.
It supports both compact and iterative extraction methods to handle different use cases.

Functions
---------
- compact_extraction(storage_path, source_path, num_subjects, num_samples, subject_ids, task)
    Performs compact extraction of the dataset.
- iterative_extraction(source_path, storage_path, chunksize, num_subjects, num_samples, subject_ids, task)
    Performs iterative extraction of the dataset.

Examples
--------
>>> from extraction_module import compact_extraction, iterative_extraction
>>> storage_path = Path("/path/to/storage")
>>> source_path = Path("/path/to/source")
>>> compact_extraction(storage_path, source_path, num_subjects=100)
>>> iterative_extraction(source_path, storage_path, chunksize=1000)
"""

import pandas as pd
import yaml
import warnings
import random

warnings.simplefilter(action='ignore', category=FutureWarning)

from copy import deepcopy
from pathlib import Path
from typing import List
from utils.IO import *
from settings import *
from .extraction_functions import make_subject_events, make_timeseries
from .event_producer import EventProducer
from .timeseries_processor import TimeseriesProcessor
from ..trackers import ExtractionTracker
from ..mimic_utils import *
from ..readers import ExtractedSetReader, EventReader
from ..writers import DataSetWriter

__all__ = ["compact_extraction", "iterative_extraction"]


def compact_extraction(storage_path: Path,
                       source_path: Path,
                       num_subjects: int = None,
                       num_samples: int = None,
                       subject_ids: list = None,
                       task: str = None) -> dict:
    """
    Perform single shot extraction of the dataset from the original source. Ensure enough RAM is available.

    Parameters
    ----------
    storage_path : Path
        The path to the storage directory where the extracted data will be saved.
    source_path : Path
        The path to the source directory containing the raw data files.
    num_subjects : int, optional
        The number of subjects to extract. If None, all subjects are extracted. Default is None.
    num_samples : int, optional
        The number of samples to extract. If None, all samples are extracted. Default is None.
    subject_ids : list of int, optional
        List of subject IDs to extract. If None, all subjects are extracted. Default is None.
    task : str, optional
        Specific task to extract data for. If None, all tasks are extracted. Default is None.

    Returns
    -------
    dict
        A dictionary containing the extracted data.

    Examples
    --------
    >>> storage_path = Path("/path/to/storage")
    >>> source_path = Path("/path/to/source")
    >>> dataset = compact_extraction(storage_path, source_path, num_subjects=100)
    """
    original_subject_ids = deepcopy(subject_ids)
    resource_folder = Path(source_path, "resources")
    tracker = ExtractionTracker(storage_path=Path(storage_path, "progress"),
                                num_subjects=num_subjects,
                                num_samples=num_samples,
                                subject_ids=subject_ids)

    dataset_writer = DataSetWriter(storage_path)
    dataset_reader = ExtractedSetReader(storage_path)

    if tracker.is_finished:  # and not compute_data:
        info_io(f"Compact data extraction already finalized in directory:\n{str(storage_path)}")
        if task is not None:
            # Make sure we don't pick empty subjects for the subsequent processing
            icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                     dtypes=convert_dtype_dict(
                                                         DATASET_SETTINGS["icu_history"]["dtype"]))

            subject_ids = get_processable_subjects(task, icu_history_df)
            subject_ids = list(set(subject_ids) & set(tracker.subject_ids))
        else:
            subject_ids = tracker.subject_ids

        if num_subjects is not None and num_subjects < len(subject_ids):
            subject_ids = random.sample(subject_ids, k=num_subjects)
        elif original_subject_ids is not None:
            subject_ids = set(original_subject_ids) & set(subject_ids)
        # If we know some processing is comming after we return all possible subjects for that task
        return dataset_reader.read_subjects(read_ids=True, subject_ids=subject_ids)

    info_io("Compact Dataset Extraction: ALL", level=0)
    info_io(f"Starting compact data extraction.")
    info_io(f"Extracting data from source:\n{str(source_path)}")
    info_io(f"Saving data at location:\n{str(storage_path)}")

    # Read Dataframes for ICU history
    if tracker.has_icu_history:
        info_io("ICU history data already extracted")
        icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                 dtypes=convert_dtype_dict(
                                                     DATASET_SETTINGS["icu_history"]["dtype"]))
        subject_info_df = dataset_reader.read_csv(Path(storage_path, "subject_info.csv"),
                                                  dtypes=convert_dtype_dict(
                                                      DATASET_SETTINGS["subject_info"]["dtype"]))
    else:
        info_io("Extracting ICU history data")
        patients_df = read_patients_csv(source_path)
        admissions_df, admissions_info_df = read_admission_csv(source_path)
        icustays_df = read_icustays_csv(source_path)

        # Make ICU history dataframe
        subject_info_df = make_subject_infos(patients_df, admissions_info_df, icustays_df)
        icu_history_df = make_icu_history(patients_df, admissions_df, icustays_df)
        tracker.has_icu_history = True
        icu_history_df.to_csv(Path(storage_path, "icu_history.csv"), index=False)
        subject_info_df.to_csv(Path(storage_path, "subject_info.csv"), index=False)

    if tracker.has_diagnoses:
        info_io("Patient diagnosis data already extracted")
        diagnoses_df = dataset_reader.read_csv(Path(storage_path, "diagnoses.csv"),
                                               dtypes=convert_dtype_dict(
                                                   DATASET_SETTINGS["diagnosis"]["dtype"]))
    else:
        info_io("Extracting patient diagnosis data")
        # Read Dataframes for diagnoses
        icd9codes_df = read_icd9codes_csv(source_path)

        #
        diagnoses_df, definition_map = make_diagnoses(source_path, icd9codes_df, icu_history_df)
        diagnoses_df.to_csv(Path(storage_path, "diagnoses.csv"), index=False)

        tracker.has_diagnoses = True

    subject_ids, icu_history_df = get_subject_ids(task=task,
                                                  num_subjects=num_subjects,
                                                  subject_ids=subject_ids,
                                                  existing_subjects=tracker.subject_ids,
                                                  icu_history_df=icu_history_df)

    subject_info_df = reduce_by_subjects(subject_info_df, subject_ids)
    diagnoses_df = reduce_by_subjects(diagnoses_df, subject_ids)

    info_io("Extracting subject ICU history")
    subject_icu_history = get_by_subject(icu_history_df,
                                         DATASET_SETTINGS["ICUHISTORY"]["sort_value"])

    info_io("Extracting subject diagnoses")
    subject_diagnoses = get_by_subject(diagnoses_df[DATASET_SETTINGS["DIAGNOSES"]["columns"]],
                                       DATASET_SETTINGS["DIAGNOSES"]["sort_value"])
    if tracker.has_bysubject_info:
        info_io("Subject diagnoses and subject ICU history already stored")
    else:
        dataset_writer.write_bysubject(
            {
                "subject_icu_history": subject_icu_history,
                "subject_diagnoses": subject_diagnoses
            },
            index=False)

    if tracker.has_subject_events:
        info_io("Subject events already extracted")
        subject_events = dataset_reader.read_events(read_ids=True)
    else:
        info_io("Extracting subject events")
        # Read Dataframes for event table
        event_reader = EventReader(source_path)
        chartevents_df = event_reader.get_all()

        # Make subject event table
        subject_events = make_subject_events(chartevents_df, icu_history_df)
        dataset_writer.write_bysubject({
            "subject_events": subject_events,
        }, index=False)
        tracker.has_subject_events = True

    if not tracker.has_timeseries or not tracker.has_episodic_data:
        info_io("Extracting subject timeseries and episodic data")
        # Read Dataframes for time series
        varmap_df = read_varmap_csv(resource_folder)
        episodic_data, timeseries = make_timeseries(subject_events, subject_diagnoses,
                                                    subject_icu_history, varmap_df)
        name_data_pair = {"episodic_data": episodic_data, "timeseries": timeseries}
        dataset_writer.write_bysubject(name_data_pair, exists_ok=True)

        tracker.subject_ids.extend(list(timeseries.keys()))
        tracker.has_episodic_data = True
        tracker.has_timeseries = True
    else:
        info_io("Subject timeseries and episodic data already extracted")

    tracker.is_finished = True
    info_io(f"Finalized data extraction in directory:\n{str(storage_path)}")
    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return dataset_reader.read_subjects(read_ids=True, subject_ids=original_subject_ids)


def iterative_extraction(source_path: Path,
                         storage_path: Path = None,
                         chunksize: int = None,
                         num_subjects: int = None,
                         num_samples: int = None,
                         subject_ids: list = None,
                         task: str = None) -> ExtractedSetReader:
    """
    Perform iterative extraction of the dataset, with specified chunk size. This will require less RAM and run on multiple processes.

    Parameters
    ----------
    source_path : Path
        The path to the source directory containing the raw data files.
    storage_path : Path, optional
        The path to the storage directory where the extracted data will be saved. Default is None.
    chunksize : int, optional
        The size of chunks to read at a time. Default is None.
    num_subjects : int, optional
        The number of subjects to extract. If None, all subjects are extracted. Default is None.
    num_samples : int, optional
        The number of samples to extract. If None, all samples are extracted. Default is None.
    subject_ids : list of int, optional
        List of subject IDs to extract. If None, all subjects are extracted. Default is None.
    task : str, optional
        Specific task to extract data for. If None, all tasks are extracted. Default is None.

    Returns
    -------
    ExtractedSetReader
        The extracted set reader to access the dataset.

    Examples
    --------
    >>> source_path = Path("/path/to/source")
    >>> storage_path = Path("/path/to/storage")
    >>> reader = iterative_extraction(source_path, storage_path, chunksize=1000, num_subjects=100)
    """
    # Not sure
    original_subject_ids = deepcopy(subject_ids)
    resource_folder = Path(source_path, "resources")

    tracker = ExtractionTracker(storage_path=Path(storage_path, "progress"),
                                num_samples=num_samples,
                                num_subjects=num_subjects,
                                subject_ids=subject_ids)

    dataset_writer = DataSetWriter(storage_path)
    dataset_reader = ExtractedSetReader(source_path)

    if tracker.is_finished:
        info_io(f"Iterative data extraction already finalized in directory:\n{storage_path}.")
        if task is not None:
            # Make sure we don't pick empty subjects for the subsequent processing
            icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                     dtypes=convert_dtype_dict(
                                                         DATASET_SETTINGS["icu_history"]["dtype"]))

            subject_ids = get_processable_subjects(task, icu_history_df)
            subject_ids = list(set(subject_ids) & set(tracker.subject_ids))
        else:
            subject_ids = tracker.subject_ids

        if num_subjects is not None and num_subjects < len(subject_ids):
            subject_ids = random.sample(subject_ids, k=num_subjects)
        elif original_subject_ids is not None:
            subject_ids = set(original_subject_ids) & set(subject_ids)
        # If we know some processing is comming after we return all possible subjects for that task
        return ExtractedSetReader(storage_path, subject_ids=subject_ids)

    info_io("Iterative Dataset Extraction: ALL", level=0)
    info_io(f"Starting iterative data extraction.")
    info_io(f"Extracting data from source:\n{str(source_path)}")
    info_io(f"Saving data at location:\n{str(storage_path)}")

    # Make ICU history dataframe
    if tracker.has_icu_history:
        info_io("ICU history data already extracted")
        icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                 dtypes=convert_dtype_dict(
                                                     DATASET_SETTINGS["icu_history"]["dtype"]))
        subject_info_df = dataset_reader.read_csv(Path(storage_path, "subject_info.csv"),
                                                  dtypes=convert_dtype_dict(
                                                      DATASET_SETTINGS["subject_info"]["dtype"]))
    else:
        # Read Dataframes for ICU history
        info_io("Extracting ICU history data")
        patients_df = read_patients_csv(source_path)

        admissions_df, admission_info_df = read_admission_csv(source_path)
        icustays_df = read_icustays_csv(source_path)
        # Generate history
        subject_info_df = make_subject_infos(patients_df, admission_info_df, icustays_df)
        icu_history_df = make_icu_history(patients_df, admissions_df, icustays_df)

        # TODO! encapsualte in function
        icu_history_df.to_csv(Path(storage_path, "icu_history.csv"), index=False)
        subject_info_df.to_csv(Path(storage_path, "subject_info.csv"), index=False)
        tracker.has_icu_history = True

    # Read Dataframes for diagnoses

    if tracker.has_diagnoses:
        info_io("Patient diagnosis data already extracted")
        diagnoses_df = dataset_reader.read_csv(Path(storage_path, "diagnoses.csv"),
                                               dtypes=convert_dtype_dict(
                                                   DATASET_SETTINGS["diagnosis"]["dtype"]))
    else:
        info_io("Extracting Patient diagnosis data")

        icd9codes_df = read_icd9codes_csv(source_path)
        diagnoses_df, definition_map = make_diagnoses(source_path, icd9codes_df, icu_history_df)
        # TODO! not working
        # make_phenotypes(diagnoses_df,
        #                definition_map).to_csv(Path(storage_path, "phenotype_matrix.csv"))
        diagnoses_df.to_csv(Path(storage_path, "diagnoses.csv"), index=False)
        tracker.has_diagnoses = True

    subject_ids, icu_history_df = get_subject_ids(task=task,
                                                  num_subjects=num_subjects,
                                                  subject_ids=subject_ids,
                                                  existing_subjects=tracker.subject_ids,
                                                  icu_history_df=icu_history_df)

    diagnoses_df = reduce_by_subjects(diagnoses_df, subject_ids)
    subject_diagnoses = get_by_subject(diagnoses_df[DATASET_SETTINGS["DIAGNOSES"]["columns"]],
                                       DATASET_SETTINGS["DIAGNOSES"]["sort_value"])
    subject_icu_history = get_by_subject(icu_history_df,
                                         DATASET_SETTINGS["ICUHISTORY"]["sort_value"])
    if not tracker.has_subject_events:
        name_data_pairs = {
            "subject_diagnoses": {
                subject_id: frame_df for subject_id, frame_df in subject_diagnoses.items()
            },
            "subject_icu_history": {
                subject_id: frame_df for subject_id, frame_df in subject_icu_history.items()
            }
        }
        dataset_writer.write_bysubject(name_data_pairs, index=False)
    else:
        info_io("Subject diagnoses and subject ICU history already stored")

    if not tracker.has_subject_events:
        info_io("Extracting subject events")

        EventProducer(source_path=source_path,
                      storage_path=storage_path,
                      num_samples=num_samples,
                      chunksize=chunksize,
                      tracker=tracker,
                      icu_history_df=icu_history_df,
                      subject_ids=subject_ids).run()
    else:
        info_io("Subject events already extracted")

    varmap_df = read_varmap_csv(resource_folder)

    if not tracker.has_episodic_data or not tracker.has_timeseries:
        info_io("Extraction timeseries data from subject events")

        # Starting the processor pool
        pool_processor = TimeseriesProcessor(storage_path=storage_path,
                                             source_path=source_path,
                                             tracker=tracker,
                                             subject_ids=subject_ids,
                                             diagnoses_df=subject_diagnoses,
                                             icu_history_df=subject_icu_history,
                                             varmap_df=varmap_df,
                                             num_samples=num_samples)

        pool_processor.run()
        info_io(f"Subject directories extracted: {len(tracker.subject_ids)}")

        tracker.has_episodic_data = True
        tracker.has_timeseries = True
    else:
        info_io(f"Timeseries data already created")

    tracker.is_finished = True
    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return ExtractedSetReader(storage_path, subject_ids=original_subject_ids)


def reduce_by_subjects(dataframe: pd.DataFrame, subject_ids: list):
    """
    Reduce the dataframe to only include the specified subject IDs.

    Parameters
    ----------
    dataframe : pd.DataFrame
        The dataframe to reduce.
    subject_ids : list
        The list of subject IDs to retain in the dataframe.

    Returns
    -------
    pd.DataFrame
        The reduced dataframe containing only the specified subject IDs.

    Examples
    --------
    >>> df = pd.DataFrame({"SUBJECT_ID": [10006, 10011, 10019], "value": [10, 20, 30]})
    >>> reduce_by_subjects(df, [10006, 10019])
       SUBJECT_ID  value
    0           10006     10
    2           10019     30
    """
    if subject_ids is not None:
        return dataframe[dataframe["SUBJECT_ID"].isin(subject_ids)]
    return dataframe


def get_subject_ids(task: str,
                    icu_history_df: pd.DataFrame,
                    subject_ids: list = None,
                    num_subjects: int = None,
                    existing_subjects: list = None):

    """
    Get the subject IDs that can be processed for the given task. Many subjects will need to be
    discarded as they do not fulfill the minimum length of stay requirement of 48H for IHM, 4H for DECOMP and LOS.

    Parameters
    ----------
    task : str
        The task for which to get the subject IDs.
    icu_history_df : pd.DataFrame
        The dataframe containing ICU history information.
    subject_ids : list of int, optional
        List of subject IDs to extract. If None, all subjects are extracted. Default is None.
    num_subjects : int, optional
        The number of subjects to extract. If None, all subjects are extracted. Default is None.
    existing_subjects : list of int, optional
        List of existing subjects to exclude from extraction. Default is None.

    Returns
    -------
    tuple
        A tuple containing the list of subject IDs and the reduced ICU history dataframe.
    """
    if existing_subjects is None:
        existing_subjects = []

    if subject_ids is not None:
        all_subjects = get_processable_subjects(task, icu_history_df)
        # Notify unknowns
        unknown_subjects = set(subject_ids) - set(icu_history_df["SUBJECT_ID"].unique())
        if unknown_subjects:
            warn_io(f"Unknown subjects passed as parameter: {*unknown_subjects,}")
        # Notify unprocessable
        unprocessable_subjects = set(subject_ids) - set(all_subjects)
        if unprocessable_subjects:
            warn_io(f"Unprocessable subjects passed as parameter: {*unprocessable_subjects,}")
        # Remove already processed
        subject_ids = list((set(subject_ids) - set(existing_subjects)) & set(all_subjects))
        icu_history_df = reduce_by_subjects(icu_history_df, subject_ids)
    elif num_subjects is not None:
        all_subjects = get_processable_subjects(task, icu_history_df)
        subject_ids = get_subjects_by_number(task, num_subjects, existing_subjects, all_subjects)
        icu_history_df = reduce_by_subjects(icu_history_df, subject_ids)
    else:
        subject_ids = None
    return subject_ids, icu_history_df


def get_subjects_by_number(task: str, num_subjects: int, existing_subjects: List[int],
                           all_subjects: List[int]):
    """
    Get a specified number of subject IDs, excluding the existing subjects.
    """
    # Determined how many are missing
    num_subjects = max(0, num_subjects - len(existing_subjects))
    # Chose from uprocessed subjects
    if num_subjects > len(all_subjects):
        raise warn_io(
            f"Number of subjects requested exceeds available subjects: {len(all_subjects)}")
    remaining_subjects = list(set(all_subjects) - set(existing_subjects))
    # if tracker is None we grab all possible subjects for return to the next processing step
    subject_ids = random.sample(remaining_subjects, k=num_subjects)
    assert len(subject_ids) == num_subjects
    return subject_ids


def get_processable_subjects(task: str, icu_history_df: pd.DataFrame):
    """
    Get the subject IDs that can be processed for a given task.
    """
    if task is not None and "label_start_time" in DATASET_SETTINGS[task]:
        # Some ids will be removed during the preprocessing step
        # We remove them here to avoid errors
        min_los = DATASET_SETTINGS[task]["label_start_time"] + \
            DATASET_SETTINGS[task]["sample_precision"]
        min_los /= 24
        icu_history_df = icu_history_df[icu_history_df["LOS"] >= min_los]
        return icu_history_df[((icu_history_df["DISCHTIME"] - icu_history_df["ADMITTIME"])
                               >= pd.Timedelta(days=min_los))]["SUBJECT_ID"].unique()
    else:
        return icu_history_df["SUBJECT_ID"].unique()


def create_split_info_csv(episodic_info_df: pd.DataFrame, subject_info_df: pd.DataFrame):
    """Create the information used by the dataset split function to split the subjects into demographics groups.
    """
    episodic_info_df["SUBJECT_ID"] = episodic_info_df["SUBJECT_ID"].astype(int)
    episodic_info_df = episodic_info_df.merge(subject_info_df,
                                              how='inner',
                                              left_on=['SUBJECT_ID', 'ICUSTAY_ID'],
                                              right_on=['SUBJECT_ID', 'ICUSTAY_ID'])
    episodic_info_df.to_csv("split_info.csv")

    return


def read_patients_csv(dataset_folder: Path):
    """
    Parameters:
        dataset_folder:     If not default dataset path at data/mimic-iii-demo/

    Returns:
        patients_df:        Patients data (birth, death, gender, ethnicity etc.)
    """
    csv_settings = DATASET_SETTINGS["PATIENTS"]
    patients_df = pd.read_csv(Path(dataset_folder, "PATIENTS.csv"),
                              dtype=convert_dtype_dict(csv_settings["dtype"]))

    patients_df = upper_case_column_names(patients_df)
    patients_df = patients_df[csv_settings["columns"]].copy()

    for column in csv_settings["convert_datetime"]:
        patients_df[column] = pd.to_datetime(patients_df[column])

    return patients_df


def read_admission_csv(dataset_folder: Path):
    """
    Parameters:
        dataset_folder:     If not default dataset path at data/mimic-iii-demo/

    Returns:
        admissions_df:      Hospital admissions data (admission, discharge, type, location, etc.)
    """
    csv_settings = DATASET_SETTINGS["ADMISSIONS"]

    admissions_df = pd.read_csv(Path(dataset_folder, "ADMISSIONS.csv"),
                                dtype=convert_dtype_dict(csv_settings["dtype"]))
    admissions_df = upper_case_column_names(admissions_df)
    for column in csv_settings["convert_datetime"]:
        admissions_df[column] = pd.to_datetime(admissions_df[column])

    admissions_info_df = admissions_df[csv_settings["info_columns"]]
    admissions_df = admissions_df[csv_settings["columns"]].copy()

    return admissions_df, admissions_info_df


def read_icustays_csv(dataset_folder: Path):
    """
    Parameters:
        dataset_folder:     If not default dataset path at data/mimic-iii-demo/

    Returns:
        icustays_df:        ICU admission data (firt & last care unit, ward id, etc.)
    """
    csv_settings = DATASET_SETTINGS["ICUSTAYS"]

    icustays_df = pd.read_csv(Path(dataset_folder, "ICUSTAYS.csv"),
                              dtype=convert_dtype_dict(csv_settings["dtype"]))
    icustays_df = upper_case_column_names(icustays_df)
    for column in csv_settings["convert_datetime"]:
        icustays_df[column] = pd.to_datetime(icustays_df[column])

    return icustays_df


def read_icd9codes_csv(dataset_folder: Path):
    """
    Parameters:
        dataset_folder:     If not default dataset path at data/mimic-iii-demo/

    Returns:
        icd9codes_df:       Dictionaries describing the ICD9 codes
    """
    csv_settings = DATASET_SETTINGS["ICD9CODES"]

    icd9codes_df = pd.read_csv(Path(dataset_folder, 'D_ICD_DIAGNOSES.csv'),
                               dtype=convert_dtype_dict(csv_settings["dtype"]))
    icd9codes_df = upper_case_column_names(icd9codes_df)
    icd9codes_df = icd9codes_df[csv_settings["columns"]]

    return icd9codes_df


def read_events_dictionary(dataset_folder: Path):
    """_summary_

    Args:
        dataset_folder (_type_): _description_
    """
    csv_settings = DATASET_SETTINGS["D_ITEMS"]
    dictionary_df = pd.read_csv(Path(dataset_folder, "D_ITEMS.csv"),
                                dtype=convert_dtype_dict(csv_settings["dtype"]))
    dictionary_df = upper_case_column_names(dictionary_df)
    dictionary_df = dictionary_df[["ITEMID", "DBSOURCE"]]

    return dictionary_df


def merge_patient_history(patients_df, admissions_df, icustays_df, min_nb_stays,
                          max_nb_stays) -> pd.DataFrame:
    """_summary_

    Args:
        patients_df (_type_): _description_
        admissions_df (_type_): _description_
        icustays_df (_type_): _description_
        min_nb_stays (_type_): _description_
        max_nb_stays (_type_): _description_
    """
    icustays_df = icustays_df[icustays_df["FIRST_CAREUNIT"] == icustays_df["LAST_CAREUNIT"]]
    icustays_df = icustays_df[icustays_df["FIRST_WARDID"] == \
                                    icustays_df["LAST_WARDID"]]

    patient_history_df = icustays_df.merge(admissions_df,
                                           how='inner',
                                           left_on=['SUBJECT_ID', 'HADM_ID'],
                                           right_on=['SUBJECT_ID', 'HADM_ID'])
    patient_history_df = patient_history_df.merge(patients_df,
                                                  how='inner',
                                                  left_on=['SUBJECT_ID'],
                                                  right_on=['SUBJECT_ID'])

    filter = patient_history_df.groupby('HADM_ID').count()[['ICUSTAY_ID']].reset_index()
    filter = filter.loc[(filter.ICUSTAY_ID >= min_nb_stays)
                        & (filter.ICUSTAY_ID <= max_nb_stays)][['HADM_ID']]
    patient_history_df = patient_history_df.merge(filter,
                                                  how='inner',
                                                  left_on='HADM_ID',
                                                  right_on='HADM_ID')

    patient_history_df['AGE'] = (patient_history_df.INTIME.dt.year - patient_history_df.DOB.dt.year)
    patient_history_df.loc[patient_history_df.AGE < 0, 'AGE'] = 90
    # Filter out children
    # patient_history_df = patient_history_df[patient_history_df.AGE > 18]

    return patient_history_df


def make_subject_infos(patients_df,
                       admission_info_df,
                       icustays_df,
                       min_nb_stays=1,
                       max_nb_stays=1) -> pd.DataFrame:
    csv_settings = DATASET_SETTINGS["subject_info"]
    subject_info_df = merge_patient_history(patients_df, admission_info_df, icustays_df,
                                            min_nb_stays, max_nb_stays)
    subject_info_df = subject_info_df[csv_settings["columns"]]
    subject_info_df = subject_info_df.rename(columns={
        "FIRST_WARDID": "WARDID",
        "FIRST_CAREUNIT": "CAREUNIT"
    })

    return subject_info_df


def make_icu_history(patients_df,
                     admissions_df,
                     icustays_df,
                     min_nb_stays=1,
                     max_nb_stays=1) -> pd.DataFrame:
    """
    Parameters:
        patients_df:        patients data
        admissions_df:      hospital admissions data
        icustays_df:        ICU stay data

    Returns:
        icu_history_df:     Description of each ICU stay with admission data, patients data and mortality
    """
    csv_settings = DATASET_SETTINGS["icu_history"]
    icu_history_df = merge_patient_history(patients_df, admissions_df, icustays_df, min_nb_stays,
                                           max_nb_stays)

    # Inunit mortality
    mortality = icu_history_df.DOD.notnull() & ((icu_history_df.INTIME <= icu_history_df.DOD) &
                                                (icu_history_df.OUTTIME >= icu_history_df.DOD))

    mortality = mortality | (icu_history_df.DEATHTIME.notnull() &
                             ((icu_history_df.INTIME <= icu_history_df.DEATHTIME) &
                              (icu_history_df.OUTTIME >= icu_history_df.DEATHTIME)))
    icu_history_df['MORTALITY_INUNIT'] = mortality.astype(int)

    # Inhospital mortality
    mortality = icu_history_df.DOD.notnull() & ((icu_history_df.ADMITTIME <= icu_history_df.DOD) &
                                                (icu_history_df.DISCHTIME >= icu_history_df.DOD))
    mortality = mortality | (icu_history_df.DEATHTIME.notnull() &
                             ((icu_history_df.ADMITTIME <= icu_history_df.DEATHTIME) &
                              (icu_history_df.DISCHTIME >= icu_history_df.DEATHTIME)))
    icu_history_df['MORTALITY'] = mortality.astype(int)
    icu_history_df['MORTALITY_INHOSPITAL'] = mortality.astype(int)

    icu_history_df = icu_history_df[csv_settings["columns"]]
    icu_history_df = icu_history_df[icu_history_df.AGE >= 18]

    return icu_history_df


def make_diagnoses(dataset_folder, icd9codes_df, icu_history_df):
    """
    Parameters:
        dataset_folder:     If not default dataset path at data/mimic-iii-demo/
        icd9codes_df:       Diagnoses item ID defintions
        icu_history_df:     ICU stays descriptions

    Returns:
        diagnoses_df:       Dataframe containing description of diagnoses with direct link to subject and ICU stay
    """
    csv_settings = DATASET_SETTINGS["DIAGNOSES_ICD"]
    diagnoses_df = pd.read_csv(Path(dataset_folder, 'DIAGNOSES_ICD.csv'),
                               dtype=convert_dtype_dict(csv_settings["dtype"]))
    diagnoses_df = upper_case_column_names(diagnoses_df)
    diagnoses_df = diagnoses_df.merge(icd9codes_df,
                                      how='inner',
                                      left_on='ICD9_CODE',
                                      right_on='ICD9_CODE')
    diagnoses_df = diagnoses_df.merge(icu_history_df[['SUBJECT_ID', 'HADM_ID',
                                                      'ICUSTAY_ID']].drop_duplicates(),
                                      how='inner',
                                      left_on=['SUBJECT_ID', 'HADM_ID'],
                                      right_on=['SUBJECT_ID', 'HADM_ID'])

    diagnoses_df[['SUBJECT_ID', 'HADM_ID',
                  'SEQ_NUM']] = diagnoses_df[['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM']].astype(int)

    # Clinical Classifications Software CSS defintions of phenotypes
    with Path(dataset_folder, "resources", "hcup_ccs_2015_definitions.yaml").open("r") as file:
        phenotypes_yaml = yaml.full_load(file)

    # Create definition map
    definition_map = dict()

    for phenotype in phenotypes_yaml:
        for code in phenotypes_yaml[phenotype]['codes']:
            definition_map[code] = (phenotype, phenotypes_yaml[phenotype]['use_in_benchmark'])

    diagnoses_df['HCUP_CCS_2015'] = diagnoses_df.ICD9_CODE.apply(lambda c: definition_map[c][0]
                                                                 if c in definition_map else None)
    diagnoses_df['USE_IN_BENCHMARK'] = diagnoses_df.ICD9_CODE.apply(
        lambda c: int(definition_map[c][1]) if c in definition_map else None)

    return diagnoses_df, definition_map


def make_phenotypes(diagnoses_df, definition_map):
    """
    Parameters:
        diagnoses_df:       Diagnoses descriptions with ICU stay information
        resource_folder:    If not default dataset path at data/mimic-iii-demo/resources/

    Returns:
        phenotypes_df:      Binary matrix with diagnoses over ICU stays
    """
    # Merge definitions to diagnoses
    phenotype_dictionary_df = pd.DataFrame(definition_map).T.reset_index().rename(columns={
        'index': 'ICD9_CODE',
        0: 'HCUP_CCS_2015',
        1: 'USE_IN_BENCHMARK'
    })
    phenotype_dictionary_df['use_in_benchmark'] = phenotype_dictionary_df[
        'use_in_benchmark'].astype(int)

    phenotypes_df = diagnoses_df.merge(phenotype_dictionary_df,
                                       how='inner',
                                       left_on='ICD9_CODE',
                                       right_on='ICD9_CODE')

    # Extract phenotypes from diagnoses
    phenotypes_df = phenotypes_df[['ICUSTAY_ID', 'HCUP_CCS_2015'
                                  ]][phenotypes_df.use_in_benchmark > 0].drop_duplicates()
    phenotypes_df['VALUE'] = 1

    # Definitions again icu stays
    phenotypes_df = phenotypes_df.pivot(index='icuystay_id',
                                        columns='HCUP_CCS_2015',
                                        values='VALUE')

    # Impute values and sort axes
    phenotypes_df = phenotypes_df.fillna(0).astype(int).sort_index(axis=0).sort_index(axis=1)

    return phenotypes_df


def get_by_subject(df, sort_by):
    """
    Parameters:
        df:         Dataframe with subject_id columns
        sort_by:    Column by which to sort the subdataframes

    Returns:
        subject_events:     Dictionary containing events by subject_id
    """
    return {id: x for id, x in df.sort_values(by=sort_by).groupby('SUBJECT_ID')}


if __name__ == "__main__":
    # subject_groups("/home/amadou/Data/ml_data/research-internship/mimic-iii-demo")
    iterative_extraction(
        storage_path=Path("/home/amadou/CodeWorkspace/data/research-internship/processed-trials"),
        source_path=Path("/home/amadou/CodeWorkspace/data/mimic-iii-demo"),
        ehr=None,
        from_storage=False,
        chunksize=10000,
        num_subjects=None)


### FILE: .\src\datasets\split\splitters.py ###
import random
import numpy as np
import pandas as pd
from typing import List, Dict
from pathlib import Path
from utils.IO import *
from datasets.readers import ProcessedSetReader, SplitSetReader
from sklearn import model_selection
from pathlib import Path
from datasets.trackers import DataSplitTracker, PreprocessingTracker
from pathos.multiprocessing import Pool, cpu_count
from utils import dict_subset
from collections import OrderedDict
from itertools import chain


class AbstractSplitter(object):

    def __init__(self, max_iter: int = 100, tolerance: float = 0.005) -> None:
        self._max_iter = max_iter
        self._tolerance = tolerance

    @staticmethod
    def _print_ratio(prefix: str, ratio: dict):
        message = [prefix + ":"]
        set_names = ["test", "val", "train"]
        for set_name in set_names:
            if set_name in ratio and ratio[set_name]:
                set_string = f" {set_name} size: ".ljust(12)
                message.append(f"{set_string}{ratio[set_name]:0.3f}")
        info_io("\n".join(message))

    def _reduce_by_ratio(self,
                         subjects: Dict[str, List[int]],
                         sample_counts: dict,
                         test_size: float = 0.0,
                         val_size: float = 0.0):
        # If val_size is specified but no val subjects are present raise an error
        if val_size and (not "val" in subjects or not subjects["val"]):
            raise ValueError(f"'val_size' parameter specified but no val subjects "
                             f"in reduce by ratio function! Val size is: {val_size}")
        # Same for test
        if test_size and (not "test" in subjects or not subjects["test"]):
            raise ValueError(f"'val_size' parameter specified but no val subjects "
                             f"in reduce by ratio function! Val size is: {val_size}")

        # Remember target ratios
        target_ratios = dict()
        if test_size and "test" in subjects:
            target_ratios["test"] = test_size
        if val_size and "val" in subjects:
            target_ratios["val"] = val_size
        if test_size and "train" in subjects:
            target_ratios["train"] = 1 - test_size - val_size

        ratio_df = self._create_ratio_df(list(chain.from_iterable(subjects.values())),
                                         sample_counts)

        processed_split_names = list()
        input_ratios = {
            set_name: ratio_df[ratio_df["participant"].isin(split_subject)]["ratio"].sum()
            for set_name, split_subject in subjects.items()
        }

        # Deviation from target ratios
        target_to_input_ratios = OrderedDict({
            set_name: input_ratios[set_name] / target_ratios[set_name]
            for set_name in target_ratios
            if not set_name in processed_split_names
        })

        # Reduction based on the smallest input/target ratio (smalles set)
        target_to_input_ratios = OrderedDict(
            sorted(target_to_input_ratios.items(), key=lambda item: item[1]))
        base = target_to_input_ratios.popitem(last=False)[0]
        base_len = ratio_df[ratio_df["participant"].isin(subjects[base])]["ratio"].sum()
        real_ratios = dict()

        # Length once reduced
        new_length = base_len * (
            1 + sum([target_ratios[name] for name in target_to_input_ratios]) / target_ratios[base])
        ratio_df["ratio"] *= 1 / new_length

        for set_name in target_to_input_ratios:
            if not (1 - target_ratios[set_name]):
                continue

            subjects[set_name], _ = self._subjects_for_ratio(
                ratio_df[ratio_df["participant"].isin(subjects[set_name])], target_ratios[set_name])

            processed_split_names.append(set_name)

        real_ratios = {
            set_name: ratio_df[ratio_df["participant"].isin(split_subject)]["ratio"].sum()
            for set_name, split_subject in subjects.items()
        }
        return subjects, real_ratios

    def _check_settings(self, settings, parent_key='', set_name=None):
        """
        Recursively checks if any setting in a nested dictionary is empty.
        Raises ValueError if an empty setting is found.

        :param settings: dict, the settings dictionary to check
        :param parent_key: str, used for recursive tracking of keys
        """
        for key, value in settings.items():
            # Construct the full key path for better error messages
            full_key = f"{parent_key}.{key}" if parent_key else key

            # Check if the value is a dictionary and if so, recurse into it
            if isinstance(value, dict) and value:
                self._check_settings(value, full_key)
            else:
                # Check if the value is considered empty (covers empty lists, None, empty strings, etc.)
                if not value:  # This checks for empty lists, None, empty strings, and other "falsy" values
                    error_msg = ""
                    if set_name:
                        error_msg += f"For set '{set_name}': "
                    error_msg += f"{full_key} setting is empty"
                    raise ValueError(error_msg)

    def _split_by_demographics(self, subject_ids: List[int], source_path: Path,
                               demographic_split: dict):
        return_ratios = dict()
        return_subjects = dict()

        # Set wise demographics config for test, val and train
        for set_name, setting in demographic_split.items():
            assert set_name in ["test", "val", "train"], "Invalid split attribute"
            self._check_settings(setting, set_name=set_name)
            curr_subject_ids = self._get_demographics(set_name.capitalize(), source_path,
                                                      subject_ids, setting)
            return_ratios[set_name] = len(curr_subject_ids) / len(subject_ids)
            return_subjects[set_name] = curr_subject_ids

        # If not train specified select the substraction of the other sets
        if not "train" in demographic_split:
            train_subjects = set(subject_ids)
            prefix = "Train"
            for set_name, setting in demographic_split.items():
                train_subjects.intersection_update(
                    self._get_demographics(prefix, source_path, subject_ids, setting, invert=True))
                prefix = ""
            return_subjects["train"] = list(train_subjects)
            return_ratios["train"] = len(train_subjects) / len(subject_ids)

        return return_subjects, return_ratios

    def _create_ratio_df(self, subject_ids: List[int], sample_counts: dict):
        subject_ratios = [
            (subject_id, sample_counts[subject_id]["total"]) for subject_id in subject_ids
        ]
        ratio_df = pd.DataFrame(subject_ratios, columns=['participant', 'ratio'])
        total_len = ratio_df["ratio"].sum()
        ratio_df["ratio"] = ratio_df["ratio"] / total_len
        ratio_df = ratio_df.sort_values('ratio')
        return ratio_df

    def _split_by_ratio(self,
                        subject_ids: List[int],
                        sample_counts: dict,
                        test_size: float = 0.0,
                        val_size: float = 0.0):
        if test_size < 0 or test_size > 1:
            raise ValueError("Invalid test size")
        if val_size < 0 or val_size > 1:
            raise ValueError("Invalid val size")
        return_ratios = dict()
        return_subjects = dict()
        ratio_df = self._create_ratio_df(subject_ids, sample_counts)

        train_subjects = set(subject_ids)

        def create_split(total_subjects, ratio_df, size):
            """_summary_
            """
            subjects, split_ratio = self._subjects_for_ratio(ratio_df, size)
            remaining_subjects = total_subjects - set(subjects)
            updated_ratio_df = ratio_df[~ratio_df.participant.isin(subjects)]
            return list(subjects), remaining_subjects, updated_ratio_df, split_ratio

        if test_size:
            return_subjects["test"], \
            train_subjects, \
            ratio_df, \
            return_ratios["test"] = create_split(total_subjects=train_subjects,
                                                 ratio_df=ratio_df,
                                                 size=test_size)
        if val_size:
            return_subjects["val"],\
            train_subjects, \
            ratio_df, \
            return_ratios["val"] = create_split(total_subjects=train_subjects,
                                                ratio_df=ratio_df,
                                                size=val_size)

        if train_subjects:
            return_subjects["train"] = list(train_subjects)[:]
            return_ratios["train"] = 1 - sum(list(return_ratios.values()))

        return return_subjects, return_ratios

    def _get_demographics(self,
                          prefix: str,
                          source_path: Path,
                          subject_ids: list,
                          settings: dict,
                          invert=False):
        if source_path is None or settings is None:
            return subject_ids
        self._check_settings(settings)
        subject_info_df = pd.read_csv(Path(source_path, "subject_info.csv"))
        subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(subject_ids)]

        if prefix is not None and prefix:
            message = [prefix + ":"]
        else:
            message = []

        def get_subjects(condition: pd.Series):
            return subject_info_df[condition]["SUBJECT_ID"].unique()

        def get_categorical_message(choice):
            choice = list(choice)
            if len(choice) > 1:
                return f"is one of " + ", ".join(
                    str(entry) for entry in choice[:-1]) + " or " + str(choice[-1])
            else:
                return f"is {choice[0]}"

        def check_setting(setting, attribute):
            if not attribute in subject_info_df.columns:
                raise ValueError(f"Invalid demographic. Choose from {*subject_info_df.columns,}\n"
                                 f"Demographic is: {attribute}")
            if "geq" in setting:
                check_range(setting["geq"], setting)
                assert not "greater" in setting, "Invalid setting, cannot have both less and leq"

            if "greater" in setting:
                check_range(setting["greater"], setting)

            if "leq" in setting and "less" in setting:
                raise ValueError("Invalid setting, cannot have both less and leq")

            if ("geq" in setting or "leq" in setting or "less" in setting or
                    "greater" in setting) and "choice" in setting:
                raise ValueError("Invalid setting, cannot have both range and choice")

        def check_range(greater_value, setting):
            for key in ["leq", "less"]:
                if key in setting:
                    if greater_value > setting[key]:
                        raise ValueError(
                            f"Invalid range: greater={greater_value} > leq={setting[key]}")

        if invert:
            exclude_subjects = set(subject_ids)
        else:
            exclude_subjects = set()

        for attribute, setting in settings.items():
            attribute_message = " "
            check_setting(setting, attribute)
            attribute_data = subject_info_df[attribute]

            if "geq" in setting:
                # We use this reversed logic to avoid including any subjects where on stay
                # may fail the specification
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data >= setting["geq"]))
                    attribute_message += f"{setting['geq']:0.3f} > "
                else:
                    exclude_subjects.update(get_subjects(attribute_data < setting["geq"]))
                    attribute_message += f"{setting['geq']:0.3f} =< "

            if "greater" in setting:
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data > setting["greater"]))
                    attribute_message += f"{setting['greater']:0.3f} >= "
                else:
                    exclude_subjects.update(get_subjects(attribute_data <= setting["greater"]))
                    attribute_message += f"{setting['greater']:0.3f} < "

            if invert and ("geq" in setting or "greater" in setting) and\
                          ("leq" in setting or "less" in setting):
                attribute_message += f"{attribute} or "
            elif invert and not ("geq" in setting or "greater" in setting) and \
                                ("leq" in setting or "less" in setting):
                pass
            else:
                attribute_message += f"{attribute} "

            if "leq" in setting:
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data < setting["leq"]))
                    attribute_message += f"{setting['leq']:0.3f} < {attribute}"
                else:
                    exclude_subjects.update(get_subjects(attribute_data >= setting["leq"]))
                    attribute_message += f"<= {setting['leq']:0.3f}"

            if "less" in setting:
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data < setting["less"]))
                    attribute_message += f"{setting['less']:0.3f} <= {attribute}"
                else:
                    exclude_subjects.update(get_subjects(attribute_data >= setting["less"]))
                    attribute_message += f"< {setting['less']:0.3f}"

            if "choice" in setting:
                categories = attribute_data.unique()
                not_choices = set(categories) - set(setting["choice"])
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data.isin(setting["choice"])))
                    attribute_message += get_categorical_message(not_choices)
                else:
                    exclude_subjects.update(get_subjects(attribute_data.isin(not_choices)))
                    attribute_message += get_categorical_message(setting["choice"])

            message.append(attribute_message)

        if prefix is not None:
            info_io("\n".join(message))
        subject_info_df = subject_info_df[~subject_info_df["SUBJECT_ID"].isin(exclude_subjects)]
        return subject_info_df["SUBJECT_ID"].unique().tolist()

    def _subjects_for_ratio(self, ratio_df: pd.DataFrame, target_size: float):
        """_summary_

        Args:
            ratio_df (pd.DataFrame): _description_
            target_size (float): _description_

        Returns:
            _type_: _description_
        """
        assert "participant" in ratio_df.columns
        assert "ratio" in ratio_df.columns
        best_diff = 1e18
        iter = 0

        def compute_ratios(random_state):
            current_size = 0
            remaining_pairs_df = ratio_df_pr
            subjects = list()
            sample_size = int(min(1, np.floor(target_size_pr / remaining_pairs_df.ratio.max())))

            while current_size < target_size_pr:
                current_to_rarget_diff = target_size_pr - current_size
                remaining_pairs_df = remaining_pairs_df[remaining_pairs_df['ratio'] <
                                                        current_to_rarget_diff]

                if remaining_pairs_df.empty:
                    break

                next_subject = remaining_pairs_df.sample(sample_size, random_state=random_state)

                current_size += sum(next_subject.ratio.tolist())
                subject_names = next_subject.participant.tolist()
                remaining_pairs_df = remaining_pairs_df[~remaining_pairs_df.participant.
                                                        isin(subject_names)]

                subjects.extend(subject_names)
                if remaining_pairs_df.empty:
                    break

                large_sample_bound = int(np.floor(
                                        abs(target_size_pr - current_size)\
                                        / remaining_pairs_df.ratio.max()))
                sample_size = min(large_sample_bound, len(remaining_pairs_df))

            diff = abs(target_size_pr - current_size)
            return diff, current_size, subjects

        def init(ratio_df, target_size):
            global ratio_df_pr, target_size_pr
            ratio_df_pr = ratio_df
            target_size_pr = target_size

        pool = Pool()  # Create a process pool
        n_cpus = cpu_count() - 1
        with Pool(n_cpus, initializer=init, initargs=(ratio_df, target_size)) as pool:
            res = pool.imap_unordered(compute_ratios,
                                      list(range(self._max_iter)),
                                      chunksize=int(np.ceil(self._max_iter / n_cpus)))
            while best_diff > self._tolerance and iter < self._max_iter:
                diff, current_size, subjects = next(res)
                iter += 1
                if diff < best_diff:
                    best_subjects, best_size, best_diff = subjects, current_size, diff

        return best_subjects, best_size

    def _split_val_from_train(self, val_size: float, split_dictionary: dict, ratios: dict,
                              split_tracker: DataSplitTracker):
        adj_val_size = val_size / (val_size + ratios["train"])
        sub_split_dictionary, sub_ratios = self._split_by_ratio(
            subject_ids=split_dictionary["train"],
            sample_counts=split_tracker.subjects,
            val_size=adj_val_size)
        split_dictionary["val"] = sub_split_dictionary["val"]
        split_dictionary["train"] = sub_split_dictionary["train"]
        ratios["val"] = sub_ratios["val"] * (val_size + ratios["train"])
        ratios["train"] = sub_ratios["train"] * (val_size + ratios["train"])
        return split_dictionary, ratios


class ReaderSplitter(AbstractSplitter):

    def split_reader(self,
                     reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path: Path = None):
        """_summary_

        Args:
            reader (ProcessedSetReader): _description_
            test_size (float): _description_
            val_size (float): _description_
        """
        info_io(f"Splitting reader", level=0)
        self._print_ratio("Target", {
            "train": 1 - test_size - val_size,
            "test": test_size,
            "val": val_size
        })
        # Resotre split state
        storage_path = Path((storage_path \
                             if storage_path is not None \
                             else reader.root_path), "split")
        # Get subject counts
        preprocessing_tracker = PreprocessingTracker(Path(reader.root_path, "progress"))

        split_tracker = DataSplitTracker(storage_path,
                                         tracker=preprocessing_tracker,
                                         test_size=test_size,
                                         val_size=val_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter)
        # Identical split settings
        if split_tracker.is_finished:
            info_io(f"Restoring split from directory:\n{reader.root_path}")
            self._print_ratio("Real", split_tracker.ratios)
            return SplitSetReader(reader.root_path, split_tracker.split)

        info_io(f"Splitting in directory:\n{reader.root_path}")
        # Apply demographic filter
        subject_ids = self._get_demographics(prefix="Demographic filter",
                                             source_path=reader.root_path,
                                             subject_ids=reader.subject_ids,
                                             settings=demographic_filter)

        # Split by demographics
        if demographic_split is not None and demographic_split:
            # Apply split
            split_dictionary, ratios = self._split_by_demographics(
                subject_ids=subject_ids,
                source_path=reader.root_path,
                demographic_split=demographic_split)

            # Enforce ratio
            if test_size or val_size:
                split_dictionary, ratios = self._reduce_by_ratio(
                    subjects=split_dictionary,
                    sample_counts=split_tracker.subjects,
                    test_size=test_size,
                    val_size=(val_size if "val" in split_dictionary else 0))

            if train_size is not None and val_size and not "val" in split_dictionary:
                # In this case we split the val set from the train set
                split_dictionary, ratios = self._split_val_from_train(
                    val_size=val_size,
                    split_dictionary=split_dictionary,
                    ratios=ratios,
                    split_tracker=split_tracker)

            # Update tracker
            split_tracker.split = split_dictionary
            split_tracker.ratios = ratios
        else:
            # Split by ratio
            split_dictionary, ratios = self._split_by_ratio(subject_ids=subject_ids,
                                                            sample_counts=split_tracker.subjects,
                                                            test_size=test_size,
                                                            val_size=val_size)
            if train_size is not None:
                # Reduce train size if specified
                if train_size > len(subject_ids):
                    warn_io(f"Train size {train_size} is larger than the number of subjects")
                else:
                    split_dictionary["train"] = random.sample(split_dictionary["train"], train_size)

                split_dictionary, ratios = self._reduce_by_ratio(
                    subjects=split_dictionary,
                    sample_counts=split_tracker.subjects,
                    test_size=test_size,
                    val_size=val_size)
            # Update tracker
            split_tracker.split = split_dictionary
            split_tracker.ratios = ratios

        self._print_ratio("Real", ratios)
        split_tracker.is_finished = True

        return SplitSetReader(reader.root_path, split_dictionary)


class CompactSplitter(AbstractSplitter):

    def split_dict(self,
                   X_subjects: Dict[str, Dict[str, pd.DataFrame]],
                   y_subjects: Dict[str, Dict[str, pd.DataFrame]],
                   test_size: float = 0.0,
                   val_size: float = 0.0,
                   demographic_split: dict = None,
                   demographic_filter: dict = None,
                   source_path: Path = None):
        """_summary_

        Args:
            reader (ProcessedSetReader): _description_
            test_size (float): _description_
            val_size (float): _description_
        """
        info_io(f"Splitting reader", level=0)

        target_ratios = {"train": 1 - test_size - val_size, "test": test_size, "val": val_size}
        message = ""
        for set_name in target_ratios:
            if target_ratios[set_name]:
                message += f"Target {set_name} size: {target_ratios[set_name]:0.3f}\n"
        info_io(message)
        info_io(f"Splitting in directory: {source_path}")
        if (demographic_filter is not None or demographic_split is not None) and not source_path:
            raise ValueError("Demographic split requires source path"
                             " to locate the subject_info.csv file")

        if demographic_filter is not None:
            subject_ids = self._get_demographics(source_path, list(X_subjects.keys()),
                                                 demographic_filter)
        else:
            subject_ids = list(X_subjects.keys())

        if source_path:
            tracker = PreprocessingTracker(Path(source_path, "progress"), subject_ids=subject_ids)
            split_tracker = DataSplitTracker(Path(source_path, "split"), tracker, test_size,
                                             val_size)
            if split_tracker.is_finished:
                ...
                # return SplitSetReader(source_path, )
        else:
            split_tracker = None

        if demographic_split is not None and ("test" in demographic_split or "train"
                                              in demographic_split or "val" in demographic_split):
            subjects, ratios = self._split_by_demographics(subject_ids, source_path,
                                                           demographic_split)
            if test_size or val_size:
                subjects, ratios = self._reduce_by_ratio(subjects=subjects,
                                                         input_ratios=ratios,
                                                         test_size=test_size,
                                                         val_size=val_size)
            if split_tracker is not None:
                for split in subjects:
                    setattr(split_tracker, split, subjects[split])
                split_tracker.ratios = ratios
        else:
            subject_counts = self._compute_subject_counts(y_subjects)
            subjects, ratios = self._split_by_ratio(subject_ids, subject_counts, test_size,
                                                    val_size)
            if split_tracker is not None:
                for split in subjects:
                    setattr(split_tracker, split, subjects[split])
                split_tracker.ratios = ratios

        message = ""
        for set_name in ratios:
            message += f"Real {set_name} size: {ratios[set_name]:0.3f}\n"
        info_io(message)

        if split_tracker is not None:
            split_tracker.is_finished = True
        dataset = {
            set_name: (dict_subset(X_subjects, subjects), dict_subset(y_subjects, subjects))
            for set_name, subjects in subjects.items()
        }

        return dataset

    def _compute_subject_counts(self, y_subjects: Dict[str, Dict[str, pd.DataFrame]]):
        subject_counts = {
            subject_id: {
                stay_id: len(stay_data) for stay_id, stay_data in subject_data.items()
            } for subject_id, subject_data in y_subjects.items()
        }
        for subject in subject_counts:
            subject_counts[subject]["total"] = sum(subject_counts[subject].values())
        return subject_counts


### FILE: .\src\datasets\split\__init__.py ###
import pandas as pd
from pathlib import Path
from typing import Dict, Union
from multipledispatch import dispatch
from numbers import Number
from utils.IO import *
from settings import *
from .splitters import ReaderSplitter, CompactSplitter
from ..readers import ProcessedSetReader, SplitSetReader

__all__ = ['train_test_split']


@dispatch(dict,
          dict,
          test_size=float,
          val_size=float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          source_path=Path)
def train_test_split(X_subjects: Dict[str, Dict[str, pd.DataFrame]],
                     y_subjects: Dict[str, Dict[str, pd.DataFrame]],
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     source_path: Path = None) -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:
    return CompactSplitter().split_dict(X_subjects=X_subjects,
                                        y_subjects=y_subjects,
                                        test_size=test_size,
                                        val_size=val_size,
                                        train_size=train_size,
                                        source_path=source_path,
                                        demographic_split=demographic_split,
                                        demographic_filter=demographic_filter)


@dispatch(ProcessedSetReader,
          test_size=float,
          val_size=float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          val_size=float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          float,
          Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          float,
          Number,
          dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader, float, float, Number, dict, dict, storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader, float, float, dict, dict, Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


### FILE: .\src\generators\pytorch.py ###
import torch
from preprocessing.scalers import AbstractScaler
from datasets.readers import ProcessedSetReader
from torch.utils.data import Dataset
from utils.IO import *
from torch.utils.data import DataLoader
from . import AbstractGenerator


class TorchGenerator(DataLoader):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler = None,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 num_workers: int = 1,
                 drop_last: bool = False,
                 bining: str = "none"):
        super().__init__(dataset=TorchDataset(reader=reader,
                                              scaler=scaler,
                                              batch_size=1,
                                              shuffle=shuffle,
                                              bining=bining),
                         batch_size=batch_size,
                         shuffle=shuffle,
                         drop_last=drop_last,
                         num_workers=num_workers,
                         collate_fn=self.collate_fn)

    def collate_fn(self, batch):
        samples, labels = zip(*batch)
        samples = torch.stack(self._zeropad_samples(samples))
        labels = torch.cat(labels)
        if labels.dim() == 1:
            return samples, labels.unsqueeze(1)
        return samples, labels

    @staticmethod
    def _zeropad_samples(data):
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        # Ensure data is a list of PyTorch tensors
        if not all(isinstance(x, torch.Tensor) for x in data):
            raise ValueError("All items in the data list must be PyTorch tensors")

        # Determine the dtype and device from the first tensor
        dtype = data[0].dtype
        device = data[0].device

        # Find the maximum length along the first dimension
        max_len = max(x.shape[0] for x in data)

        # Pad each tensor to the maximum length
        padded_data = [
            torch.cat(
                [x,
                 torch.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype, device=device)],
                dim=0) for x in data
        ]

        # Stack all padded tensors into a single tensor
        return padded_data


class TorchDataset(AbstractGenerator, Dataset):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler = None,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 bining: str = "none"):
        super(TorchDataset, self).__init__(reader=reader,
                                           scaler=scaler,
                                           batch_size=batch_size,
                                           shuffle=shuffle,
                                           bining=bining)

    def __getitem__(self, index=None):
        X, y = super().__getitem__(index)
        X = X.squeeze()
        return torch.from_numpy(X).to(torch.float32), torch.from_numpy(y).to(torch.int8)


### FILE: .\src\generators\stream.py ###
import torch
import pandas as pd
import numpy as np
from typing import List
from preprocessing.scalers import AbstractScaler
from datasets.readers import ProcessedSetReader
from torch.utils.data import Dataset
from utils.IO import *
from torch.utils.data import DataLoader
from . import AbstractGenerator


class RiverGenerator(AbstractGenerator, Dataset):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler,
                 shuffle: bool = True,
                 bining: str = "none"):
        super(RiverGenerator, self).__init__(reader=reader,
                                             scaler=scaler,
                                             batch_size=1,
                                             shuffle=shuffle,
                                             bining=bining)
        self._names: List[str] = None
        self._labels: List[str] = None
        self._index = 0
        self._row_only = True

    def __iter__(self):
        self._index = 0
        return self

    def __next__(self, *args, **kwargs):
        if self._index >= self.steps:
            raise StopIteration
        X, y = super().__getitem__()
        X = np.squeeze(X)
        if self._names is None:
            self._names = [str(i) for i in range(714)]
        X = dict(zip(self._names, X))
        y = np.squeeze(y)
        if y.shape:
            if self._labels is None:
                self._labels = [str(i) for i in range(len(y))]
            y = dict(zip(self._labels, y))
        else:
            y = float(y)
        self._index += 1
        return X, y


### FILE: .\src\generators\tf2.py ###
from preprocessing.scalers import AbstractScaler
from datasets.readers import ProcessedSetReader
from tensorflow.keras.utils import Sequence
from utils.IO import *
from . import AbstractGenerator


class TFGenerator(AbstractGenerator, Sequence):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler = None,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 bining: str = "none"):
        super(TFGenerator, self).__init__(reader=reader,
                                          scaler=scaler,
                                          batch_size=batch_size,
                                          shuffle=shuffle,
                                          bining=bining)

    def __getitem__(self, index=None):
        return super().__getitem__(index)


### FILE: .\src\generators\__init__.py ###
"""Preprocessing file

This file provides the implemented preprocessing functionalities.

"""

import numpy as np
import pandas as pd
import random

from copy import deepcopy
from pathlib import Path
from utils.IO import *
from preprocessing.scalers import AbstractScaler
from datasets.trackers import PreprocessingTracker
from datasets.readers import ProcessedSetReader
from metrics import CustomBins, LogBins


class AbstractGenerator(object):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 bining: str = "none"):
        super().__init__()
        self._batch_size = batch_size
        self._shuffle = shuffle
        self._reader = reader
        self._columns = None
        self._tracker = PreprocessingTracker(storage_path=Path(reader.root_path, "progress"))
        self._steps = self._count_batches()
        self._subject_ids = reader.subject_ids
        self._scaler = scaler
        self._remaining_ids = deepcopy(self._reader.subject_ids)
        self.generator = self._generator()
        self._row_only = False

        if bining not in ["none", "log", "custom"]:
            raise ValueError("Bining must be one of ['none', 'log', 'custom']")
        self._bining = bining

    @property
    def steps(self):
        return self._steps

    def __getitem__(self, index=None):
        X_batch, y_batch = list(), list()
        for _ in range(self._batch_size):
            X, y = next(self.generator)
            X_batch.append(X)
            y_batch.append(y)
        X_batch = self._zeropad_samples(X_batch)
        y_batch = np.array(y_batch)
        return X_batch.astype(np.float32), y_batch.astype(np.float32)

    def _count_batches(self):
        """
        """
        return int(
            np.floor(
                sum([
                    self._tracker.subjects[subject_id]["total"]
                    for subject_id in self._reader.subject_ids
                ])) / self._batch_size)

    def __len__(self):
        'Denotes the number of batches per epoch'
        return self._steps

    def on_epoch_end(self):
        ...

    def _generator(self):

        while True:
            data, subjects = self._reader.random_samples(return_ids=True)
            X_subject, y_subject = data.values()
            for X_stay, y_stay in zip(X_subject, y_subject):
                if self._columns is None:
                    self._columns = X_stay.columns
                X_stay[self._columns] = self._scaler.transform(X_stay[self._columns])

                Xs, ys, ts = self.read_timeseries(X_df=X_stay,
                                                  y_df=y_stay,
                                                  row_only=self._row_only,
                                                  bining=self._bining)

                (Xs, ys, ts) = self._shuffled_data([Xs, ys, ts])

                index = 0
                while index < len(ys):
                    yield Xs[index], ys[index]
                    index += 1

    @staticmethod
    def read_timeseries(X_df: pd.DataFrame, y_df: pd.DataFrame, row_only=False, bining="none"):
        """
        """
        Xs = list()
        ys = list()
        ts = list()

        for timestamp in y_df.index:

            y = np.squeeze(y_df.loc[timestamp].values)
            if not y.shape:
                y = float(y)
                if bining == "log":
                    LogBins.get_bin_log(y)
                elif bining == "custom":
                    CustomBins.get_bin_custom(y)

            if row_only:
                X = X_df.loc[timestamp].values
            else:
                X = X_df.loc[:timestamp].values

            Xs.append(X)
            ys.append(y)
            ts.append(timestamp)

        return Xs, ys, ts

    def _shuffled_data(self, data):
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        assert len(data) >= 2

        if type(data[0][0]) == pd.DataFrame:
            data[0] = [x.values for x in data[0]]

        data = list(zip(*data))

        if self._shuffle:
            random.shuffle(data)

        residual_length = len(data) % self._batch_size
        head = data[:len(data) - residual_length]
        residual = data[len(data) - residual_length:]

        head.sort(key=(lambda x: x[0].shape[0]))

        batches = [head[i:i + self._batch_size] for i in range(0, len(head), self._batch_size)]

        if self._shuffle:
            random.shuffle(batches)

        data = list()

        for batch in batches:
            data += batch

        data += residual
        data = list(zip(*data))

        return data

    @staticmethod
    def _zeropad_samples(data):
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        dtype = data[0].dtype
        max_len = max([x.shape[0] for x in data])
        ret = [
            np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)],
                           axis=0) for x in data
        ]
        return np.array(ret)


### FILE: .\src\metrics\stream.py ###
import collections
import numpy as np
from typing import Dict
from river import metrics as river_metrics
from scipy import integrate
from metrics import CustomBins
from river import metrics
import warnings

# Suppress the specific deprecation warning
warnings.filterwarnings(
    "ignore",
    category=DeprecationWarning,
)


class LOSClassificationReport(river_metrics.ClassificationReport):

    def update(self, y_true, y_pred, w=1):
        y_true_bin = CustomBins.get_bin_custom(y_true)
        y_pred_bin = CustomBins.get_bin_custom(y_pred)
        return super().update(y_true_bin, y_pred_bin, w)


class LOSCohenKappa(river_metrics.CohenKappa):

    def __init__(self, cm=None, weights=None):
        super().__init__(cm)
        self.weights = weights
        self.nbins = CustomBins.nbins

    def update(self, y_true, y_pred):
        y_true_bin = CustomBins.get_bin_custom(y_true)
        y_pred_bin = CustomBins.get_bin_custom(y_pred)
        self.cm.update(y_true_bin, y_pred_bin)
        return self

    def get(self):
        # Number of classes
        classes = self.cm.classes
        n_classes = len(classes)

        # Calculate expected matrix
        expected = np.zeros((n_classes, n_classes))
        sum0 = self.cm.sum_col
        sum1 = self.cm.sum_row
        total_sum0 = self.cm.n_samples

        # Calculate the numerator and denominator for kappa
        numerator = 0
        denominator = 0

        # Fill weight matrix according to the specified weights
        for i in range(n_classes):
            for j in range(n_classes):
                expected[i, j] = (sum0[j] * sum1[i]) / total_sum0
                if self.weights is None:
                    weights = 0 if i == j else 1
                elif self.weights == "linear":
                    weights = abs(i - j)
                else:  # quadratic
                    weights = (i - j)**2
                numerator += weights * self.cm[i][j]
                denominator += weights * expected[i, j]

        # Compute kappa
        if denominator == 0:
            return 0.0
        k = numerator / denominator
        return 1 - k

    def revert(self, y_true, y_pred):
        y_true_bin = CustomBins.get_bin_custom(y_true)
        y_pred_bin = CustomBins.get_bin_custom(y_pred)
        self.cm.revert(y_true_bin, y_pred_bin)
        return self


class PRAUC(river_metrics.ROCAUC):

    def __init__(self, n_thresholds=2, pos_val=True):
        self.pos_val = pos_val
        self.n_thresholds = n_thresholds
        self.thresholds = [i / (n_thresholds - 1) for i in range(n_thresholds)]
        self.thresholds[0] -= 1e-7
        self.thresholds[-1] += 1e-7
        self.cms = [river_metrics.ConfusionMatrix() for _ in range(n_thresholds)]

    def update(self, y_true, y_pred, w=1.0):
        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
        for t, cm in zip(self.thresholds, self.cms):
            cm.update(y_true == self.pos_val, p_true > t, w)

    def get(self):
        tprs = [0] * self.n_thresholds
        fprs = [0] * self.n_thresholds

        def safe_div(a, b):
            if a == 0 and b == 0:
                return 1.0
            try:
                return a / b
            except ZeroDivisionError:
                return 0.0

        for i, cm in enumerate(self.cms):
            tp = cm.true_positives(self.pos_val)
            fp = cm.false_positives(self.pos_val)
            fn = cm.false_negatives(self.pos_val)
            tprs[i] = safe_div(tp, tp + fp)  # precision
            fprs[i] = safe_div(tp, tp + fn)  # recall

        return -integrate.trapezoid(x=fprs, y=tprs)


class MicroROCAUC(river_metrics.ROCAUC):

    def __init__(self, n_thresholds=10, pos_val=True):
        self._y_true_all = []
        self._y_pred_all = []
        super().__init__(n_thresholds=n_thresholds, pos_val=pos_val)

    def update(self, y_true: dict, y_pred: dict):
        for yt, yp in zip(y_true.values(), y_pred.values()):
            super().update(yt, yp)
        return self

    def revert(self, y_true: dict, y_pred: dict):
        for yt, yp in zip(y_true.values(), y_pred.values()):
            super().revert(yt, yp)
        return self

    def works_with(self, y_true: dict, y_pred: dict):
        return isinstance(y_true, dict) and isinstance(y_pred, dict)


class MacroROCAUC(river_metrics.base.MultiClassMetric):

    def __init__(self, n_thresholds=10, pos_val=True):
        self._n_thresholds = n_thresholds
        self._pos_val = pos_val
        self._per_class_rocaucs = collections.defaultdict(river_metrics.ROCAUC)
        self._classes = set()

    def update(self, y_true: dict, y_pred: dict):
        for label_name, y_label in y_pred.items():
            self._classes.add(label_name)
            self._per_class_rocaucs[label_name].update(y_true[label_name], y_label)
        return self

    def get(self):
        # Calculate the macro-average ROC AUC
        return np.mean([rocauc.get() for rocauc in self._per_class_rocaucs.values()])

    def revert(self, y_true, y_pred):
        for label_name, y_label in y_pred.items():
            self._per_class_rocaucs[label_name].revert(y_true[label_name], y_label)
        return self

    @property
    def bigger_is_better(self):
        return True

    def works_with(self, y_true, y_pred):
        return isinstance(y_true, dict) and isinstance(y_pred, dict)


if __name__ == '__main__':

    y_pred = [23, 60, 100, 130, 160, 190, 220, 250, 360, 830]
    y_true = [11.45, 35.07, 59.20, 83.38, 107.48, 131.57, 155.64, 179.66, 254.30, 585.32]

    # ---------------- Comparing Cohen's Kappa using River with sklearn --------------------
    print("--- Comparing Cohen's Kappa ---")
    metric = LOSCohenKappa(weights='linear')
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    """
    from time import time
    start = time()
    for _ in range(100000):
        metric.get()
    end = time()
    print("Time taken new implementation", end - start)
    #
    metric = river_metrics.CohenKappa()
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    from time import time
    start = time()
    for _ in range(100000):
        metric.get()
    end = time()
    print("Time taken legacy", end - start)
    metric.get()
    """
    print("Cohen's Kappa (river)", metric)

    from sklearn.metrics import cohen_kappa_score

    class CustomBins:
        inf = 1e18
        bins = [(-inf, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 14),
                (14, +inf)]
        nbins = len(bins)
        # TODO: whats this
        means = [
            11.450379, 35.070846, 59.206531, 83.382723, 107.487817, 131.579534, 155.643957,
            179.660558, 254.306624, 585.325890
        ]

    def get_bin_custom(x, nbins, one_hot=False):
        for i in range(nbins):
            a = CustomBins.bins[i][0] * 24.0
            b = CustomBins.bins[i][1] * 24.0
            if a <= x < b:
                if one_hot:
                    ret = np.zeros((CustomBins.nbins,))
                    ret[i] = 1
                    return ret
                return i
        return None

    y_true_bins = [get_bin_custom(x, CustomBins.nbins) for x in y_true]
    prediction_bins = [get_bin_custom(x, CustomBins.nbins) for x in y_pred]
    kappa = cohen_kappa_score(y_true_bins, prediction_bins, weights='linear')

    print("Cohen's Kappa (scikit)", kappa)

    from river import metrics
    from sklearn.metrics import roc_auc_score, precision_recall_curve, auc

    # Binary classification data
    y_true = [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
    y_pred = [0.1, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.35, 0.0, 0.8, 0.0, 0.2, 0.8]

    # ---------------- Comparing ROC AUC using River with sklearn --------------------
    print("--- Comparing AUC ROC ---")
    metric = metrics.ROCAUC(n_thresholds=20)
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    print("Auc-roc (river)", metric)
    # Compute roc curve
    auc_roc = roc_auc_score(y_true, y_pred)
    print("Auc-roc (scikit)", auc_roc)

    # ---------------- Comparing Precision-Recall AUC using River with sklearn --------------------
    print("--- Comparing AUC PR ---")
    metric = PRAUC()
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    pr_auc = metric.get()
    print("Auc-pr (river)", str(metric))
    # Compute precision-recall curve
    precision, recall, _ = precision_recall_curve(y_true, np.array(y_pred).astype(int).tolist())
    # Compute the area under the curve
    pr_auc = auc(recall, precision)
    print("Auc-pr (scikit)", pr_auc)

    # Multi-class classification data
    y_true_multi = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0],
                    [0, 0, 1], [1, 0, 0], [1, 0, 1]]
    y_true_labeled = list()
    for y in y_true_multi:
        y_true_labeled.append({str(i): yt for i, yt in enumerate(y)})
    y_pred_multi = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.3, 0.6], [0.7, 0.2, 0.1],
                    [0.1, 0.6, 0.3], [0.2, 0.1, 0.7], [0.6, 0.3, 0.1], [0.3, 0.5, 0.2],
                    [0.2, 0.1, 0.7], [0.7, 0.2, 0.1]]
    y_pred_labeled = list()
    for y in y_pred_multi:
        y_pred_labeled.append({str(i): yp for i, yp in enumerate(y)})

    # ---------------- Comparing Micro-Macro ROC AUC using River with sklearn --------------------
    print("--- Comparing Micro-Macro ROC AUC ---")
    micro_rocauc = MicroROCAUC()
    macro_rocauc = MacroROCAUC()

    for yt, yp in zip(y_true_labeled, y_pred_labeled):
        micro_rocauc = micro_rocauc.update(yt, yp)
        macro_rocauc = macro_rocauc.update(yt, yp)

    print(f'Micro-average Auc-roc (river): {micro_rocauc.get():.4f}')
    print(f'Macro-average Auc-roc (river): {macro_rocauc.get():.4f}')
    from sklearn.metrics import roc_auc_score

    # Compute micro-average ROC AUC using sklearn
    micro_rocauc_sklearn = roc_auc_score(y_true_multi,
                                         y_pred_multi,
                                         average='micro',
                                         multi_class='ovr')
    print(f'Micro-average auc-roc (sklearn): {micro_rocauc_sklearn:.4f}')

    # Compute macro-average ROC AUC using sklearn
    macro_rocauc_sklearn = roc_auc_score(y_true_multi,
                                         y_pred_multi,
                                         average='macro',
                                         multi_class='ovr')
    print(f'Macro-average auc-roc (sklearn): {macro_rocauc_sklearn:.4f}')


### FILE: .\src\metrics\__init__.py ###
import numpy as np
import bisect
from settings import *


class CustomBins:
    inf = 1e18
    bins = [(-np.inf, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 14),
            (14, np.inf)]
    means = [
        11.450379, 35.070846, 59.206531, 83.382723, 107.487817, 131.579534, 155.643957, 179.660558,
        254.306624, 585.325890
    ]
    nbins = len(bins)
    # Precompute scaled bin boundaries
    scaled_bins = [(a * 24.0, b * 24.0) for a, b in bins]
    lower_bounds = [a * 24.0 for a, b in bins]

    @staticmethod
    def get_bin_custom(x, one_hot=False):
        index = bisect.bisect_right(CustomBins.lower_bounds, x) - 1
        if one_hot:
            ret = np.zeros((CustomBins.nbins,))
            ret[index] = 1
            return ret
        return index


class LogBins:
    nbins = 10
    means = [
        0.611848, 2.587614, 6.977417, 16.465430, 37.053745, 81.816438, 182.303159, 393.334856,
        810.964040, 1715.702848
    ]

    def get_bin_log(x, nbins=10, one_hot=False):
        binid = int(np.log(x + 1) / 8.0 * nbins)
        if binid < 0:
            binid = 0
        if binid >= nbins:
            binid = nbins - 1

        if one_hot:
            ret = np.zeros((LogBins.nbins,))
            ret[binid] = 1
            return ret
        return binid


### FILE: .\src\models\callbacks.py ###
"https://stackoverflow.com/questions/60727279/save-history-of-model-fit-for-different-epochs"
"https://stackoverflow.com/questions/69595923/how-to-decrease-the-learning-rate-every-10-epochs-by-a-factor-of-0-9"

import json
import tensorflow.keras.backend as K
from pathlib import Path
from tensorflow.keras import callbacks, backend


class HistoryCheckpoint(callbacks.Callback):
    """
    """

    def __init__(self, storage_path):
        """
        """
        self.storage_file = Path(storage_path, "history.json")
        super().__init__()

    def on_epoch_end(self, epoch, logs=None):
        """
        """

        if ('lr' not in logs.keys()):
            logs.setdefault('lr', 0)
            logs['lr'] = K.get_value(self.model.optimizer.lr)

        if self.storage_file.is_file():
            with open(self.storage_file, 'r+') as file:
                eval_hist = json.load(file)
        else:
            eval_hist = dict()

        for key, value in logs.items():
            if not key in eval_hist:
                eval_hist[key] = list()

            eval_hist[key].append(float(value))

        with open(self.storage_file, 'w') as file:
            json.dump(eval_hist, file, indent=4)

        return


class DecayLearningRate(callbacks.Callback):

    def __init__(self, freq, factor):
        """
        """
        self.freq = freq
        self.factor = factor

    def on_epoch_end(self, epoch, logs=None):
        """
        """
        if epoch % self.freq == 0 and not epoch == 0:  # adjust the learning rate
            lr = float(backend.get_value(self.model.optimizer.lr))  # get the current learning rate
            new_lr = lr * self.factor
            backend.set_value(self.model.optimizer.lr,
                              new_lr)  # set the learning rate in the optimizer


### FILE: .\src\models\trackers.py ###
import numpy as np
from pathlib import Path
from utils.IO import *
from storable import storable
from utils import update_json
from settings import *


@storable
class ModelHistory():

    train_loss: dict = {}
    val_loss: dict = {}
    best_val: dict = {"loss": float('inf'), "epoch": np.nan}
    best_train: dict = {"loss": float('inf'), "epoch": np.nan}
    test_loss: float = np.nan

    def to_json(self):
        update_json(Path(self._path.parent, f"{self._path.stem}.json"), self._progress)
        return self._progress


class LocalModelHistory():

    train_loss: dict = {}
    val_loss: dict = {}
    best_val: dict = {"loss": float('inf'), "epoch": np.nan}
    best_train: dict = {"loss": float('inf'), "epoch": np.nan}
    test_loss: float = np.nan

    def to_json(self):
        return {
            "train_loss": self.train_loss,
            "val_loss": self.val_loss,
            "best_val": self.best_val,
            "best_train": self.best_train,
            "test_loss": self.test_loss
        }


@storable
class RiverHistory():

    train_metrics: dict = {}
    val_metrics: dict = {}
    test_metrics: dict = {}

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def to_text(self, report_path: Path, report_dict: dict):
        report = ""
        for key, value in report_dict.items():
            report += f"{key}:\n {value}\n"

        with open(report_path, 'w') as file:
            file.write(report)
        return report

    def to_json(self):
        numeric_dict = {}
        for attribute, history in self._progress.items():
            numeric_dict[attribute] = dict()
            for metric, value in history.items():
                if self._allowed_key(metric):
                    numeric_dict[attribute][metric] = value
        report_dict = {}
        for attribute, history in self._progress.items():
            for metric, value in history.items():
                if not metric in numeric_dict[attribute]:
                    report_dict[metric] = value

        update_json(Path(self._path.parent, f"{self._path.stem}.json"), numeric_dict)
        self.to_text(Path(self._path.parent, f"{self._path.stem}.txt"), report_dict)
        return self._progress


class LocalRiverHistory():

    train_metrics: dict = {}
    val_metrics: dict = {}
    test_metrics: dict = {}

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in self._non_numeric_metrics])

    def to_json(self):
        return {
            "train_metrics": self.train_metrics,
            "val_metrics": self.val_metrics,
            "test_metrics": self.test_metrics
        }


### FILE: .\src\models\__init__.py ###


### FILE: .\src\models\pytorch\lstm.py ###
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pickle
from typing import Union, List, Dict
from models.trackers import ModelHistory, LocalModelHistory
from torch.utils.data import DataLoader
from collections import defaultdict
from pathlib import Path
from torchmetrics import Metric
from tensorflow.keras.utils import Progbar
from utils import to_snake_case
from torch.optim import Optimizer
from utils.IO import *
from settings import *
from .mappings import *


class LSTMNetwork(nn.Module):

    def __init__(self,
                 layer_size: Union[List[int], int],
                 dropout: float,
                 input_dim: int,
                 bidirectional: bool = False,
                 recurrent_dropout: float = 0.,
                 final_activation: str = None,
                 target_repl: bool = False,
                 output_dim: int = 1,
                 depth: int = 1,
                 model_path: Path = None):
        super(LSTMNetwork, self).__init__()
        self._model_path = model_path
        if self._model_path is not None:
            # Persistent history
            self._model_path.mkdir(parents=True, exist_ok=True)
            self._history = ModelHistory(Path(self._model_path, "history"))
        else:
            # Mimics the storable
            self._history = LocalModelHistory()
        self._layer_size = layer_size
        self._dropout_rate = dropout
        self._recurrent_dropout = recurrent_dropout
        self._depth = depth
        self._bidirectional = bidirectional

        if final_activation is None:
            if output_dim == 1:
                self._final_activation = nn.Sigmoid()
            else:
                self._final_activation = nn.Softmax(dim=-1)
        else:
            self._final_activation = activation_mapping[final_activation]

        self._output_dim = output_dim

        if output_dim == 1:
            self._task = "binary"
            self._num_classes = 1
        elif isinstance(self._final_activation, nn.Softmax):
            self._task = "multiclass"
            self._num_classes = output_dim
        elif isinstance(self._final_activation, nn.Sigmoid):
            self._task = "multilabel"
            self._num_classes = output_dim

        if isinstance(layer_size, int):
            self._hidden_sizes = [layer_size] * depth
        else:
            self._hidden_sizes = layer_size
            if depth != 1:
                warn_io("Specified hidden sizes and depth are not consistent. "
                        "Using hidden sizes and ignoring depth.")

        self.lstm_layers = nn.ModuleList()
        input_size = input_dim
        for i, hidden_size in enumerate(self._hidden_sizes):
            self.lstm_layers.append(
                nn.LSTM(input_size=input_size,
                        hidden_size=hidden_size,
                        num_layers=1,
                        batch_first=True,
                        dropout=(recurrent_dropout if i < depth - 1 else 0),
                        bidirectional=bidirectional))
            input_size = hidden_size * (2 if bidirectional else 1)

        self.dropout = nn.Dropout(dropout)
        self.output_layer = nn.Linear(input_size, self._output_dim)

    @property
    def optimizer(self):
        if hasattr(self, "_optimizer"):
            return self._optimizer

    @property
    def loss(self):
        if hasattr(self, "_loss"):
            return self._loss

    def save(self, epoch):
        """_summary_
        """
        if self._model_path is not None:
            checkpoint_path = Path(self._model_path, f"cp-{epoch:04}.ckpt")
            torch.save(self.state_dict(), checkpoint_path)

    def load(self, epochs: int, weights_only=False):
        """_summary_

        Returns:
            _type_: _description_
        """
        if self._model_path is not None:
            latest_epoch = self._latest_epoch(epochs, self._model_path)
            if weights_only:
                checkpoint_path = Path(self._model_path, f"cp-{latest_epoch:04}.ckpt")
                if checkpoint_path.is_file():
                    model_state = torch.load(checkpoint_path)
                    self.load_state_dict(model_state)
                    print(f"Loaded model parameters from {checkpoint_path}")
                    return 1
            else:
                checkpoint_path = Path(self._model_path, f"cp-{latest_epoch:04}.pkl")
                if checkpoint_path.is_file():
                    with open(checkpoint_path, "rb") as load_file:
                        load_params = pickle.load(load_file)
                    for key, value in load_params.items():
                        setattr(self, key, value)
                    print(f"Loaded model from {checkpoint_path}")

                    return 1
        return 0

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def _clean_directory(self, best_epoch: int, epochs: int, keep_latest: bool = True):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """

        [
            folder.unlink()
            for i in range(epochs + 1)
            for folder in self._model_path.iterdir()
            if f"{i:04d}" in folder.name and ((i != epochs) or not keep_latest) and
            (i != best_epoch) and (".ckpt" in folder.name) and folder.is_file()
        ]

    def forward(self, x):
        x.to(self._device)
        # Masking is not natively supported in PyTorch LSTM, assume x is already preprocessed if necessary
        for lstm in self.lstm_layers:
            x, _ = lstm(x)
            x = self.dropout(x)
        x = x[:, -1, :]
        x = self.output_layer(x)
        if self._final_activation:
            x = self._final_activation(x)
        return x

    def _get_metrics(self, metrics: Dict[str, Metric]):
        keys = list([metric for metric in metrics.keys() if self._allowed_key(metric)])
        values = [metrics[key].get() for key in keys]
        return tuple(zip(keys, values))

    def _init_metrics(self, metrics, prefix: str = None) -> Dict[str, Metric]:
        settings = {"task": self._task, "num_classes": self._num_classes}
        return_metrics = dict()
        for metric in metrics:
            if isinstance(metric, str):
                metric_name = metric
                metric = metric_mapping[metric]
            else:
                metric_name = to_snake_case(metric.__name__)
            if isinstance(metric, type):
                metric = metric(**settings)
            if prefix is not None:
                metric_name = f"{prefix}_{metric_name}"
            return_metrics[metric_name] = metric
        return return_metrics

    def compile(self,
                optimizer: Dict[str, Union[str, type, Optimizer]] = None,
                loss=None,
                metrics: Dict[str, Union[str, type, Metric]] = None,
                class_weight=None):
        if isinstance(optimizer, str):
            if optimizer in optimizer_mapping:
                self._optimizer = optimizer_mapping[optimizer](self.parameters(), lr=0.001)
            else:
                raise ValueError(f"Optimizer {optimizer} not supported."
                                 f"Supported optimizers are {optimizer_mapping.keys()}")
        elif optimizer is None:
            self._optimizer = optim.Adam(self.parameters(), lr=0.001)
        else:
            self._optimizer = optimizer

        if isinstance(loss, str):
            if loss in loss_mapping:
                self._loss = loss_mapping[loss](weight=class_weight)
            else:
                raise ValueError(f"Loss {loss} not supported."
                                 f"Supported losses are {loss_mapping.keys()}")
        else:
            self._loss = loss
        if metrics is not None:
            self._metrics = self._init_metrics(metrics)
            self._train_metrics = self._init_metrics(metrics)
            self._test_metrics = self._init_metrics(metrics, prefix="test")
            self._val_metrics = self._init_metrics(metrics, prefix="val")

        self._device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.to(self._device)

    def _latest_epoch(self, epochs: int, filepath: Path):
        """_summary_

        Returns:
            _type_: _description_
        """
        if self._model_path is None:
            return 0
        check_point_epochs = [
            i for i in range(epochs + 1) for folder in filepath.iterdir()
            if f"{i:04d}" in folder.name
        ]

        if check_point_epochs:
            return max(check_point_epochs)

        return 0

    def _update_metrics(self, metrics: Dict[str, Metric], y_pred, y_true):
        # https://torchmetrics.readthedocs.io/en/v0.8.0/pages/classification.html
        # What do we need?
        for _, metric in metrics.items():
            metric.update(y_pred, y_true.astype(int))

    def _train(self,
               train_generator: DataLoader,
               epoch: int = 0,
               epochs: int = 1,
               sample_weights: dict = None,
               has_val: bool = False):
        print(f'\nEpoch {epoch}/{epochs}')
        self.train()
        train_losses = []
        generator_size = len(train_generator)
        self._train_progbar = Progbar(generator_size)

        for batch_idx, (inputs, labels) in enumerate(train_generator):
            inputs = inputs.to(self._device)
            labels = labels.to(self._device)
            self._optimizer.zero_grad()
            outputs = self(inputs)
            if sample_weights is not None:
                loss = self._loss(outputs, labels, sample_weight=sample_weights)
            else:
                loss = self._loss(outputs, labels)
            self._update_metrics(self._train_metrics, outputs, labels)
            loss.backward()
            self._optimizer.step()
            train_losses.append(loss.item())

            self._train_progbar.update(batch_idx + 1,
                                       values=[('loss', loss.item())] +
                                       self._get_metrics(self._train_metrics),
                                       finalize=(batch_idx == generator_size and not has_val))

        avg_train_loss = np.mean(train_losses)
        self._history.train_loss[epoch] = avg_train_loss
        if self._history.best_train["loss"] > avg_train_loss:
            self._history.best_train["loss"] = avg_train_loss
            self._history.best_train["epoch"] = epoch

    def _evaluate(self,
                  val_generator: DataLoader,
                  val_frequency: int,
                  epoch: int,
                  is_test: bool = False):

        if val_generator is not None and (epoch) % val_frequency == 0:
            self.eval()
            val_losses = []
            with torch.no_grad():
                for val_inputs, val_labels in val_generator:
                    val_inputs = val_inputs.to(self._device)
                    val_labels = val_labels.to(self._device)
                    val_outputs = self(val_inputs)
                    val_loss = self._loss(val_outputs, val_labels)
                    val_losses.append(val_loss.item())

            avg_val_loss = np.mean(val_losses)
            if is_test:
                self._history.test_loss = avg_val_loss
            else:
                self._history.val_loss[epoch] = avg_val_loss
                if self._history.best_val["loss"] > avg_val_loss:
                    self._history.best_val["loss"] = avg_val_loss
                    self._history.best_val["epoch"] = epoch
                if hasattr(self, "_train_progbar"):
                    self._train_progbar.update(self._train_progbar.target,
                                               values=[('loss', avg_val_loss),
                                                       ('val_loss', avg_val_loss)])
            return avg_val_loss

    def evaluate(self, test_generator: DataLoader):
        self._evaluate(val_generator=test_generator, val_frequency=0, epoch=0)

    def fit(self,
            train_generator: DataLoader,
            epochs: int,
            patience: int = None,
            save_best_only: bool = True,
            restore_best_weights: bool = True,
            sample_weights: dict = None,
            val_frequency: int = 1,
            val_generator: DataLoader = None,
            model_path: Path = None):
        if model_path is not None:
            self._model_path = model_path
            self._model_path.mkdir(parents=True, exist_ok=True)
        if patience is None:
            patience = epochs
        if val_generator is None:
            warn_io("WARNING:tensorflow:Early stopping conditioned on metric `val_loss` "
                    "which is not available. Available metrics are: loss")
        self._patience_counter = 0
        initial_epoch = self._latest_epoch(epochs, self._model_path)
        self.load(epochs, weights_only=True)

        for epoch in range(initial_epoch + 1, epochs + 1):

            self._train(train_generator=train_generator,
                        epoch=epoch,
                        epochs=epochs,
                        sample_weights=sample_weights,
                        has_val=val_generator is not None)

            self._evaluate(val_generator=val_generator, val_frequency=val_frequency, epoch=epoch)

            if val_generator is not None:
                best_epoch = self._history.best_val["epoch"]
            else:
                best_epoch = self._history.best_train["epoch"]
            if self._checkpoint(save_best_only=save_best_only,
                                restore_best_weights=restore_best_weights,
                                epoch=epoch,
                                epochs=epochs,
                                best_epoch=best_epoch,
                                patience=patience):
                break
        return self._history.to_json()

    def _checkpoint(self, save_best_only, restore_best_weights, epoch, epochs, best_epoch,
                    patience):
        if self._model_path is None:
            return False
        if save_best_only and best_epoch == epoch:
            self.save(epoch)
            self._clean_directory(best_epoch, epochs, True)
            self._patience_counter = 0
        elif not save_best_only and self._model_path is not None:
            self.save(epoch)
            self._patience_counter += 1
            if self._patience_counter >= patience:
                if restore_best_weights:
                    self.load_state_dict(
                        torch.load(Path(self._model_path, f"cp-{best_epoch:04}.ckpt")))
                return True

        return False


if __name__ == "__main__":
    from tests.settings import *
    import datasets
    from preprocessing.scalers import MIMICMinMaxScaler
    from generators.pytorch import TorchGenerator
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task="IHM")

    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)

    from models.tf2.lstm import LSTMNetwork
    from tests.settings import *
    scaler = MIMICMinMaxScaler().fit_reader(reader.train)
    train_generator = TorchGenerator(reader=reader.train, scaler=scaler, batch_size=2, shuffle=True)
    val_generator = TorchGenerator(reader=reader.val, scaler=scaler, batch_size=2, shuffle=True)

    import torch
    import torch.nn as nn
    import torch.optim as optim

    from models.pytorch.lstm import LSTM
    model_path = Path(TEMP_DIR, "torch_lstm")
    model_path.mkdir(parents=True, exist_ok=True)
    model = LSTMNetwork(10,
                        0.2,
                        59,
                        bidirectional=False,
                        recurrent_dropout=0.,
                        task=None,
                        target_repl=False,
                        output_dim=1,
                        depth=2)  #,
    #model_path=model_path)

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    model.compile(optimizer=optimizer, loss=criterion)
    # Example training loop
    history = model.fit(train_generator=train_generator, val_generator=val_generator, epochs=40)
    print(history)


### FILE: .\src\models\pytorch\mappings.py ###
import torch.optim as optim
import torch.nn as nn
import torchmetrics

__all__ = ["optimizer_mapping", "loss_mapping", "metric_mapping", "activation_mapping"]

optimizer_mapping = {
    "sgd": optim.SGD,
    "adam": optim.Adam,
    "adadelta": optim.Adadelta,
    "adagrad": optim.Adagrad,
    "adamax": optim.Adamax,
    "rmsprop": optim.RMSprop,
    "nadam": optim.NAdam
}

loss_mapping = {
    "binary_crossentropy": nn.BCELoss,
    "logits_binary_crossentropy": nn.BCEWithLogitsLoss,
    "categorical_crossentropy": nn.CrossEntropyLoss,
    "kld": nn.KLDivLoss,
    "mean_squared_error": nn.MSELoss,
    "mean_absolute_error": nn.L1Loss,
    "hinge": nn.HingeEmbeddingLoss,
    "poisson": nn.PoissonNLLLoss,
    "cosine_similarity": nn.CosineEmbeddingLoss,
    "huber": nn.SmoothL1Loss,
}

metric_mapping = {
    "accuracy": torchmetrics.Accuracy,
    "precision": torchmetrics.Precision,
    "recall": torchmetrics.Recall,
    "f1": torchmetrics.F1Score,
    "roc_auc": torchmetrics.AUROC,
    "msl_error": torchmetrics.MeanSquaredLogError,
    "mean_squared_error": torchmetrics.MeanSquaredError,
    "mean_absolute_error": torchmetrics.MeanAbsoluteError,
    "mae": torchmetrics.MeanAbsoluteError,
    "mse": torchmetrics.MeanSquaredError,
    "r2": torchmetrics.R2Score,
    "mape": torchmetrics.MeanAbsolutePercentageError,
    "confusion_matrix": torchmetrics.ConfusionMatrix,
    "cohen_kappa": torchmetrics.CohenKappa,
    "pr_auc": torchmetrics.PrecisionRecallCurve
}

activation_mapping = {"sigmoid": nn.Sigmoid(), "softmax": nn.Softmax(dim=-1), "relu": nn.ReLU()}


### FILE: .\src\models\stream\ensemble.py ###
from river.ensemble import ADWINBaggingClassifier, AdaBoostClassifier, BaggingClassifier, LeveragingBaggingClassifier, SRPClassifier, StackingClassifier, VotingClassifier


### FILE: .\src\models\stream\forest.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.forest import ARFClassifier as _ARFClassifier
from river.forest import AMFClassifier as _AMFClassifier

__all__ = ["ARFClassifier", "AMFClassifier"]


class ARFClassifier(AbstractRiverModel, _ARFClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "arf_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _ARFClassifier.__init__(self, *args, **kwargs)


class AMFClassifier(AbstractRiverModel, _AMFClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "amf_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _AMFClassifier.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\linear_model.py ###
from pathlib import Path
from models.stream import AbstractRiverModel, AbstractMultioutputClassifier
from river.linear_model import ALMAClassifier as _ALMAClassifier
from river.linear_model import LogisticRegression as _LogisticRegression
from river.linear_model import LinearRegression as _LinearRegression

__all__ = ["ALMAClassifier", "LogisticRegression"]


class ALMAClassifier(AbstractRiverModel, _ALMAClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "alma_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _ALMAClassifier.__init__(self, *args, **kwargs)


class LogisticRegression(AbstractRiverModel, _LogisticRegression):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "log_reg_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _LogisticRegression.__init__(self, *args, **kwargs)


class LinearRegression(AbstractRiverModel, _LinearRegression):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "log_reg_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _LinearRegression.__init__(self, *args, **kwargs)


class MultiOutputLogisticRegression(AbstractRiverModel, AbstractMultioutputClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "log_reg_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        AbstractMultioutputClassifier.__init__(self, _LogisticRegression(*args, **kwargs))


if __name__ == '__main__':

    import datasets
    from river import optim
    from tests.settings import *
    from metrics.stream import MacroROCAUC, MicroROCAUC, PRAUC
    from river.metrics import ROCAUC
    from generators.stream import RiverGenerator
    from preprocessing.scalers import MIMICMinMaxScaler
    from preprocessing.imputers import PartialImputer

    ### Try LOS ###
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="LOS")

    imputer = PartialImputer().fit_reader(reader)
    scaler = MIMICMinMaxScaler(imputer=imputer).fit_reader(reader)
    # reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    model = LinearRegression(metrics=["cohen_kappa", "mae"], optimizer=optim.SGD(0.00005))
    generator = RiverGenerator(reader, scaler=scaler)
    model.fit(generator)
    ### Try IHM ###
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="IHM")

    imputer = PartialImputer().fit_reader(reader)
    scaler = MIMICMinMaxScaler(imputer=imputer).fit_reader(reader)
    # reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    model = LogisticRegression(metrics=[PRAUC, ROCAUC])
    generator = RiverGenerator(reader, scaler=scaler)
    model.fit(generator)

    ### Try PHENO ###
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="PHENO")

    imputer = PartialImputer().fit_reader(reader)
    scaler = MIMICMinMaxScaler(imputer=imputer).fit_reader(reader)
    # reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    model = MultiOutputLogisticRegression(metrics=[MacroROCAUC, MicroROCAUC])
    generator = RiverGenerator(reader, scaler=scaler)
    model.fit(generator)
    """
    metric1 = MacroROCAUC()
    metric2 = MicroROCAUC()
    for x, y in generator:
        model.learn_one(x, y)
        y_pred = model.predict_proba_one(x, predict_for=True)
        metric1.update(y, y_pred)
        metric2.update(y, y_pred)
        print(metric1, end="\r")
    print(metric1)
    print(metric2)
    """


### FILE: .\src\models\stream\mappings.py ###
import river.metrics as metrics
from metrics.stream import PRAUC, MacroROCAUC, MicroROCAUC, LOSCohenKappa, LOSClassificationReport

metric_mapping = {
    "accuracy": metrics.Accuracy,
    "precision": metrics.Precision,
    "recall": metrics.Recall,
    "f1": metrics.F1,
    "roc_auc": metrics.ROCAUC(n_thresholds=20),
    "log_loss": metrics.LogLoss,
    "mae": metrics.MAE,
    "mse": metrics.MSE,
    "rmse": metrics.RMSE,
    "r2": metrics.R2,
    "mape": metrics.MAPE,
    "smape": metrics.SMAPE,
    "confusion_matrix": metrics.ConfusionMatrix,
    "classification_report": metrics.ClassificationReport,
    "los_classification_report": LOSClassificationReport,
    "cohen_kappa": LOSCohenKappa(weights="linear"),
    "pr_auc": PRAUC(n_thresholds=20),
    "micro_roc_auc": MicroROCAUC(n_thresholds=20),
    "macro_roc_auc": MacroROCAUC(n_thresholds=20)
}


### FILE: .\src\models\stream\naive_bayes.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.naive_bayes import GaussianNB as _GaussianNB

__all__ = ["GaussianNBClassifier"]


class GaussianNBClassifier(AbstractRiverModel, _GaussianNB):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "gaussian_nb_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _GaussianNB.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\neighbors.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.neighbors import KNNClassifier as _KNNClassifier

__all__ = ["KNNClassifier"]


class KNNClassifier(AbstractRiverModel, _KNNClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "knn_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _KNNClassifier.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\tree.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.tree import HoeffdingAdaptiveTreeClassifier as _HoeffdingAdaptiveTreeClassifier
from river.tree import HoeffdingTreeClassifier as _HoeffdingTreeClassifier

__all__ = ["HoeffdingAdaptiveTreeClassifier", "HoeffdingTreeClassifier"]


class HoeffdingAdaptiveTreeClassifier(AbstractRiverModel, _HoeffdingAdaptiveTreeClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "ha_tree_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _HoeffdingAdaptiveTreeClassifier.__init__(self, *args, **kwargs)


class HoeffdingTreeClassifier(AbstractRiverModel, _HoeffdingTreeClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "h_tree_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _HoeffdingTreeClassifier.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\__init__.py ###
import os
import collections
import copy
import functools
import pickle
import warnings
from typing import Dict
from river.metrics.base import Metric
from river import base
from river import linear_model
from pathlib import Path
from models.stream.mappings import metric_mapping
from generators.stream import RiverGenerator
from models.trackers import RiverHistory, LocalRiverHistory
from tensorflow.keras.utils import Progbar
from utils import to_snake_case, dict_subset
from abc import ABC
from copy import deepcopy
from settings import *


class AbstractRiverModel(ABC):

    def __init__(self, model_path: Path = None, metrics: list = [], name: str = None):
        self._name = (name if name is not None else self._default_name)
        self._model_path = model_path

        self._history = self._init_history(path=model_path)

        if not self.load():
            self._metrics = self._init_metrics(metrics)
            self._train_metrics = self._init_metrics(metrics)
            self._test_metrics = self._init_metrics(metrics, prefix="test")
            self._val_metrics = self._init_metrics(metrics, prefix="val")

    def _init_history(self, path: Path):
        if self._model_path is not None:
            # Persistent history
            self._model_path.mkdir(parents=True, exist_ok=True)
            return RiverHistory(Path(path, "history"))
        else:
            # Mimics the storable
            return LocalRiverHistory()

    def _init_metrics(self, metrics, prefix: str = None) -> Dict[str, Metric]:
        return_metrics = dict()
        for metric in metrics:
            if isinstance(metric, str):
                metric_name = metric
                metric = metric_mapping[metric]
            else:
                metric_name = to_snake_case(metric.__name__)
            if isinstance(metric, type):
                metric = metric()
            if prefix is not None:
                metric_name = f"{prefix}_{metric_name}"
            return_metrics[metric_name] = metric
        return return_metrics

    def _update_metrics(self, metrics: Dict[str, Metric], x, y_true, y_pred):
        y_label = None

        for _, metric in metrics.items():
            if not hasattr(metric, "requires_labels") or not metric.requires_labels:
                metric.update(y_true, y_pred)
            else:
                if y_label is None:
                    y_label = self.predict_one(x)
                metric.update(y_true, y_label)

    def _update_history(self, history_dict: dict, metrics: Dict[str, Metric]):
        for name, metric in metrics.items():
            try:
                history_dict[name] = metric.get()
            except NotImplementedError as e:
                history_dict[name] = metric

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def fit(self,
            train_generator: RiverGenerator,
            val_generator: RiverGenerator = None,
            model_path: Path = None):
        if model_path is not None:
            self._model_path = model_path
            self._history = self._init_history(path=model_path)
        self.load()
        self.train(generator=train_generator, has_val=val_generator is not None)
        if val_generator is not None:
            self.evaluate(generator=val_generator, is_val=True)

        self.save()
        return self._history.to_json()

    def train(self, generator: RiverGenerator, has_val: bool = False):
        generator_size = len(generator)
        self._train_progbar = Progbar(generator_size)

        for batch_idx, (x, y) in enumerate(generator):
            self.learn_one(x, y)
            if hasattr(self, "predict_proba_one"):
                y_pred = self.predict_proba_one(x)
            else:
                y_pred = self.predict_one(x)
            self._update_metrics(metrics=self._train_metrics, x=x, y_true=y, y_pred=y_pred)
            self._train_progbar.update(batch_idx + 1,
                                       values=self._get_metrics(self._train_metrics),
                                       finalize=(batch_idx == generator_size and not has_val))

        self._update_history(history_dict=self._history.train_metrics, metrics=self._train_metrics)
        return self._train_metrics

    def _get_metrics(self, metrics: Dict[str, Metric]):
        keys = list([metric for metric in metrics.keys() if self._allowed_key(metric)])
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=DeprecationWarning)
            values = [metrics[key].get() for key in keys]
        return tuple(zip(keys, values))

    def predict(self):
        pass

    def test(self, generator: RiverGenerator):
        metrics = self.evaluate(generator=generator, is_test=True)
        self._history.to_json()
        return metrics

    def evaluate(self, generator: RiverGenerator, is_val: bool = False, is_test: bool = False):
        if is_val:
            eval_metric = self._val_metrics
        elif is_test:
            eval_metric = self._test_metrics
        else:
            eval_metric = deepcopy(self._metrics)

        for _, (x, y) in enumerate(generator):
            y_pred = self.predict_one(x)
            self._update_metrics(metrics=eval_metric, x=x, y_true=y, y_pred=y_pred)

        if is_val:
            self._train_progbar.update(self._train_progbar.target,
                                       values=self._get_metrics(self._train_metrics) +
                                       self._get_metrics(self._val_metrics))

            self._update_history(history_dict=self._history.val_metrics, metrics=self._val_metrics)

        if is_test:
            self._update_history(history_dict=self._history.test_metrics,
                                 metrics=self._test_metrics)

        return eval_metric

    def save(self, model_path=None):
        """_summary_
        """
        if model_path is not None:
            storage_path = Path(model_path, f"{self._name}.pkl")
        elif self._model_path is not None and self._name is not None:
            storage_path = Path(self._model_path, f"{self._name}.pkl")
        else:
            storage_path = None
        if storage_path is not None:
            with open(storage_path, "wb") as save_file:
                # Storables can't be pickled
                save_dict = dict_subset(
                    self.__dict__,
                    list(set(self.__dict__.keys()) - set(["_history", "_train_progbar"])))
                pickle.dump(obj=save_dict, file=save_file, protocol=2)

    def load(self):
        """_summary_

        Returns:
            _type_: _description_
        """

        def inner_load(self, path: Path):
            if path.is_file():
                if os.path.getsize(path) > 0:
                    with open(path, "rb") as load_file:
                        load_params = pickle.load(load_file)
                    for key, value in load_params.items():
                        setattr(self, key, value)
                    return 1
            return 0

        if self._model_path is not None and self._name is not None:
            return inner_load(self, Path(self._model_path, f"{self._name}.pkl"))

        return 0


class AbstractMultioutputClassifier(base.Wrapper, base.Classifier):

    def __init__(self, classifier):
        self.classifier = classifier
        new_clf = functools.partial(copy.deepcopy, classifier)
        self.classifiers = collections.defaultdict(new_clf)
        self.classes = set()

    @property
    def _wrapped_model(self):
        return self.classifier

    @property
    def _multiclass(self):
        return True

    @classmethod
    def _unit_test_params(cls):
        yield {"classifier": linear_model.LogisticRegression()}

    def learn_one(self, x, y, **kwargs):
        for label_name, y_label in y.items():
            self.classes.add(label_name)
            self.classifiers[label_name].learn_one(x, y_label)

    def predict_one(self, x, **kwargs):
        predictions = {}
        for label in self.classifiers:
            predictions[label] = self.classifiers[label].predict_one(x)

        return predictions

    def predict_proba_one(self, x, predict_for=None):
        predictions = {}
        for label in self.classifiers:
            predictions[label] = self.classifiers[label].predict_proba_one(x)
            if predict_for is not None:
                predictions[label] = predictions[label][predict_for]

        return predictions


### FILE: .\src\models\tf2\logistic_regression.py ###
#
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras import layers
from tensorflow.keras.optimizers import RMSprop, Adam
from managers import ReducedCheckpointManager


class IncrementalLogReg(object):

    def __init__(self,
                 task: str,
                 input_dim: int = 714,
                 alpha: float = 0.001,
                 max_iter: int = 1000,
                 output_dim: int = None,
                 random_state: int = 42) -> None:

        self.max_iter = max_iter
        # TODO! not realy happy with this solution
        self.alpha = alpha
        activation = {
            "DECOMP": 'sigmoid',
            "IHM": 'sigmoid',
            "LOS": 'softmax',
            "PHENO": 'softmax',
            None: None if output_dim == 1 else 'softmax'
        }

        num_classes = {"DECOMP": 1, "IHM": 1, "LOS": 1, "PHENO": 25, None: output_dim}

        self._num_outputs = num_classes[task]

        input = layers.Input(shape=(input_dim), name='x')
        x = layers.Dense(self._num_outputs, activation=activation[task], input_dim=input_dim)(input)
        self._model = Model(inputs=input, outputs=x)
        self._output_tensor = x

    def load(self, checkpoint_folder):
        manager = ReducedCheckpointManager(checkpoint_folder)

        if not manager.is_empty():
            self._model = manager.load_model()

        return

    def latest_epoch(self, checkpoint_folder):
        manager = ReducedCheckpointManager(checkpoint_folder)
        return manager.latest_epoch()

    def __getattr__(self, name: str):
        """ Surrogate to the _model attributes internals.

        Args:
            name (str): name of the method/attribute

        Returns:
            any: method/attribute of _model
        """
        if name in ["load", "_model", "compile"]:
            return self.__getattribute__(name)
        return getattr(self._model, name)

    def compile(self,
                optimizer="rmsprop",
                metrics=None,
                loss_weights=None,
                weighted_metrics=None,
                run_eagerly=None,
                steps_per_execution=None,
                jit_compile=None,
                **kwargs):
        """_summary_

        Args:
            optimizer (str, optional): _description_. Defaults to "rmsprop".
            metrics (_type_, optional): _description_. Defaults to None.
            loss_weights (_type_, optional): _description_. Defaults to None.
            weighted_metrics (_type_, optional): _description_. Defaults to None.
            run_eagerly (_type_, optional): _description_. Defaults to None.
            steps_per_execution (_type_, optional): _description_. Defaults to None.
            jit_compile (_type_, optional): _description_. Defaults to None.

        Returns:
            _type_: _description_
        """
        if isinstance(optimizer, str):
            optimizer_switch = {"rmsprop": RMSprop(self.alpha), "adam": Adam(self.alpha)}
            optimizer = optimizer_switch[optimizer]
        else:
            optimizer.learning_rate = self.alpha
        if self._num_outputs == 1:
            loss = "binary_crossentropy"
        else:
            loss = "categorical_crossentropy"
        return self._model.compile(optimizer=optimizer,
                                   loss=loss,
                                   metrics=metrics,
                                   loss_weights=loss_weights,
                                   weighted_metrics=weighted_metrics,
                                   run_eagerly=run_eagerly,
                                   steps_per_execution=steps_per_execution,
                                   **kwargs)


### FILE: .\src\models\tf2\lstm.py ###
import pdb
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras import layers
from tensorflow.keras.layers import Bidirectional


class LSTMNetwork(Model):
    """
    """

    def __init__(self,
                 layer_size,
                 dropout,
                 input_dim,
                 bidirectional=False,
                 recurrent_dropout=0.,
                 task=None,
                 target_repl=False,
                 output_dim=1,
                 depth=1):
        """
        """
        self.layer_size = layer_size
        self.dropout_rate = dropout
        self.recurrent_dropout = recurrent_dropout
        self.depth = depth

        final_activation = {
            "DECOMP": 'sigmoid',
            "IHM": 'sigmoid',
            "LOS": 'softmax',
            "PHENO": 'softmax',
            None: None if output_dim == 1 else 'softmax'
        }

        num_classes = {"DECOMP": 1, "IHM": 1, "LOS": 10, "PHENO": 25, None: output_dim}

        # Input layers and masking
        input = layers.Input(shape=(None, input_dim), name='x')

        x = layers.Masking()(input)

        # TODO: compare bidirectional runs to one directiona

        if type(layer_size) == int:
            iterator = [layer_size] * (depth - 1)
        else:
            iterator = layer_size[:-1]
            layer_size = layer_size[-1]

        for i, size in enumerate(iterator):
            if bidirectional:
                num_units = size // 2
                x = Bidirectional(
                    layers.LSTM(units=num_units,
                                activation='tanh',
                                return_sequences=True,
                                recurrent_dropout=recurrent_dropout,
                                dropout=dropout,
                                name=f"lstm_hidden_{i}"))(x)
            else:
                x = layers.LSTM(units=size,
                                activation='tanh',
                                return_sequences=True,
                                recurrent_dropout=recurrent_dropout,
                                dropout=dropout,
                                name=f"lstm_hidden_{i}")(x)

        # Output module of the network
        return_sequences = target_repl
        '''        
        x = Bidirectional(layers.LSTM(units=layer_size,
                                      activation='tanh',
                                      return_sequences=return_sequences,
                                      dropout=dropout_rate,
                                      recurrent_dropout=recurrent_dropout,
                                      name=f"lstm_hidden_{depth}"))(x)
        '''
        x = layers.LSTM(units=layer_size,
                        activation='tanh',
                        return_sequences=return_sequences,
                        dropout=dropout,
                        recurrent_dropout=recurrent_dropout)(x)

        x = layers.Dense(num_classes[task], activation=final_activation[task])(x)

        super(LSTMNetwork, self).__init__(inputs=[input], outputs=[x])


### FILE: .\src\models\tf2\transformer.py ###
"""https://keras.io/examples/timeseries/timeseries_transformer_classification/"""

from tensorflow import keras
from tensorflow.keras import layers


def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size,
                                  num_heads=num_heads,
                                  dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def Transformer(input_shape,
                head_size,
                num_heads,
                ff_dim,
                num_transformer_blocks,
                mlp_units,
                dropout=0,
                mlp_dropout=0,
                n_classes=2):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format="channels_first")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation="softmax")(x)
    return keras.Model(inputs, outputs)


### FILE: .\src\models\tf2\__init__.py ###


### FILE: .\src\pipelines\pytorch.py ###
import datasets
from pathlib import Path
from generators.pytorch import TorchGenerator
from preprocessing.scalers import AbstractScaler
from utils.IO import *
from datasets.readers import ProcessedSetReader
from pipelines import AbstractPipeline


class TorchPipeline(AbstractPipeline):

    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        return TorchGenerator(reader=reader, scaler=scaler, **generator_options)

    def fit(self,
            epochs: int,
            no_subdirs: bool = False,
            result_name: str = None,
            patience: int = None,
            save_best_only: bool = True,
            restore_best_weights: bool = True,
            sample_weights: dict = None,
            val_frequency=1,
            restore_last_run: bool = False):

        self._init_result_path(result_name=result_name,
                               restore_last_run=restore_last_run,
                               no_subdirs=no_subdirs)

        info_io(f"Training model in directory\n{self._result_path}")
        self._model.fit(self._train_generator,
                        model_path=self._result_path,
                        epochs=epochs,
                        patience=patience,
                        save_best_only=save_best_only,
                        restore_best_weights=restore_best_weights,
                        sample_weights=sample_weights,
                        val_frequency=val_frequency,
                        val_generator=self._val_generator)

        return self._model


if __name__ == "__main__":
    from models.pytorch.lstm import LSTMNetwork
    from tests.settings import *
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task="IHM")

    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)

    model = LSTMNetwork(10,
                        0.2,
                        59,
                        bidirectional=False,
                        recurrent_dropout=0.,
                        task=None,
                        target_repl=False,
                        output_dim=1,
                        depth=1)

    pipe = TorchPipeline(storage_path=Path(TEMP_DIR, "torch_pipeline"),
                         reader=reader,
                         model=model,
                         compile_options={
                             "optimizer": "adam",
                             "loss": "logits_binary_crossentropy"
                         },
                         generator_options={
                             "batch_size": 1,
                             "shuffle": True
                         }).fit(epochs=10, save_best_only=False)


### FILE: .\src\pipelines\stream.py ###
import datasets
from typing import Union
from pathlib import Path
from generators.stream import RiverGenerator
from preprocessing.scalers import AbstractScaler
from preprocessing.imputers import AbstractImputer, PartialImputer
from datasets.readers import ProcessedSetReader, SplitSetReader
from pipelines import AbstractPipeline
from utils.IO import *
from settings import *


class RiverPipeline(AbstractPipeline):

    def __init__(self,
                 storage_path: Path,
                 reader: Union[ProcessedSetReader, SplitSetReader],
                 model,
                 generator_options: dict = {},
                 model_options: dict = {},
                 scaler_options: dict = {},
                 split_options: dict = {},
                 imputer_options: dict = {},
                 scaler: AbstractScaler = None,
                 scaler_type: str = "minmax",
                 imputer: AbstractImputer = None):

        self._reader = self._split_data(data_split_options=split_options, reader=reader)
        self._imputer = self._init_imputer(storage_path=storage_path,
                                           imputer=imputer,
                                           imputer_options=imputer_options,
                                           reader=reader)

        scaler_options["imputer"] = self._imputer

        super().__init__(storage_path=storage_path,
                         reader=reader,
                         model=model,
                         generator_options=generator_options,
                         model_options=model_options,
                         scaler_options=scaler_options,
                         compile_options={},
                         split_options=split_options,
                         scaler=scaler,
                         scaler_type=scaler_type)

    def _init_imputer(self, storage_path: Path, imputer_options: dict, imputer: AbstractImputer,
                      reader: Union[ProcessedSetReader, SplitSetReader]):
        if isinstance(reader, SplitSetReader):
            reader = reader.train
        if imputer is not None and isinstance(imputer, AbstractImputer):
            return imputer.fit_reader(reader)
        elif imputer is not None and imputer(imputer, type):
            imputer = imputer(storage_path=storage_path, **imputer_options)
            return imputer.fit_reader(reader)
        elif imputer is None:
            imputer = PartialImputer(storage_path=storage_path, **imputer_options)
            return imputer.fit_reader(reader)

    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        return RiverGenerator(reader=reader, scaler=scaler, **generator_options)

    def fit(self,
            result_name: str = None,
            no_subdirs: bool = False,
            restore_last_run: bool = False,
            **kwargs):

        self._init_result_path(result_name=result_name,
                               restore_last_run=restore_last_run,
                               no_subdirs=no_subdirs)

        info_io(f"Training model in directory\n{self._result_path}")
        self._model.fit(self._train_generator,
                        model_path=self._result_path,
                        val_generator=self._val_generator,
                        **kwargs)

        return self._model

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def print_metrics(self, metrics: dict):
        stringified_dict = {k: str(v) for k, v in metrics.items()}

        # Find the maximum length of keys and values
        max_key_length = max(len(key) for key in stringified_dict.keys()) + 4
        max_value_length = max(
            len(value) for key, value in stringified_dict.items() if self._allowed_key(key)) + 4

        # Print the header with calculated padding
        msg = ""
        msg += f"{'Key':<{max_key_length}} {'Value':<{max_value_length}}\n"
        msg += '-' * (max_key_length + max_value_length + 1) + "\n"

        # Print the dictionary items with calculated padding
        non_numeric_values = None
        for key, value in stringified_dict.items():
            if self._allowed_key(key):
                msg += f"{key:<{max_key_length}} {value:<{max_value_length}}" + "\n"
            else:
                non_numeric_values = (key, value)
        if non_numeric_values is not None:
            msg += "\n" + "\n".join(non_numeric_values)

        info_io(msg)

    def test(self, **kwargs):

        info_io(f"Testing model in directory\n{self._result_path}")
        metrics = self._model.test(self._test_generator, **kwargs)
        self.print_metrics(metrics)

        return metrics


if __name__ == '__main__':
    import datasets
    from tests.settings import *
    # from models.stream.linear_model import LogisticRegression
    from metrics.stream import PRAUC

    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="IHM")

    model_path = Path(TEMP_DIR, "arf_ihm")
    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    # model = LogisticRegression(model_path=model_path, metrics=["accuracy", PRAUC])
    pipe = RiverPipeline(storage_path=Path(TEMP_DIR, "river_pipeline"),
                         reader=reader,
                         model=model,
                         generator_options={
                             "shuffle": True
                         }).fit()


### FILE: .\src\pipelines\tf2.py ###
import datasets
import re
from pathlib import Path
from models.callbacks import HistoryCheckpoint
from generators.tf2 import TFGenerator
from preprocessing.scalers import AbstractScaler
from managers import HistoryManager
from managers import CheckpointManager
from datasets.readers import ProcessedSetReader
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from utils.IO import *
from pipelines import AbstractPipeline


class TFPipeline(AbstractPipeline):

    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        return TFGenerator(reader=reader, scaler=scaler, **generator_options)

    def _init_callbacks(self,
                        patience: int = None,
                        restore_best_weights=True,
                        save_weights_only: bool = False,
                        save_best_only: bool = True):
        self._standard_callbacks = []
        if "val" in self._split_names:
            if patience is not None:
                es_callback = EarlyStopping(patience=patience,
                                            restore_best_weights=restore_best_weights)
                self._standard_callbacks.append(es_callback)

        if self._result_path is not None:
            cp_pattern = str(Path(self._result_path, "cp-{epoch:04d}.ckpt"))
            cp_callback = ModelCheckpoint(filepath=cp_pattern,
                                          save_weights_only=save_weights_only,
                                          save_best_only=save_best_only,
                                          verbose=0)
            self._standard_callbacks.append(cp_callback)

            hist_callback = HistoryCheckpoint(self._result_path)
            self._standard_callbacks.append(hist_callback)

    def _init_managers(self, epochs: int):
        self._hist_manager = HistoryManager(str(self._result_path))

        self._manager = CheckpointManager(str(self._result_path), epochs, custom_objects={})

    def fit(self,
            epochs: int,
            no_subdirs: bool = False,
            result_name: str = None,
            patience: int = None,
            restore_best_weights=True,
            save_weights_only: bool = False,
            class_weight: dict = None,
            sample_weight: dict = None,
            save_best_only: bool = True,
            callbacks: list = [],
            validation_freq: int = 1,
            restore_last_run: bool = False):

        self._init_result_path(result_name=result_name,
                               restore_last_run=restore_last_run,
                               no_subdirs=no_subdirs)
        info_io(f"Training model in directory\n{self._result_path}")
        self._init_managers(epochs)
        self._init_callbacks(patience=patience,
                             restore_best_weights=restore_best_weights,
                             save_weights_only=save_weights_only,
                             save_best_only=save_best_only)

        self._model.fit(self._train_generator,
                        validation_data=self._val_generator,
                        epochs=epochs,
                        steps_per_epoch=self._train_generator.steps,
                        callbacks=callbacks + self._standard_callbacks,
                        class_weight=class_weight,
                        sample_weight=sample_weight,
                        initial_epoch=self._manager.latest_epoch(),
                        validation_steps=self._val_steps,
                        validation_freq=validation_freq)

        self._hist_manager.finished()
        _, best_epoch = self._hist_manager.best
        self._manager.clean_directory(best_epoch)

        return self._model


if __name__ == "__main__":
    from models.tf2.lstm import LSTMNetwork
    from tests.settings import *
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task="IHM")

    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)

    model = LSTMNetwork(10,
                        0.2,
                        59,
                        bidirectional=False,
                        recurrent_dropout=0.,
                        task=None,
                        target_repl=False,
                        output_dim=1,
                        depth=1)

    pipe = TFPipeline(storage_path=Path(TEMP_DIR, "tf_pipeline"),
                      reader=reader,
                      model=model,
                      compile_options={
                          "optimizer": "adam",
                          "loss": "binary_crossentropy"
                      },
                      generator_options={
                          "batch_size": 1,
                          "shuffle": True
                      }).fit(epochs=10, save_best_only=False)

    pipe = TFPipeline(storage_path=Path(TEMP_DIR, "tf_pipeline"),
                      reader=reader,
                      model=model,
                      compile_options={
                          "optimizer": "adam",
                          "loss": "binary_crossentropy"
                      },
                      generator_options={
                          "batch_size": 1,
                          "shuffle": True
                      }).fit(epochs=10, save_best_only=False)

    pipe = TFPipeline(storage_path=Path(TEMP_DIR, "tf_pipeline"),
                      reader=reader,
                      model=model,
                      compile_options={
                          "optimizer": "adam",
                          "loss": "binary_crossentropy"
                      },
                      generator_options={
                          "batch_size": 1,
                          "shuffle": True
                      }).fit(epochs=20, save_best_only=False, restore_last_run=True)


### FILE: .\src\pipelines\__init__.py ###
import re
import datasets
from typing import Union
from pathlib import Path
from generators.tf2 import TFGenerator
from preprocessing.scalers import AbstractScaler, MIMICMinMaxScaler, MIMICStandardScaler, MIMICMaxAbsScaler, MIMICRobustScaler
from datasets.readers import ProcessedSetReader
from datasets.readers import ProcessedSetReader, SplitSetReader
from abc import ABC, abstractmethod
from utils.IO import *


class AbstractPipeline(ABC):

    def __init__(
        self,
        storage_path: Path,
        reader: Union[ProcessedSetReader, SplitSetReader],
        model,
        generator_options: dict = {},
        model_options: dict = {},
        scaler_options: dict = {},
        compile_options: dict = {},
        split_options: dict = {},
        scaler: AbstractScaler = None,
        scaler_type: str = "minmax",
    ):
        self._storage_path = storage_path
        self._storage_path.mkdir(parents=True, exist_ok=True)
        self._model = model
        self._generator_options = generator_options
        self._data_split_options = split_options
        self._split_names = ["train"]
        self._reader = self._split_data(data_split_options=split_options, reader=reader)

        self._scaler = self._init_scaler(storage_path=storage_path,
                                         scaler_type=scaler_type,
                                         scaler_options=scaler_options,
                                         scaler=scaler,
                                         reader=reader)

        self._init_generators(generator_options=generator_options,
                              scaler=self._scaler,
                              reader=reader)

        self._init_model(model=model, model_options=model_options, compiler_options=compile_options)

    def _init_model(self, model, model_options, compiler_options):
        if isinstance(model, type):
            self._model = model(**model_options)
        if hasattr(model, "optimizer") and model.optimizer is None:
            self._model.compile(**compiler_options)

    def _split_data(self, data_split_options: dict, reader: Union[ProcessedSetReader,
                                                                  SplitSetReader]):
        if isinstance(reader, ProcessedSetReader) and data_split_options:
            return datasets.train_test_split(reader, **data_split_options)
        return reader

    @abstractmethod
    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        ...

    def _init_scaler(self, storage_path: Path, scaler_type: str, scaler_options: dict,
                     scaler: Union[AbstractScaler, type], reader: Union[ProcessedSetReader,
                                                                        SplitSetReader]):
        if isinstance(reader, SplitSetReader):
            reader = reader.train
        if not scaler_type in ["minmax", "standard", "maxabs", "robust"]:
            raise ValueError(
                f"Invalid scaler type: {scaler_type}. Must be either 'minmax' or 'standard'.")

        if scaler is not None and isinstance(scaler, AbstractScaler):
            return scaler.fit_reader(reader)
        elif scaler is not None and isinstance(scaler, type):
            scaler = scaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "minmax":
            scaler = MIMICMinMaxScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "standard":
            scaler = MIMICStandardScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "maxabs":
            scaler = MIMICMaxAbsScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "robust":
            scaler = MIMICRobustScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)

    @staticmethod
    def _check_generator_sanity(set_name: str, reader: ProcessedSetReader, generator: TFGenerator,
                                generator_options):
        if not len(generator):
            if reader.subject_ids:
                msg = f"{set_name.capitalize()} generator has no steps, while {len(reader.subject_ids)}"
                msg += f" subjects are present in reader. "
                if "batch_size" in generator_options:
                    msg += f"Consider reducing batch size: {generator_options['batch_size']}."

                raise ValueError(msg)
            else:
                raise ValueError(
                    f"{set_name.capitalize()} reader has no subjects. Consider adjusting the {set_name} split size."
                )

    def _init_generators(self, generator_options: dict, scaler: AbstractScaler,
                         reader: Union[ProcessedSetReader, SplitSetReader]):
        if isinstance(reader, ProcessedSetReader):
            self._train_generator = self._create_generator(reader=reader,
                                                           scaler=scaler,
                                                           **generator_options)

            self._val_generator = None
            self._val_steps = None
        elif isinstance(reader, SplitSetReader):
            self._train_generator = self._create_generator(reader=reader.train,
                                                           scaler=scaler,
                                                           **generator_options)

            self._check_generator_sanity(set_name="train",
                                         generator_options=generator_options,
                                         reader=reader.train,
                                         generator=self._train_generator)

            if "val" in reader.split_names:
                self._split_names.append("val")
                self._val_generator = self._create_generator(reader=reader.val,
                                                             scaler=scaler,
                                                             **generator_options)

                self._val_steps = len(self._val_generator)
                self._check_generator_sanity(set_name="val",
                                             generator_options=generator_options,
                                             reader=reader.val,
                                             generator=self._val_generator)

            else:
                self._val_generator = None
                self._val_steps = None

            if "test" in reader.split_names:
                self._split_names.append("test")
                self._test_generator = self._create_generator(reader=reader.test,
                                                              scaler=scaler,
                                                              **generator_options)

                self._check_generator_sanity(set_name="test",
                                             generator_options=generator_options,
                                             reader=reader.test,
                                             generator=self._test_generator)

    def _init_result_path(self,
                          result_name: str,
                          restore_last_run: bool = False,
                          no_subdirs: bool = False):
        if no_subdirs:
            if result_name is not None:
                warn_io("Ignoring result_name, as no_subdirs is set to True.")
            self._result_path = self._storage_path
        elif result_name is not None:
            self._result_path = Path(self._storage_path, result_name)
        else:
            # Iterate over files in the directory
            pattern = re.compile(r"(\d+)")
            result_numbers = []
            for file in self._storage_path.iterdir():
                if file.is_dir() and file.name.startswith("results"):
                    match = pattern.search(file.name)
                    if match:
                        result_numbers.append(int(match.group(0)))

            # Determine the next number
            if not result_numbers:
                next_number = 0
            else:
                next_number = max(result_numbers, default=0) + int(not restore_last_run)
            self._result_path = Path(self._storage_path, f"results{next_number:04d}")
        self._result_path.mkdir(parents=True, exist_ok=True)

    @abstractmethod
    def fit(self, epochs: int, result_name: str = None, no_subdirs: bool = False, *args, **kwargs):
        ...

    # def test(self):
    #     self._model.test(self._test_generator)


### FILE: .\src\preprocessing\discretizers.py ###

import pandas as pd
import numpy as np
import os
import json
import datetime
from multiprocess import Manager
from pathlib import Path
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from pandas.api.types import is_datetime64_any_dtype as is_datetime
from utils import dict_subset

from settings import *
from utils.IO import *
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from datasets.mimic_utils import convert_dtype_value
from datasets.trackers import PreprocessingTracker
from . import AbstractProcessor


class MIMICDiscretizer(AbstractProcessor):
    """ Discretize batch data provided by reader.
    """

    def __init__(self,
                 task: str,
                 reader: ProcessedSetReader,
                 tracker: PreprocessingTracker,
                 storage_path: Path,
                 time_step_size: float = None,
                 start_at_zero: bool = True,
                 impute_strategy: str = "previous",
                 mode: str = "legacy",
                 eps: float = 1e-6,
                 verbose: bool = False):
        """
        """
        self._storage_path = storage_path
        self._writer = (None if storage_path is None else DataSetWriter(self._storage_path))
        self._reader = reader
        if tracker is not None:
            self._tracker = tracker
        else:
            self._tracker = (None if storage_path is None else PreprocessingTracker(
                Path(storage_path, "progress")))
        self._lock = Manager().Lock()
        self._verbose = verbose
        self._discretized_reader = (None if storage_path is None else ProcessedSetReader(
            root_path=storage_path))

        self._time_step_size = time_step_size
        self._start_at_zero = start_at_zero
        self._eps = eps
        if not impute_strategy in ["normal", "previous", "next", "zero"]:
            raise ValueError(
                f"Impute strategy must be one of 'normal', 'previous', 'zero' or 'next'. Impute strategy is {impute_strategy}"
            )
        self._impute_strategy = impute_strategy
        if not mode in ["legacy", "experimental"]:
            raise ValueError(f"Mode must be one of 'legacy' or 'experimental'. Mode is {mode}")
        if mode == "experimental":
            raise NotADirectoryError("Implemented but untested. Will yield nan values.")
        self._mode = mode
        if not task in TASK_NAMES:
            raise ValueError(f"Task name must be one of {TASK_NAMES}. Task name is {task}")
        self._task_name = task

        with open(Path(os.getenv("CONFIG"), "datasets.json")) as file:
            config_dictionary = json.load(file)
            self._dtypes = config_dictionary["timeseries"]["dtype"]
        with open(Path(os.getenv("CONFIG"), "discretizer_config.json")) as file:
            config_dictionary = json.load(file)
            self._possible_values = config_dictionary['possible_values']
            self._is_categorical = config_dictionary['is_categorical_channel']
            self._impute_values = config_dictionary['normal_values']

    @property
    def tracker(self) -> PreprocessingTracker:
        return self._tracker

    @property
    def subjects(self) -> list:
        """_summary_

        Returns:
            list: _description_
        """
        if self._reader is None:
            return []
        return self._reader.subject_ids

    def save_data(self, subjects: list = None) -> None:
        """_summary_

        Args:
            task_path (_type_, optional): _description_. Defaults to None.
        """
        if self._writer is None:
            info_io("No storage path provided. Data will not be saved.")
            return
        with self._lock:
            if subjects is None:
                self._writer.write_bysubject({"X": self._X_discretized}, file_type="hdf5")
                self._writer.write_bysubject({"y": self._y_discretized}, file_type="hdf5")
            else:
                self._writer.write_bysubject({"X": dict_subset(self._X_discretized, subjects)},
                                             file_type="hdf5")
                self._writer.write_bysubject({"y": dict_subset(self._y_discretized, subjects)},
                                             file_type="hdf5")

        return

    def transform_subject(self, subject_id: int):
        X_processed, y_processed = self._reader.read_sample(subject_id,
                                                            read_ids=True,
                                                            data_type=pd.DataFrame).values()
        X = {subject_id: X_processed}
        y = {subject_id: y_processed}

        X_discretized = self.transform(X, y)
        if X_discretized is None:
            return None, None
        if self._tracker is None:
            return X_discretized, y

        with self._lock:
            tracking_info = self._tracker.subjects[subject_id]
        return (X_discretized, y), tracking_info

    def transform(self, X_dict, y_dict):
        """
        """
        n_subjects = 0
        n_stays = 0
        n_samples = 0
        n_skip = 0

        if self._verbose:
            info_io(f"Discretizing processed data:\n"
                    f"Discretized subjects: {0}\n"
                    f"Discretized stays: {0}\n"
                    f"Discretized samples: {0}\n"
                    f"Skipped subjects: {0}")

        self._samples_processed = 0

        self._X_discretized = dict()
        self._y_discretized = dict()

        for subject_id in X_dict.keys():
            X_subject = X_dict[subject_id]
            self._X_discretized[subject_id] = dict()
            self._y_discretized[subject_id] = dict()
            tracking_info = dict()

            for stay_id in X_subject:
                X_df = X_subject[stay_id]
                if self._mode == "experimental" and self._impute_strategy in ["previous", "next"]:
                    X_df = self._impute_data(X_df)
                    X_df = self._categorize_data(X_df)
                    X_df = self._bin_data(X_df)
                else:
                    X_df = self._categorize_data(X_df)
                    X_df = self._bin_data(X_df)
                    X_df = self._impute_data(X_df)
                self._X_discretized[subject_id][stay_id] = X_df
                self._y_discretized[subject_id][stay_id] = y_dict[subject_id][stay_id]

                tracking_info[stay_id] = len(y_dict[subject_id][stay_id])

                if self._verbose:
                    info_io(
                        f"Discretizing processed data:\n"
                        f"Discretized subjects: {n_subjects}\n"
                        f"Discretized stays: {n_stays}\n"
                        f"Discretized samples: {n_samples}"
                        f"Skipped subjects: {n_skip}",
                        flush_block=True)

            n_subjects += 1
            if self._tracker is not None:
                with self._lock:
                    self._tracker.subjects.update({subject_id: tracking_info})

            if not len(self._y_discretized[subject_id]) or not len(self._X_discretized[subject_id]):
                del self._y_discretized[subject_id]
                del self._X_discretized[subject_id]
                n_skip += 1
            else:
                n_subjects += 1

        if self._verbose:
            info_io(
                f"Discretizing processed data:\n"
                f"Discretized subjects: {n_subjects}\n"
                f"Discretized stays: {n_stays}\n"
                f"Discretized samples: {n_samples}"
                f"Skipped subjects: {n_skip}",
                flush_block=True)

        return self._X_discretized

    def _bin_data(self, X):
        """
        """

        if self._time_step_size is not None:
            # Get data frame parameters
            start_timestamp = (0 if self._start_at_zero else X.index[0])

            if is_datetime(X.index):
                ts = list((X.index - start_timestamp) / datetime.timedelta(hours=1))
            else:
                ts = list(X.index - start_timestamp)

            # Maps sample_periods to discretization bins
            tsid_to_bins = list(map(lambda x: int(x / self._time_step_size - self._eps), ts))
            # Tentative solution
            if self._start_at_zero:
                N_bins = tsid_to_bins[-1] + 1
            else:
                N_bins = tsid_to_bins[-1] - tsid_to_bins[0] + 1

            # Reduce DataFrame to bins, keep original channels
            X['bins'] = tsid_to_bins
            X = X.groupby('bins').last()
            X = X.reindex(range(N_bins))

        # return categorized_data
        return X

    def _impute_data(self, X):
        """
        """
        if self._start_at_zero:
            tsid_to_bins = list(map(lambda x: int(x / self._time_step_size - self._eps), X.index))
            start_count = 0
            while not start_count in tsid_to_bins:
                start_count += 1
                X = pd.concat([pd.DataFrame(pd.NA, index=[0], columns=X.columns, dtype="Int32"), X])

        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=self._possible_values.keys())
        if self._impute_strategy == "normal":
            for column in X:
                if column in self._impute_values.keys():
                    X[column] = X[column].replace(
                        np.nan,
                        convert_dtype_value(self._impute_values[column], self._dtypes[column]))
                elif column.split("->")[0] in self._impute_values.keys():
                    cat_name, cat_value = column.split("->")
                    X[column] = X[column].replace(
                        np.nan, (1.0 if cat_value == self._impute_values[cat_name] else 0.0))

        elif self._impute_strategy in ["previous", "next"]:
            X = (X.ffill() if self._impute_strategy == "previous" else X.bfill())
            for column in X:
                if X[column].isnull().values.any():
                    if column in self._impute_values:
                        impute_value = convert_dtype_value(self._impute_values[column],
                                                           self._dtypes[column])
                    else:
                        cat_name, cat_value = column.split("->")
                        impute_value = (1.0 if cat_value == self._impute_values[cat_name] else 0.0)
                    X[column] = X[column].replace(np.nan, impute_value)
        elif self._impute_strategy == "zero":
            X = X.fillna(0)
        return X

    def _impute(self, train_data, imp_strategy='mean'):
        """
        """
        imputer = SimpleImputer(missing_values=np.nan, strategy=imp_strategy)
        imputer.fit(train_data)
        train_data = imputer.transform(train_data)

        return train_data

    def _categorize_data(self, X):
        """
        """
        categorized_data = X

        for column in X:
            if not self._is_categorical[column]:
                continue

            categories = [str(cat) for cat in self._possible_values[column]]

            # Finding nan indices
            nan_indices = categorized_data[column].isna()
            if not nan_indices.all() and nan_indices.any():
                # Handling non-NaN values: encode them
                non_nan_values = categorized_data.pop(column).dropna().astype(
                    'string').values.reshape(-1, 1)
                encoder = OneHotEncoder(categories=list(
                    np.array(categories).reshape(1, len(categories))),
                                        handle_unknown='ignore')
                encoded_values = encoder.fit_transform(non_nan_values).toarray()

                # Initialize an array to hold the final encoded values including NaNs
                encoded_channel = np.full((categorized_data.shape[0], len(categories)), np.nan)

                # Fill in the encoded values at the right indices
                non_nan_indices = ~nan_indices
                encoded_channel[non_nan_indices] = encoded_values
            elif not nan_indices.any():
                encoder = OneHotEncoder(categories=[np.array(categories).reshape(-1)],
                                        handle_unknown='ignore')
                encoded_channel = encoder.fit_transform(
                    categorized_data.pop(column).astype('string').values.reshape(-1, 1)).toarray()
            else:
                categorized_data.pop(column)
                encoded_channel = np.full((categorized_data.shape[0], len(categories)), np.nan)

            categorized_data = pd.concat([\
                categorized_data,
                pd.DataFrame(\
                    encoded_channel,
                    columns=[f"{column}->{category}" for category in categories],
                    index=categorized_data.index)
                ],
                axis=1)

        return categorized_data


### FILE: .\src\preprocessing\feature_engines.py ###
"""Dataset file

This file allows access to the dataset as specified.

Todo:

YerevaNN/mimic3-benchmarks
"""
import numpy as np
import pandas as pd
import json
from scipy.stats import skew
from numpy import random
from multiprocess import Manager
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from utils import dict_subset
from utils.IO import *
from pathlib import Path
from datasets.trackers import PreprocessingTracker
from . import AbstractProcessor


class MIMICFeatureEngine(AbstractProcessor):
    """_summary_
    """

    def __init__(self,
                 config_dict: Path,
                 task: str,
                 reader: ProcessedSetReader = None,
                 storage_path: Path = None,
                 tracker: PreprocessingTracker = None,
                 verbose=False) -> None:
        """_summary_

        Args:
            config_dict (Path): _description_
            task (str): _description_
            storage_path (Path, optional): _description_. Defaults to None.
            source_path (Path, optional): _description_. Defaults to None.
            tracking (bool, optional): _description_. Defaults to True.
            save_as_sample (bool, optional): _description_. Defaults to True.
        """
        self._storage_path = storage_path
        self._writer = (DataSetWriter(storage_path) if storage_path is not None else None)
        self._reader = reader
        if tracker is not None:
            self._tracker = tracker
        else:
            self._tracker = (PreprocessingTracker(Path(storage_path, "progress"))
                             if storage_path is not None else None)
        self._task = task
        self._save_as_samples = (True if task in ["IHM", "PHENO"]\
                                 else False)
        self._subsample_switch = {
            "first_percentage":
                lambda start_t, end_t, percentage: (start_t, start_t +
                                                    (end_t - start_t) * percentage / 100.0),
            "last_percentage":
                lambda start_t, end_t, percentage: (end_t -
                                                    (end_t - start_t) * percentage / 100.0, end_t)
        }

        self._lock = Manager().Lock()
        self._verbose = verbose

        with open(config_dict) as file:
            config_dict = json.load(file)
            self._sampler_combinations = config_dict["sampler_combinations"]
            self._impute_config = config_dict["channels"]
            self._channel_names = config_dict["channel_names"]

    @property
    def subjects(self) -> list:
        """_summary_

        Returns:
            list: _description_
        """
        return self._reader.subject_ids

    def transform_subject(self, subject_id: int):
        X_processed, y_processed = self._reader.read_sample(subject_id,
                                                            read_ids=True,
                                                            data_type=pd.DataFrame).values()
        X = {subject_id: X_processed}
        y = {subject_id: y_processed}
        if X is None or y is None:
            return None, None

        X_engineered, y_engineered, _ = self.transform(X, y)
        if X_engineered is None or y_engineered is None:
            return None, None
        if self._tracker is None:
            return X_engineered, y_engineered

        with self._lock:
            tracking_info = self._tracker.subjects[subject_id]
        return (X_engineered, y_engineered), tracking_info

    def transform(self, X_dict: dict, y_dict: dict):
        """_summary_

        Args:
            X_dict (dict): _description_
            y_dict (dict): _description_

        Returns:
            _type_: _description_
        """
        n_subjects = 0
        n_stays = 0
        n_samples = 0

        if self._verbose:
            info_io(f"Engineering processed data:\n"
                    f"Engineered subjects: {0}\n"
                    f"Engineered stays: {0}\n"
                    f"Engineered samples: {0}")

        self._samples_processed = 0

        self._X_processed = dict()
        self._y_processed = dict()
        self._t_processed = dict()

        for subject_id in X_dict.keys():
            X_subject = X_dict[subject_id]
            y_subject = y_dict[subject_id]
            self._X_processed[subject_id] = dict()
            self._y_processed[subject_id] = dict()
            self._t_processed[subject_id] = dict()
            tracking_info = dict()

            for stay_id in X_subject:
                X_df = X_subject[stay_id]
                y_df = y_subject[stay_id]

                X_ss, ys, ts = self._engineer_stay(X_df, y_df)
                X_ss, ys, ts = self._convert_feature_dtype(X_ss, ys, ts)
                self._X_processed[subject_id][stay_id] = X_ss
                self._y_processed[subject_id][stay_id] = np.atleast_2d(ys)
                self._t_processed[subject_id][stay_id] = ts
                tracking_info[stay_id] = len(ys)
                n_samples += len(ys)
                n_stays += 1

                if self._verbose:
                    info_io(
                        f"Engineering processed data:\n"
                        f"Engineered subjects: {n_subjects}\n"
                        f"Engineered stays: {n_stays}\n"
                        f"Engineered samples: {n_samples}",
                        flush_block=True)

            n_subjects += 1
            if self._tracker is not None:
                with self._lock:
                    self._tracker.subjects.update({subject_id: tracking_info})

        if self._verbose:
            info_io(
                f"Engineering processed data:\n"
                f"Engineered subjects: {n_subjects}\n"
                f"Engineered stays: {n_stays}\n"
                f"Engineered samples: {n_samples}",
                flush_block=True)

        return self._X_processed, self._y_processed, self._t_processed

    def _engineer_stay(self, X_df, y_df):
        X_df = self._make_categorical_data(X_df)
        Xs, ys, ts = self._read_timeseries_windows(X_df, y_df)

        (Xs, ys, ts) = self._shuffle([Xs, ys, ts])

        X_ss = list()
        ys = list(ys)
        ts = list(ts)

        for df in Xs:
            subsamples = [[
                self._channel_subsampler(df[column], *combination)
                for combination in self._sampler_combinations
            ]
                          for column in self._channel_names]
            # Iterating by channel name from config allows normalization
            # and ensures comparability to ground truth data from original dir

            engineered_features = [
                self._make_engineered_features(channel.values)
                for subsample in subsamples
                for channel in subsample
            ]

            X_ss.append(np.concatenate(engineered_features))

            self._samples_processed += 1

        return X_ss, ys, ts

    def _convert_feature_dtype(self, X, y, t):
        """_summary_

        Args:
            X (_type_): _description_
            y (_type_): _description_
            t (_type_): _description_
        """
        return X, y, t

    def save_data(self, subject_ids: list = None) -> None:
        """_summary_

        Args:
            storage_path (Path, optional): _description_. Defaults to None.
            task (str, optional): _description_. Defaults to None.
        """
        if subject_ids is None:
            name_data_pairs = {
                "X": self._X_processed,
                "y": self._y_processed,
                "t": self._t_processed
            }
        else:
            name_data_pairs = {
                "X": dict_subset(self._X_processed, subject_ids),
                "y": dict_subset(self._y_processed, subject_ids),
                "t": dict_subset(self._t_processed, subject_ids)
            }
        with self._lock:
            self._writer.write_bysubject(name_data_pairs, file_type="hdf5")

        def create_df(data, file_name) -> pd.DataFrame:
            if file_name == "X":
                dfs = pd.DataFrame([([subject_id, stay_id] +
                                     np.squeeze(frame).tolist()) if len(np.squeeze(frame)) > 1 else
                                    ([subject_id, stay_id, float(frame)])
                                    for subject_id, subject_stays in data.items()
                                    for stay_id, frame in subject_stays.items()])

            elif file_name == "y":
                dfs = pd.DataFrame([([subject_id, stay_id] +
                                     frame.tolist()) if isinstance(frame.tolist(), list) else
                                    ([subject_id, stay_id, float(frame)])
                                    for subject_id, subject_stays in data.items()
                                    for stay_id, frame in subject_stays.items()])
            dfs = dfs.rename(columns={0: "subject_id", 1: "stay_id"})
            if not len(dfs):
                return
            return dfs

        def append_data(X: dict, y: dict):

            def append(dfs: pd.DataFrame, file_name: str):
                file = Path(self._storage_path, f"{file_name}.csv")
                if file.is_file():
                    dfs.to_csv(file, mode='a', header=False, index=False)
                else:
                    dfs.to_csv(file, index=False)

            X_df = create_df(X, "X")
            y_df = create_df(y, "y")
            if y_df is None or not len(y_df) or not len(X_df):
                return
            append(X_df, "X")
            append(y_df, "y")

        if self._save_as_samples:
            with self._lock:
                append_data(self._X_processed, self._y_processed)

        return

    def _shuffle(self, data) -> None:
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        assert len(data) >= 2

        data = list(zip(*data))
        random.shuffle(data)
        data = list(zip(*data))

        return data

    def _make_categorical_data(self, X):
        """_summary_

        Args:
            X (_type_): _description_

        Returns:
            _type_: _description_
        """
        replace_dict = {'nan': np.nan}

        for channel in self._impute_config.keys():
            if 'values' in self._impute_config[channel].keys():
                replace_dict.update(self._impute_config[channel]['values'])

        with pd.option_context('future.no_silent_downcasting', True):
            X = X.replace(replace_dict).astype(float)
        return X

    def _make_engineered_features(self, data):
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        functions = [min, max, np.mean, np.std, skew, len]
        import warnings
        warnings.filterwarnings("error")

        if len(data) == 0:
            engineered_data = np.full((len(functions,)), np.nan)
        else:
            # !TODO DEBUGGING
            engineered_data = [
                fn(data) if fn is not skew or
                (len(data) > 1 and not all(i == data[0]
                                           for i in data) or fn is len) else
                0  #TODO! This will fail and be NaN in Windows
                for fn in functions
            ]
            engineered_data = np.array(engineered_data, dtype=np.float32)

        return engineered_data

    def _channel_subsampler(self, Sr: pd.Series, sampler_function, percentage):
        """_summary_

        Args:
            Sr (pd.Series): _description_
            sampler_function (_type_): _description_
            percentage (_type_): _description_

        Returns:
            _type_: _description_
        """
        Sr = Sr.dropna()

        if len(Sr) == 0:
            return pd.DataFrame()

        start_t = Sr.index[0]
        end_t = Sr.index[-1]

        sampled_start_t, sampled_end_t = self._subsample_switch[sampler_function](start_t, end_t,
                                                                                  percentage)

        return Sr[(Sr.index < sampled_end_t + 1e-6) & (Sr.index > sampled_start_t - 1e-6)]

    def _timeseries_subsampler(self, X: pd.DataFrame, sampler_function, percentage):
        """_summary_

        Args:
            X (pd.DataFrame): _description_
            sampler_function (_type_): _description_
            percentage (_type_): _description_

        Returns:
            _type_: _description_
        """
        if len(X) == 0:
            data = np.full((6), np.nan)
        else:
            start_t = X.index[0]
            end_t = X.index[-1]

            sampled_start_t, sampled_end_t = self._subsample_switch[sampler_function](start_t,
                                                                                      end_t,
                                                                                      percentage)

            data = X[(X.index < sampled_end_t + 1e-6) & (X.index > sampled_start_t - 1e-6)]

            if len(data) == 0:
                data = pd.DataFrame(np.full((len(X,)), np.nan))

        return [data[channel] for channel in data]

    def _read_timeseries_windows(self, X_df: pd.DataFrame, y_df: pd.DataFrame) -> 'tuple[list]':
        """_summary_

        Args:
            X_df (pd.DataFrame): _description_
            y_df (pd.DataFrame): _description_

        Raises:
            ValueError: _description_

        Returns:
            tuple: _description_
        """
        Xs = list()
        ys = list()
        ts = list()

        for i in range(len(y_df)):
            index = i

            if index < 0 or index >= len(y_df):
                raise ValueError(
                    "Index must be from 0 (inclusive) to number of examples (exclusive).")

            t = y_df.reset_index().iloc[index, 0]
            y = y_df.reset_index().iloc[index, 1:]
            X = X_df[X_df.index < t + 1e-6]

            Xs.append(X)
            ys.append(y)
            ts.append(t)

        return Xs, ys, ts

    def _convert_feature_dtype(self, X, y, t):
        """_summary_

        Args:
            X (_type_): _description_
            y (_type_): _description_
            t (_type_): _description_
        """
        X = np.stack(X)
        return np.array(X), np.array(y), np.array(t)


### FILE: .\src\preprocessing\imputers.py ###
import time
import pickle
import os
import numpy as np
import pandas as pd
import warnings
from pathlib import Path
from sklearn.impute import SimpleImputer
from utils.IO import *
from utils import is_allnan
from preprocessing import AbstractScikitProcessor as AbstractImputer


class PartialImputer(SimpleImputer, AbstractImputer):

    def __init__(self,
                 missing_values=np.nan,
                 strategy='mean',
                 verbose=1,
                 copy=True,
                 fill_value=0,
                 add_indicator=False,
                 keep_empty_features=True,
                 storage_path=None):
        """_summary_

        Args:
            storage_path (_type_, optional): _description_. Defaults to None.
            verbose (int, optional): _description_. Defaults to 1.
        """
        self._verbose = verbose
        self.statistics_ = None
        self.n_features_in_ = 0
        self.n_samples_in_ = 0
        self._storage_name = "partial_imputer.pkl"
        if storage_path is not None:
            self._storage_path = Path(storage_path, self._storage_name)
            self._storage_path.parent.mkdir(parents=True, exist_ok=True)
        else:
            self._storage_path = None
        super().__init__(missing_values=missing_values,
                         strategy=strategy,
                         copy=copy,
                         fill_value=fill_value,
                         add_indicator=add_indicator,
                         keep_empty_features=keep_empty_features)

    def partial_fit(self, X):
        """_summary_

        Args:
            X (_type_): _description_
        """
        warnings.filterwarnings("ignore")
        n = len(X)
        if is_allnan(X):
            return self
        avg = np.nanmean(X, axis=0)

        if self.statistics_ is None:
            self.fit(X)
            self.n_samples_in_ = n
        else:
            self.statistics_ = np.nanmean([self.n_samples_in_ * self.statistics_, n * avg],
                                          axis=0) / (self.n_samples_in_ + n)
        self.n_samples_in_ += n
        warnings.filterwarnings("default")
        return self

    @classmethod
    def _get_param_names(cls):
        return []


### FILE: .\src\preprocessing\preprocessors.py ###
"""Dataset file

This file allows access to the dataset as specified.

Todo:


YerevaNN/mimic3-benchmarks
"""
import numpy as np
import dateutil
import pandas as pd
from pathlib import Path
from multiprocess import Manager
from datasets.writers import DataSetWriter
from datasets.readers import ExtractedSetReader, ProcessedSetReader
from datasets.trackers import PreprocessingTracker
from utils import dict_subset
from utils.IO import *
from settings import *
from preprocessing import AbstractProcessor


class MIMICPreprocessor(AbstractProcessor):
    """_summary_
    """

    def __init__(self,
                 phenotypes_yaml: dict,
                 task: str,
                 label_type: str = "sparse",
                 reader: ExtractedSetReader = None,
                 tracker: PreprocessingTracker = None,
                 storage_path: Path = None,
                 verbose: bool = False):
        """_summary_ 
        TODO what is this source_path thing here

        Args:
            config_dict (_type_): _description_
            output_type (str, optional): _description_. Defaults to "sparse".
            task_path (str, optional): _description_. Defaults to "".
        """

        if label_type not in ["sparse", "one-hot"]:
            raise ValueError(f"Type must be one of {*['sparse', 'one-hot'],}")

        self._label_type = label_type
        self._storage_path = storage_path

        if task not in TASK_NAMES:
            raise ValueError(f"Task must be one of {*TASK_NAMES,}")

        self._task = task
        self._writer = (None if storage_path is None else DataSetWriter(self._storage_path))
        self._extracted_set_reader = reader  # the set we are trying to read from
        if tracker is not None:
            self._tracker = tracker
        else:
            self._tracker = (None if storage_path is None else PreprocessingTracker(
                Path(storage_path, "progress")))
        self._processed_set_reader = (None if storage_path is None else ProcessedSetReader(
            root_path=storage_path))
        self._lock = Manager().Lock()
        self._phenotypes_yaml = phenotypes_yaml
        self._verbose = verbose

    @property
    def subjects(self) -> list:
        """_summary_

        Returns:
            list: _description_
        """
        if self._extracted_set_reader is None:
            return []
        return self._extracted_set_reader.subject_ids

    def transform_subject(self, subject_id: int) -> None:
        """_summary_

        Args:
            subject_id (_type_): _description_

        Returns:
            _type_: _description_
        """
        subject_data = self._extracted_set_reader.read_subject(subject_id, read_ids=True)
        if not subject_data:
            return None, None
        del subject_data["subject_events"]
        X, y = self.transform({subject_id: subject_data})
        if not X or not y:
            return None, None
        if self._tracker is None:
            return X, y
        with self._lock:
            tracking_info = self._tracker.subjects[subject_id]
        return (X, y), tracking_info

    def transform(self, dataset: dict):
        """_summary_

        Args:
            timeseries (_type_): _description_
            episodic_data (_type_): _description_
            diagnoses (_type_): _description_
            icuhistory (_type_): _description_
            task (_type_): _description_
            task_path (_type_, optional): _description_. Defaults to None.

        Returns:
            _type_: _description_
        """
        self._X = dict()
        self._y = dict()

        # Tracking variables
        n_subjects = 0
        n_stays = 0
        n_skip = 0
        n_samples = 0

        start_verbose = True

        if self._task in ["LOS"] and self._label_type == "one-hot":
            info_io(f"Only sparse output_type is available for task {self._task}!"
                    f" Argument {self._label_type} disregarded")
            self._label_type = "sparse"

        for subject, subject_data in dataset.items():
            skip_subject = False

            subject_timeseries: pd.DataFrame = subject_data['timeseries']
            diagnoses_df: pd.DataFrame = subject_data['subject_diagnoses']
            icuhistory_df: pd.DataFrame = subject_data['subject_icu_history']
            episodic_data_df: pd.DataFrame = subject_data['episodic_data']

            self._X[subject] = dict()
            self._y[subject] = dict()

            tracking_info = dict()

            if (self._tracker is not None) and \
               (subject in self._tracker.subjects) and \
               (not self._X[subject]):
                # Do not reprocess already existing directories
                self._X[subject], self._y[subject] = self._processed_set_reader.read_sample(
                    str(subject), read_ids=True).values()
                skip_subject = True
                continue
            elif start_verbose:
                if self._verbose:
                    info_io(f"Processing timeseries data:\n"
                            f"Processed subjects: {0}\n"
                            f"Processed stays: {0}\n"
                            f"Processed samples: {0}\n"
                            f"Skipped subjects: {0}")
                    start_verbose = False

            for icustay in subject_timeseries:
                stay_timeseries_df = subject_timeseries[icustay]
                cur_episodic_data_df = episodic_data_df.loc[icustay]
                cur_icuhistory_sr = icuhistory_df.reset_index().set_index("ICUSTAY_ID").loc[icustay]

                if not pd.isna(cur_episodic_data_df["MORTALITY"]):
                    mortality = int(cur_episodic_data_df["MORTALITY"])
                else:
                    continue

                if self._task == "IHM":
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_inhospital_mortality_data(
                            stay_timeseries_df, cur_episodic_data_df, mortality)

                elif self._task == "DECOMP":
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_decompensation_data(stay_timeseries_df,
                                                                 cur_episodic_data_df,
                                                                 cur_icuhistory_sr)
                elif self._task == "LOS":
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_length_of_stay_data(stay_timeseries_df,
                                                                 cur_episodic_data_df)
                elif self._task == "PHENO":
                    stay_diagnoses_df = diagnoses_df[diagnoses_df['ICUSTAY_ID'] == icustay]
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_pheontyping_data(stay_timeseries_df,
                                                              cur_episodic_data_df,
                                                              stay_diagnoses_df,
                                                              self._phenotypes_yaml)
                else:
                    raise ValueError(
                        "Task must be one of: in_hospital_mortality, decompensation, length_of_stay, phenotyping"
                    )
                if self._y[subject][icustay].empty:
                    del self._y[subject][icustay]
                    del self._X[subject][icustay]
                    continue
                else:
                    tracking_info[icustay] = len(self._y[subject][icustay])
                    n_stays += 1
                    n_samples += len(self._y[subject][icustay])
                    if self._verbose:
                        info_io(
                            f"Processing timeseries data:\n"
                            f"Processed subjects: {n_subjects}\n"
                            f"Processed stays: {n_stays}\n"
                            f"Processed samples: {n_samples}\n"
                            f"Skipped subjects: {n_skip}",
                            flush_block=True)

            if skip_subject:
                continue

            if self._tracker is not None and tracking_info:
                with self._lock:
                    self._tracker.subjects.update({subject: tracking_info})

            if not len(self._y[subject]) or not len(self._X[subject]):
                del self._y[subject]
                del self._X[subject]
                n_skip += 1
            else:
                n_subjects += 1
        if self._verbose:
            info_io(
                f"Processing timeseries data:\n"
                f"Processed subjects: {n_subjects}\n"
                f"Processed stays: {n_stays}\n"
                f"Processed samples: {n_samples}\n"
                f"Skipped subjects: {n_skip}",
                flush_block=True)

        return self._X, self._y

    def save_data(self, subjects: list = None) -> None:
        """_summary_

        Args:
            task_path (_type_, optional): _description_. Defaults to None.
        """
        if self._writer is None:
            info_io("No storage path provided. Data will not be saved.")
            return
        with self._lock:
            if subjects is None:
                self._writer.write_bysubject({"X": self._X})
                self._writer.write_bysubject({"y": self._y})
            else:
                self._writer.write_bysubject({"X": dict_subset(self._X, subjects)})
                self._writer.write_bysubject({"y": dict_subset(self._y, subjects)})

        return

    def make_inhospital_mortality_data(self, timeseries_df: pd.DataFrame,
                                       episodic_data_df: pd.DataFrame, mortality: int):
        """_summary_

        Args:
            timeseries_df (_type_): _description_
            mortality (_type_): _description_

        Returns:
            _type_: _description_
        """
        precision = IHM_SETTINGS['sample_precision']
        label_start_time = IHM_SETTINGS['label_start_time']
        los = 24.0 * episodic_data_df.loc['LOS']  # in hours
        if los < label_start_time:
            return pd.DataFrame(columns=timeseries_df.columns), \
                   pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')
        X: pd.DataFrame = timeseries_df[(timeseries_df.index < label_start_time + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y'])

        y = np.array([(X.index[-1], mortality)])
        y = pd.DataFrame(y, columns=['Timestamp', 'y']).set_index('Timestamp')

        return X, y

    def make_decompensation_data(self, timeseries_df: pd.DataFrame, episodic_data_df: pd.DataFrame,
                                 icu_stay):
        """_summary_

        Args:
            timeseries_df (_type_): _description_
            episodic_data_df (_type_): _description_
            icuhistory_df (_type_): _description_

        Returns:
            _type_: _description_
        """
        precision = DECOMP_SETTINGS['sample_precision']
        sample_rate = DECOMP_SETTINGS['sample_rate']
        label_start_time = DECOMP_SETTINGS['label_start_time']
        future_time_interval = DECOMP_SETTINGS['future_time_interval']
        # future_time_interval is Hours in which the preson will have to be dead for label to be 1

        mortality = int(episodic_data_df.loc["MORTALITY"])

        los = 24.0 * episodic_data_df.loc['LOS']  # in hours

        deathtime = icu_stay['DEATHTIME']
        intime = icu_stay['INTIME']

        if pd.isnull(deathtime):
            lived_time = 1e18
        else:
            if isinstance(icu_stay['DEATHTIME'], str):
                deathtime = dateutil.parser.parse(deathtime)
            if isinstance(icu_stay['INTIME'], str):
                intime = dateutil.parser.parse(intime)
            lived_time = (deathtime - intime).total_seconds() / 3600.0

        X: pd.DataFrame = timeseries_df[(timeseries_df.index < los + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')

        event_times = timeseries_df.index[(timeseries_df.index < los + precision)
                                          & (timeseries_df.index > -precision)]

        sample_times = np.arange(0.0, min(los, lived_time) + precision, sample_rate)
        sample_times = list(filter(lambda x: x > label_start_time, sample_times))

        # At least one measurement
        sample_times = list(filter(lambda x: x > event_times[0], sample_times))

        y = list()

        for hour in sample_times:
            if mortality == 0:
                cur_mortality = 0
            else:
                cur_mortality = int(lived_time - hour < future_time_interval)
            y.append((hour, cur_mortality))

        if not y:
            return pd.DataFrame(columns=X.columns), pd.DataFrame(columns=['Timestamp', 'y'])

        y = pd.DataFrame(y, columns=['Timestamp', 'y']).set_index('Timestamp')

        return X, y

    def make_length_of_stay_data(self, timeseries_df: pd.DataFrame, episodic_data_df: pd.DataFrame):
        """_summary_

        Args:
            timeseries_df (_type_): _description_
            episodic_data_df (_type_): _description_

        Returns:
            _type_: _description_
        """
        precision = LOS_SETTINGS['sample_precision']
        sample_rate = LOS_SETTINGS['sample_rate']
        label_start_time = LOS_SETTINGS['label_start_time']
        bins = LOS_SETTINGS['bins']

        los = 24.0 * episodic_data_df.loc['LOS']  # in hours

        X: pd.DataFrame = timeseries_df[(timeseries_df.index < los + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')

        event_times = timeseries_df.index[(timeseries_df.index < los + precision)
                                          & (timeseries_df.index > -precision)]

        sample_times = np.arange(0.0, los + precision, sample_rate)
        sample_times = list(filter(lambda x: x > label_start_time, sample_times))
        sample_times = list(filter(lambda x: x > event_times[0], sample_times))

        y = list()

        for hour in sample_times:
            if self._label_type == "one-hot":
                category_index = np.digitize(los - hour, bins) - 1
                remaining_los = [0] * 10
                remaining_los[category_index] = 1
                y.append((hour, remaining_los))

            elif self._label_type == "sparse":
                y.append((hour, los - hour))

        y = pd.DataFrame(y, columns=['Timestamp', 'y']).set_index('Timestamp')
        if y.empty:
            # Will prevent the sample from being used
            return pd.DataFrame(columns=X.columns), y

        return X, y

    def make_pheontyping_data(self, timeseries_df: pd.DataFrame, episodic_data_df: pd.DataFrame,
                              diagnoses_df: pd.DataFrame, phenotypes_yaml: pd.DataFrame):
        """_summary_

        Args:
            timeseries_df (_type_): _description_
            episodic_data_df (_type_): _description_
            diagnoses_df (_type_): _description_
            phenotypes_yaml (_type_): _description_

        Returns:
            _type_: _description_
        """
        precision = PHENOT_SETTINGS['sample_precision']
        valid_ids = PHENOT_SETTINGS['valid_ids']

        los = 24.0 * episodic_data_df.loc['LOS']  # in hours
        X: pd.DataFrame = timeseries_df[(timeseries_df.index < los + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')

        # Dictionary mapping codes to groups
        code_to_group = {}
        for group in phenotypes_yaml:
            codes = phenotypes_yaml[group]['codes']
            for code in codes:
                if code not in code_to_group:
                    code_to_group[code] = group
                else:
                    assert code_to_group[code] == group

        # index is ID of phenotype in the yaml
        id_to_group = sorted(phenotypes_yaml.keys())
        group_to_id = dict((x, i) for (i, x) in enumerate(id_to_group))

        # Index is pheontype designation and item is id
        cur_labels = [0] * len(id_to_group)

        for index, row in diagnoses_df.iterrows():
            if row['USE_IN_BENCHMARK']:
                code = row['ICD9_CODE']
                group = code_to_group[code]
                group_id = group_to_id[group]
                cur_labels[group_id] = 1

        # Only use in benchmark = True labels
        if not valid_ids:
            valid_ids = [
                i for (i, x) in enumerate(cur_labels)
                if phenotypes_yaml[id_to_group[i]]['use_in_benchmark']
            ]
            PHENOT_SETTINGS['valid_ids'] = valid_ids

        cur_labels = [cur_labels[index] for index in valid_ids]

        y = [los] + cur_labels
        y = pd.DataFrame((y,), columns=["Timestamp"] + [f"y{i}" for i in range(len(y) - 1)])
        y = y.set_index('Timestamp')

        return X, y


### FILE: .\src\preprocessing\scalers.py ###
import numpy as np
import pandas as pd
from typing import Union
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler
from utils.IO import *
from pathlib import Path
from preprocessing import AbstractScikitProcessor

__all__ = [
    "AbstractScaler", "MIMICStandardScaler", "MIMICMinMaxScaler", "MIMICMaxAbsScaler",
    "MIMICRobustScaler"
]


class AbstractScaler(AbstractScikitProcessor):

    def __init__(self, storage_path=None, imputer=None, verbose=True):
        if storage_path is not None:
            self._storage_path = Path(storage_path, self._storage_name)
            self._storage_path.parent.mkdir(parents=True, exist_ok=True)
        else:
            self._storage_path = None
        self._verbose = verbose
        self._imputer = imputer


class MIMICStandardScaler(StandardScaler, AbstractScaler):
    """
    """

    def __init__(self,
                 storage_path=None,
                 imputer=None,
                 copy=True,
                 with_mean=True,
                 with_std=True,
                 verbose=True):
        """_summary_

        Args:
            storage_path (_type_): _description_
            copy (bool, optional): _description_. Defaults to True.
            with_mean (bool, optional): _description_. Defaults to True.
            with_std (bool, optional): _description_. Defaults to True.
        """
        self._storage_name = "standard_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        StandardScaler.__init__(self, copy=copy, with_mean=with_mean, with_std=with_std)

    @classmethod
    def _get_param_names(cls):
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


class MIMICMinMaxScaler(MinMaxScaler, AbstractScaler):
    """
    """

    def __init__(self,
                 storage_path=None,
                 imputer=None,
                 verbose=True,
                 feature_range=(0, 1),
                 copy=True,
                 clip=False):
        """_summary_

        Args:
            storage_path (_type_, optional): _description_. Defaults to None.
            verbose (int, optional): _description_. Defaults to 1.
        """
        self._storage_name = "minmax_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        MinMaxScaler.__init__(self, feature_range=feature_range, copy=copy, clip=clip)

    @classmethod
    def _get_param_names(cls):
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:

        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


class MIMICMaxAbsScaler(MaxAbsScaler, AbstractScaler):

    def __init__(self, storage_path=None, imputer=None, verbose=True, copy=True):
        self._storage_name = "maxabs_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        super().__init__(copy=copy)

    @classmethod
    def _get_param_names(cls):
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:

        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


class MIMICRobustScaler(RobustScaler, AbstractScaler):

    def __init__(self,
                 storage_path=None,
                 imputer=None,
                 with_centering=True,
                 with_scaling=True,
                 quantile_range=(25.0, 75.0),
                 copy=True,
                 verbose=True):
        self._storage_name = "robust_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        super().__init__(with_centering=with_centering,
                         with_scaling=with_scaling,
                         quantile_range=quantile_range,
                         copy=copy)

    @classmethod
    def _get_param_names(cls):
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:

        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


### FILE: .\src\preprocessing\__init__.py ###
import os
import numpy as np
import pickle
from utils.IO import *
from tensorflow.keras.utils import Progbar
from pathlib import Path
from typing import List, Tuple
from abc import ABC, abstractmethod


class AbstractProcessor(ABC):
    """_summary_
    """

    @abstractmethod
    def __init__(self) -> None:
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        ...

    @property
    @abstractmethod
    def subjects(self) -> List[int]:
        ...

    @abstractmethod
    def transform(self, *args, **kwargs):
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        ...

    @abstractmethod
    def transform_subject(self, subject_id: int) -> Tuple[dict, dict, dict]:
        ...

    @abstractmethod
    def save_data(self, subject_ids: list = None) -> None:
        ...


class AbstractScikitProcessor(ABC):

    @abstractmethod
    def __init__(self, storage_path: Path):
        """_summary_

        Args:
            storage_path (_type_): _description_
        """
        ...

    @abstractmethod
    def transform(self, X: np.ndarray):
        ...

    @abstractmethod
    def fit(self, X: np.ndarray):
        ...

    @abstractmethod
    def partial_fit(self, X: np.ndarray):
        ...

    def save(self, storage_path=None):
        """_summary_
        """
        if storage_path is not None:
            self._storage_path = Path(storage_path, self._storage_name)
        if self._storage_path is None:
            raise ValueError("No storage path provided!")
        with open(self._storage_path, "wb") as save_file:
            pickle.dump(obj=self.__dict__, file=save_file, protocol=2)

    def load(self):
        """_summary_

        Returns:
            _type_: _description_
        """
        if self._storage_path is not None:
            if self._storage_path.is_file():
                if os.path.getsize(self._storage_path) > 0:
                    info_io(f"Loading {Path(self._storage_path).stem} from:\n{self._storage_path}")
                    with open(self._storage_path, "rb") as load_file:
                        load_params = pickle.load(load_file)
                    for key, value in load_params.items():
                        setattr(self, key, value)

                    return 1
        return 0

    def fit_dataset(self, X):
        """_summary_

        Args:
            discretizer (_type_): _description_
            X (_type_): _description_
        """
        if self._verbose:
            info_io(f"Fitting scaler to dataset of size {len(X)}")
            progbar = Progbar(len(X), unit_name='step')
        n_fitted = 0

        for frame in X:
            if hasattr(self, "_imputer") and self._imputer is not None:
                frame = self._imputer.transform(frame)
            self.partial_fit(frame)
            n_fitted += 1
            if self._verbose:
                progbar.update(n_fitted)

        self.save()

        if self._verbose:
            info_io(f"Done computing new {Path(self._storage_path).stem}.")
        return self

    def fit_reader(self, reader, save=False):
        """_summary_

        Args:
            discretizer (_type_): _description_
            reader (_type_): _description_
        """
        if self._storage_path is None:
            self._storage_path = Path(reader.root_path, self._storage_name)
            self._storage_path.parent.mkdir(parents=True, exist_ok=True)
        if self.load():
            return self
        if self._verbose:
            info_io(
                f"Fitting {Path(self._storage_path).stem} to reader of size {len(reader.subject_ids)}"
            )
            progbar = Progbar(len(reader.subject_ids), unit_name='step')

        n_fitted = 0

        for subject_id in reader.subject_ids:
            X_subjects, _ = reader.read_sample(subject_id).values()
            for frame in X_subjects:
                if hasattr(self, "_imputer") and self._imputer is not None:
                    frame = self._imputer.transform(frame)
                self.partial_fit(frame)
            n_fitted += 1
            if self._verbose:
                progbar.update(n_fitted)

        self.save()

        if self._verbose:
            info_io(
                f"Done computing new {Path(self._storage_path).stem}.\nSaved in location {self._storage_path}!"
            )

        return self


### FILE: .\src\utils\IO.py ###
"""Utility file

This file implements functionalities used by other modules and accessible to 
the user

Todo:
    - Use a settings.json
    - fix the progressbar bullshit

"""

# Import necessary packages
import datetime
import os
import datetime
import inspect
import pdb
import sys
from pathlib import Path
from colorama import init, Fore, Style

__all__ = ["info_io", "info_io", "tests_io", "error_io", "debug_io", "warn_io", "suppress_io"]

WORKINGDIR = os.getenv("WORKINGDIR")

# Initialize Colorama
init(autoreset=True)


def _clean_path(path):
    path = Path(path).relative_to(WORKINGDIR)
    if str(path).startswith("src"):
        return Path(path).relative_to("src")
    elif str(path).startswith("tests"):
        return Path(path).relative_to("tests")
    elif str(path).startswith("script"):
        return Path(path).relative_to("script")
    elif str(path).startswith("examples"):
        return Path(path).relative_to("examples")


DEBUG_OPTION = None
main_name = Path(sys.argv[0])
if "pytest" in str(main_name):
    PATH_PADDING = max([
        len(str(_clean_path(path)))
        for path in Path(WORKINGDIR).glob("**/*")
        if path.suffix == ".py"
    ])
else:
    # Doxygen may break on this
    try:
        PATH_PADDING = max([
            len(str(_clean_path(path)))
            for path in Path(WORKINGDIR).glob("**/*")
            if path.suffix == ".py" and not path.name.startswith("tests")
        ] + [len(str(main_name.relative_to(WORKINGDIR)))])
    except ValueError:
        PATH_PADDING = 0
# {path: len(str(path)) for path in Path().glob("**/*") if path.suffix == ".py"}
SRC_DIR = Path(WORKINGDIR, "src")
SCRIPT_DIR = Path(WORKINGDIR, "scripts")
TEST_DIR = Path(os.getenv("TESTS"))
EXAMPLE_DIR = Path(os.getenv("EXAMPLES"))
# Might need adjustment
HEADER_LENGTH = 5 + 19 + 3 + PATH_PADDING + 3 + 2 + 4
TAG_PADDING = 8


def _get_relative_path(file_path: Path):
    file_path = str(file_path)
    if "src" in file_path:
        relativa_path = SRC_DIR
        return Path(file_path).relative_to(relativa_path)
    elif "script" in file_path:
        relativa_path = SCRIPT_DIR
        return Path(file_path).relative_to(relativa_path)
    elif "tests" in file_path:
        relativa_path = TEST_DIR
        return Path(file_path).relative_to(relativa_path)
    elif "examples" in file_path:
        relativa_path = EXAMPLE_DIR
        return Path(file_path).relative_to(relativa_path)
    return file_path


def _get_line(caller):
    path = str(_get_relative_path(caller.filename)) + " "
    return f"{Fore.LIGHTWHITE_EX}{str(datetime.datetime.now().strftime('%m-%d %H:%M:%S'))[:19]}{Style.RESET_ALL} : {path:-<{PATH_PADDING}} : {Fore.LIGHTCYAN_EX}L {caller.lineno:<4}{Style.RESET_ALL}"


def _print_iostring(string: str, line_header: str, flush_block: bool, collor: str):
    if flush_block:
        lines = string.split('\n')
        num_lines = len(lines)
        sys.stdout.write(f"\x1b[{num_lines}A")
        sys.stdout.write('\x1b[2K')
        print(lines[0])
        for line in lines[1:]:
            sys.stdout.write('\x1b[2K')
            print(" " * HEADER_LENGTH + f"{collor}-{Style.RESET_ALL} {line}")

        sys.stdout.flush()
        return True
    elif "\n" in string:
        lines = string.split('\n')
        print(lines[0])
        for line in lines[1:]:
            print(" " * HEADER_LENGTH + f"{collor}-{Style.RESET_ALL} {line}")
        return True
    return False


def info_io(message: str,
            level: int = None,
            end: str = None,
            flush: bool = None,
            unflush: bool = None,
            flush_block: bool = False):
    """
    Prints a message with an information header, which can be formatted at different levels, or without any header formatting.
    Level 0: Wide header
    Level 1: Medium header
    Level 2: Narrow header
    Level None: No header formatting, behaves like the original info_io.
    
    Args:
    message (str): The message to log.
    level (int, optional): The level of the header, or None for no header formatting. Defaults to None.
    end (str, optional): How to end the print, e.g., "\n" (newline). Defaults to None.
    flush (bool, optional): Whether to forcibly flush the stream. Defaults to None.
    unflush (bool, optional): If True, adds an additional newline before the message. Defaults to None.
    flush_block (bool, optional): If True, prints the whole block at once to avoid disruptions. Defaults to False.

    """
    base_io("INFO", Fore.BLUE, message, level, end, flush, unflush, flush_block)


def tests_io(message: str,
             level: int = None,
             end: str = None,
             flush: bool = None,
             unflush: bool = None,
             flush_block: bool = False):
    """
    Prints a message with an information header, which can be formatted at different levels, or without any header formatting.
    Level 0: Wide header
    Level 1: Medium header
    Level 2: Narrow header
    Level None: No header formatting, behaves like the original info_io.
    
    Args:
    message (str): The message to log.
    level (int, optional): The level of the header, or None for no header formatting. Defaults to None.
    end (str, optional): How to end the print, e.g., "\n" (newline). Defaults to None.
    flush (bool, optional): Whether to forcibly flush the stream. Defaults to None.
    unflush (bool, optional): If True, adds an additional newline before the message. Defaults to None.
    flush_block (bool, optional): If True, prints the whole block at once to avoid disruptions. Defaults to False.
    
    """
    base_io("TEST", Fore.GREEN, message, level, end, flush, unflush, flush_block)


def debug_io(message: str,
             end: str = None,
             flush: bool = None,
             unflush: bool = None,
             flush_block: bool = False):
    """
    """
    # TODO: Improve this solution (Adopted because file level declaration will initialize before the var can be changed by test fixtures)
    global DEBUG_OPTION
    if DEBUG_OPTION is None:
        DEBUG_OPTION = os.getenv("DEBUG", "0")
    if DEBUG_OPTION == "0":
        return
    base_io("DEBUG", Fore.YELLOW, message, None, end, flush, unflush, flush_block)


def warn_io(message: str,
            end: str = None,
            flush: bool = None,
            unflush: bool = None,
            flush_block: bool = False):
    """
    """
    base_io("WARN", "\033[38;5;208m", message, None, end, flush, unflush, flush_block)


def error_io(message: str,
             exception_type: Exception,
             end: str = None,
             flush: bool = None,
             unflush: bool = None,
             flush_block: bool = False):
    """"""
    base_io("ERROR", Fore.RED, message, None, end, flush, unflush, flush_block)
    raise exception_type


def base_io(info_tag: str,
            collor: str,
            message: str,
            level: int = None,
            end: str = None,
            flush: bool = None,
            unflush: bool = None,
            flush_block: bool = False):

    # Get caller information
    caller = inspect.getframeinfo(inspect.stack()[2][0])
    line_header = _get_line(caller)

    if level is None:
        header_message = message
    else:
        assert level in [0, 1, 2], ValueError("Header level must be one of 0, 1, 2")
        width = [60, 50, 40][level]
        header_message = '-' * 10 + ' ' + collor + f'{message}{Style.RESET_ALL} '.ljust(
            width + len({Style.RESET_ALL}), "-")
        flush_block = False  # Disable flush_block when level is specified
    collor_info_tag = f"{collor}{info_tag}{Style.RESET_ALL} "
    info_tag_padding = TAG_PADDING + len(collor) + len(Style.RESET_ALL)

    io_string = f"{collor_info_tag.ljust(info_tag_padding, '-')} {line_header} {collor}-{Style.RESET_ALL} {header_message}"

    if level in [0, 1]:
        print(" " * HEADER_LENGTH + "-")
    if unflush:
        print()
    if not _print_iostring(io_string, line_header, flush_block, collor):
        if flush:
            end = "\r"
        print(io_string, end=end, flush=flush)


#https://stackoverflow.com/questions/8391411/how-to-block-calls-to-print
class suppress_io:

    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


### FILE: .\src\utils\notebooks.py ###
import ipywidgets as widgets

# The docker environment manipulates the python path to include our source directory
# Execute this from within the docker environ to make these import work
import keras
import matplotlib.pyplot as plt
import numpy as np
from tensorflow import math

# The docker environment manipulates the python path to include our source directory
# Execute this from within the docker environ to make these import work
import visualization

label_name = {
    "hhourly": 'energy(kWh/hh)',
    "hourly": 'energy(kWh/hh)_sum',
    "hdaily": 'energy(kWh/hh)_sum',
    "qdaily": 'energy(kWh/hh)_sum',
    "daily": 'energy_sum'
}


class Metric():
    """
    """

    def __init__(self, bin_averages):
        """"""
        self.bin_averages = bin_averages

    def cat_mse(self, y_true, y_pred):
        """
        """
        y_pred = math.argmax(y_pred, axis=1)
        y_true = math.argmax(y_true, axis=1)
        y_pred = np.array([self.bin_averages[val] for val in y_pred.numpy()])
        y_true = np.array([self.bin_averages[val] for val in y_true.numpy()])

        return keras.losses.mse(y_pred, y_true)


class History():

    def __init__(self):
        self.acc = {}
        self.loss = {}
        self.mse = {}

    def update(self, value, history):
        self.acc[f"{value}_acc"] = history.history["accuracy"]
        self.acc[f"{value}_val_acc"] = history.history["val_accuracy"]

        self.loss[f"{value}_loss"] = history.history["loss"]
        self.loss[f"{value}_val_loss"] = history.history["val_loss"]

        self.mse[f"{value}_cat_mse"] = history.history["cat_mse"]
        self.mse[f"{value}_val_cat_mse"] = history.history["val_cat_mse"]


def make_hist_plot(values, history, type):
    plt.clf()
    colors = ["darkviolet", "darkgreen", "goldenrod", "darkblue"]
    plt.figure(figsize=(10, 5))

    for value, color in zip(values, colors):
        key = f"{value}_{type}"
        plt.plot(history[key], label=key, color=color)
        val_key = f"{value}_val_{type}"
        plt.plot(history[val_key], label=val_key, color=color, linestyle="--")

    plt.grid()
    plt.legend()

### FILE: .\src\utils\__init__.py ###
"""Utility file

This file implements functionalities used by other modules and accessible to 
the user

Todo:
    - Use a settings.json
    - fix the progressbar bullshit

"""

import re
import json
import numpy as np
import pandas as pd
from typing import Dict, Union
from multipledispatch import dispatch

from utils.IO import *
from pathlib import Path


@dispatch(dict)
def get_sample_size(X):
    """
    """
    n_samples = 0
    for data in X.values():
        if isinstance(data, dict):
            n_samples += get_sample_size(data)
        else:
            n_samples += len(data)

    return n_samples


@dispatch(list)
def get_sample_size(X):
    """
    """
    n_samples = 0
    for subject in X:
        if isinstance(X, dict):
            for stay in X:
                n_samples += len(stay)
        else:
            n_samples += len(subject)

    return n_samples


def is_numerical(df: pd.DataFrame) -> bool:
    # Check if the DataFrame is numerical
    # This is the worst implementation but what works works
    try:
        df.astype(float)
        return True
    except:
        pass
    if (df.dtypes == object).any() or\
       (df.dtypes == "category").any() or\
       (df.dtypes == "datetime64[ns]").any():
        return False
    return True


def to_snake_case(string):
    # Replace spaces and hyphens with underscores
    string = re.sub(r'[\s-]+', '_', string)

    # Replace camel case and Pascal case with underscores
    string = re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', string)

    # Convert the string to lowercase
    string = string.lower()

    return string


def is_colwise_numerical(df: pd.DataFrame) -> Dict[str, bool]:
    return {col: is_numerical(df[[col]]) for col in df.columns}


def is_allnan(data: Union[pd.DataFrame, pd.Series, np.ndarray]):
    if isinstance(data, (pd.DataFrame, pd.Series)):
        return data.isna().all().all()
    elif isinstance(data, np.ndarray):
        return np.isnan(data).all()
    else:
        raise TypeError("Input must be a pandas DataFrame, Series, or numpy array.")


def update_json(json_path, items: dict):
    """
    """
    if not json_path.parent.is_dir():
        json_path.parent.mkdir(parents=True, exist_ok=True)
    if not json_path.is_file():
        with open(json_path, 'w+') as file:
            json.dump({}, file)

    with open(json_path, 'r') as file:
        json_data = json.load(file)

    json_data.update(items)
    try:
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)
    except KeyboardInterrupt:
        info_io("Finishing JSON operation before interupt.")
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)

    return json_data


def load_json(json_path):
    """
    """
    if not json_path.is_file():
        return {}

    with open(json_path, 'r') as file:
        json_data = json.load(file)

    return json_data


def write_json(json_path, json_data):
    """
    """
    try:
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)
    except KeyboardInterrupt:
        info_io("Finishing JSON operation before interupt.")
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)

    return


def dict_subset(dictionary: dict, keys: list):
    """_summary_

    Args:
        dictionary (dict): _description_
        keys (list): _description_
    """
    return {k: dictionary[k] for k in keys if k in dictionary}


def is_iterable(obj):
    """_summary_

    Args:
        obj (_type_): _description_

    Returns:
        _type_: _description_
    """
    return hasattr(obj, '__iter__')


def make_prediction_vector(model, generator, batches=20, bin_averages=None):
    """_summary_
    """
    # TODO! fix bin averages instead
    Xs = list()
    ys = list()

    for _ in range(batches):
        X, y = generator.next()
        Xs.append(X)
        ys.append(y)

    y_true = np.concatenate(ys)
    y_pred = np.concatenate([model.predict(X, verbose=0) for X in Xs])

    if bin_averages:
        # TODO! shape mismatch betwenn prediction vector length and length of bin averages
        y_pred = np.array([
            bin_averages[int(label)]
            if label < len(bin_averages) else bin_averages[len(bin_averages) - 1]
            for label in np.argmax(y_pred, axis=1)
        ]).reshape(1, -1)

        if len(y_true.shape) > 1:
            y_true = np.argmax(y_true, axis=1)

        y_true = np.array([
            bin_averages[int(label)]
            if label < len(bin_averages) else bin_averages[len(bin_averages) - 1]
            for label in y_true
        ]).reshape(1, -1)

    # TODO! this should be y_pred y_true
    return y_pred, y_true


def count_csv_size(file_path: Path):

    def blocks(files, size=65536):
        while True:
            b = files.read(size)
            if not b:
                break
            yield b

    with open(file_path, "r", encoding="utf-8", errors='ignore') as f:
        file_length = sum(bl.count("\n") for bl in blocks(f))

    return file_length - 1


### FILE: .\tests\conftest.py ###
import shutil
import pytest
import os
import re
import datasets
import pandas as pd
from typing import Dict
from pathlib import Path
from tests.settings import *
from utils.IO import *
from datasets.readers import ExtractedSetReader, EventReader, ProcessedSetReader
from settings import *

collect_ignore = ['src/utils/IO.py']


def pytest_configure(config) -> None:
    os.environ["DEBUG"] = "0"

    if SEMITEMP_DIR.is_dir():
        shutil.rmtree(str(SEMITEMP_DIR))

    [
        datasets.load_data(chunksize=75835,
                           source_path=TEST_DATA_DEMO,
                           storage_path=SEMITEMP_DIR,
                           preprocess=True,
                           task=name) for name in TASK_NAMES
    ]


@pytest.fixture(scope="function", autouse=True)
def cleanup():
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))

    # Execution
    yield

    # Clean after execution
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))


@pytest.fixture(scope="session")
def extracted_reader() -> ExtractedSetReader:
    reader = datasets.load_data(chunksize=75835,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR)
    return reader


@pytest.fixture(scope="session")
def subject_ids(extracted_reader: ExtractedSetReader) -> list:
    icu_history = extracted_reader.read_csv("icu_history.csv")
    subjects = icu_history["SUBJECT_ID"].astype(int).unique().tolist()
    return subjects


@pytest.fixture(scope="session")
def preprocessed_readers() -> Dict[str, ProcessedSetReader]:
    return {
        name:
            datasets.load_data(chunksize=75835,
                               source_path=TEST_DATA_DEMO,
                               storage_path=SEMITEMP_DIR,
                               preprocess=True,
                               task=name) for name in TASK_NAMES
    }


@pytest.fixture(scope="session")
def engineered_readers() -> Dict[str, ProcessedSetReader]:
    return {
        name:
            datasets.load_data(chunksize=75835,
                               source_path=TEST_DATA_DEMO,
                               storage_path=SEMITEMP_DIR,
                               engineer=True,
                               task=name) for name in TASK_NAMES
    }


@pytest.fixture(scope="session")
def discretized_readers() -> Dict[str, ProcessedSetReader]:
    return {
        name:
            datasets.load_data(chunksize=75835,
                               source_path=TEST_DATA_DEMO,
                               storage_path=SEMITEMP_DIR,
                               discretize=True,
                               task=name) for name in TASK_NAMES
    }


@pytest.fixture(scope="session")
def discretizer_listfiles() -> None:
    list_files = dict()
    for task_name in TASK_NAMES:
        # Path to discretizer sets
        test_data_dir = Path(TEST_GT_DIR, "discretized", TASK_NAME_MAPPING[task_name])
        # Listfile with truth values
        listfile = pd.read_csv(Path(test_data_dir, "listfile.csv")).set_index("stay")
        stay_name_regex = r"(\d+)_episode(\d+)_timeseries\.csv"

        listfile = listfile.reset_index()
        listfile["subject"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(1))
        listfile["icustay"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(2))
        listfile = listfile.set_index("stay")
        list_files[task_name] = listfile
    return list_files


def pytest_unconfigure(config) -> None:
    os.environ["DEBUG"] = "0"
    if SEMITEMP_DIR.is_dir():
        shutil.rmtree(str(SEMITEMP_DIR))


### FILE: .\tests\decorators.py ###
import pytest
import functools


class repeat:

    def __init__(self, times):
        self.times = times

    def __call__(self, func):

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for _ in range(self.times):
                func(*args, **kwargs)

        return wrapper


### FILE: .\tests\settings.py ###
import json
import os
from dotenv import load_dotenv
from pathlib import Path

load_dotenv(verbose=False)

__all__ = [
    'TEST_SETTINGS', 'SEMITEMP_DIR', 'TEMP_DIR', 'DEVTEMP_DIR', 'TEST_DATA_DIR', 'TEST_DATA_DEMO',
    'TEST_GT_DIR', 'TASK_NAMES', 'FTASK_NAMES', 'TASK_NAME_MAPPING', 'DATASET_SETTINGS',
    'DECOMP_SETTINGS', 'LOS_SETTINGS', 'PHENOT_SETTINGS', 'IHM_SETTINGS'
]

TEST_SETTINGS = json.load(Path(os.getenv("TESTS"), "etc", "test.json").open())
SEMITEMP_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data", "semitemp")
TEMP_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data", "temp")
DEVTEMP_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data", "devtemp")
TEST_DATA_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data")
TEST_DATA_DEMO = Path(TEST_DATA_DIR, "physionet.org", "files", "mimiciii-demo", "1.4")
TEST_GT_DIR = Path(TEST_DATA_DIR, "generated-benchmark")

TASK_NAMES = ["IHM", "DECOMP", "LOS", "PHENO"]
FTASK_NAMES = ["in-hospital-mortality", "decompensation", "length-of-stay", "phenotyping"]
TASK_NAME_MAPPING = dict(zip(TASK_NAMES, FTASK_NAMES))

with Path(os.getenv("CONFIG"), "datasets.json").open() as file:
    DATASET_SETTINGS = json.load(file)
    DECOMP_SETTINGS = DATASET_SETTINGS["DECOMP"]
    LOS_SETTINGS = DATASET_SETTINGS["LOS"]
    PHENOT_SETTINGS = DATASET_SETTINGS["PHENO"]
    IHM_SETTINGS = DATASET_SETTINGS["IHM"]


### FILE: .\tests\test_generators.py ###
import datasets
import pytest
from generators.tf2 import TFGenerator
from generators.pytorch import TorchGenerator
from generators.stream import RiverGenerator
from preprocessing.scalers import MIMICMinMaxScaler
import numpy as np
from utils.IO import *
from datasets.readers import ProcessedSetReader
from tests.settings import *
from preprocessing.imputers import PartialImputer


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_tf_generator(task_name, discretized_readers):
    tests_io(f"Test case tf2 iterative generator for task: {task_name}", level=0)

    # Prepare generator inputs
    reader = discretized_readers[task_name]
    scaler = MIMICMinMaxScaler().fit_reader(reader)

    # Bining types for LOS
    for bining in ["none", "log", "custom"]:
        # Batch sizes for dimensional robustness
        for batch_size in [1, 8, 16]:
            tests_io(f"Test case batch size: {batch_size}" + \
                    (f" and bining: {bining}" if task_name == "LOS" else ""))

            # Create generator
            generator = TFGenerator(reader=reader,
                                    scaler=scaler,
                                    batch_size=batch_size,
                                    bining=bining,
                                    shuffle=True)
            assert len(generator)
            for batch in range(len(generator)):
                # Get batch
                X, y = generator.__getitem__()
                # Check batch
                assert_batch_sanity(X, y, batch_size, bining)
                tests_io(f"Successfully tested {batch + 1} batches", flush=True)
            tests_io(f"Successfully tested {batch + 1} batches")
        if task_name != "LOS":
            break


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_torch_generator(task_name, discretized_readers):
    tests_io(f"Test case torch iterative generator for task: {task_name}", level=0)

    # Prepare generator inputs
    reader = discretized_readers[task_name]
    scaler = MIMICMinMaxScaler().fit_reader(reader)

    # Bining types for LOS
    for bining in ["none", "log", "custom"]:
        # Batch sizes for dimensional robustness
        for batch_size in [1, 8, 16]:
            tests_io(f"Test case batch size: {batch_size}" + \
                    (f" and bining: {bining}" if task_name == "LOS" else ""))
            # Create generator
            generator = TorchGenerator(reader=reader,
                                       scaler=scaler,
                                       batch_size=batch_size,
                                       bining=bining,
                                       drop_last=True,
                                       shuffle=True)
            assert len(generator)
            for batch, (X, y) in enumerate(generator):
                # Get batch
                X = X.numpy()
                y = y.numpy()
                # Check batch
                assert_batch_sanity(X, y, batch_size, bining)

                tests_io(f"Successfully tested {batch + 1} batches", flush=True)
            tests_io(f"Successfully tested {batch + 1} batches")
        if task_name != "LOS":
            break


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_river_generator(task_name, engineered_readers):
    tests_io(f"Test case river generator for task: {task_name}", level=0)

    # Prepare generator inputs
    reader = engineered_readers[task_name]
    imputer = PartialImputer().fit_reader(reader)
    scaler = MIMICMinMaxScaler(imputer=imputer).fit_reader(reader)

    # Bining types for LOS
    for bining in ["log", "custom"]:  # ["none", "log", "custom"]:
        # No Batch sizes this is a stream
        generator = RiverGenerator(reader=reader, scaler=scaler, shuffle=True, bining=bining)
        if task_name == "LOS":
            tests_io(f"Test case with bining: {bining}")
        # Create generator
        for batch, (X, y) in enumerate(generator):
            # Get batch
            X = np.fromiter(X.values(), dtype=float)
            if task_name == "PHENO" or (task_name == "LOS" and bining != "none"):
                y = np.fromiter(y.values(), dtype=float)
            assert_sample_sanity(X, y, bining)
            tests_io(f"Successfully tested {batch + 1} batches", flush=True)
        tests_io(f"Successfully tested {batch + 1} batches")
        if task_name != "LOS":
            break


def assert_batch_sanity(X: np.ndarray, y: np.ndarray, batch_size: int, bining: str):
    # The batch might be sane but I am not
    assert not np.isnan(X).any()
    assert not np.isnan(y).any()
    assert X.shape[0] == batch_size
    assert X.shape[2] == 59
    assert X.dtype == np.float32
    assert y.dtype == np.int8
    assert y.shape[0] == batch_size
    if task_name in ["PHENO"]:
        assert y.shape[1] == 25
    elif task_name in ["DECOMP", "IHM"]:
        assert y.shape[1] == 1
    elif task_name in ["LOS"]:
        # Depending on the binning this changes
        if bining == "none":
            assert y.shape[1] == 1
        elif bining in ["log", "custom"]:
            assert y.shape[1] == 10


def assert_sample_sanity(X: np.ndarray, y: np.ndarray, bining: str):
    assert not np.isnan(X).any()
    assert not np.isnan(y).any()
    assert len(X) == 714
    if task_name == "PHENO":
        assert len(y) == 25
    elif task_name == "LOS":
        if bining == "none":
            assert len(y) == 1
        elif bining in ["log", "custom"]:
            assert len(y) == 10
    else:
        assert isinstance(y, (float, int, bool))


if __name__ == "__main__":
    # for task_name in TASK_NAMES:
    for task_name in ["LOS"]:
        """
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    discretize=True,
                                    time_step_size=1.0,
                                    start_at_zero=True,
                                    impute_strategy='previous',
                                    task=task_name)
        # test_tf_generator(task_name, {task_name: reader})
        test_torch_generator(task_name, {task_name: reader})
        """
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    engineer=True,
                                    task=task_name)
        test_river_generator(task_name, {task_name: reader})


### FILE: .\tests\__init__.py ###



### FILE: .\tests\etc\convert_columns.py ###
import pandas as pd
from datasets.mimic_utils import upper_case_column_names
from tests.settings import *

for csv in TEST_DATA_DEMO.iterdir():
    if csv.is_dir() or csv.suffix != ".csv":
        continue

    df = pd.read_csv(csv,
                     dtype={
                         "ROW_ID": 'Int64',
                         "ICUSTAY_ID": 'Int64',
                         "HADM_ID": 'Int64',
                         "SUBJECT_ID": 'Int64',
                         "row_id": 'Int64',
                         "icustay_id": 'Int64',
                         "hadm_id": 'Int64',
                         "subject_id": 'Int64'
                     },
                     low_memory=False)
    df = upper_case_column_names(df)
    df.to_csv(csv, index=False)


### FILE: .\tests\etc\discretize_data.py ###
import os
import sys

sys.path.append(os.getenv("WORKINGDIR"))
import pandas as pd
from pathlib import Path
from tests.settings import *
from mimic3benchmark.readers import DecompensationReader

from mimic3models.preprocessing import Discretizer
from mimic3benchmark.readers import InHospitalMortalityReader, DecompensationReader, LengthOfStayReader, PhenotypingReader

# Set the paths to the data files
processed_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation")
}

discretized_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "decompensation")
}

readers = {
    "IHM": InHospitalMortalityReader,
    "LOS": LengthOfStayReader,
    "PHENO": PhenotypingReader,
    "DECOMP": DecompensationReader
}

impute_strategies = ['zero', 'normal_value', 'previous', 'next']
start_times = ['zero', 'relative']

# Discritize the data from processed directory using different discretizer settings
for task in TASK_NAMES:
    list_file_path = Path(processed_paths[task], "listfile.csv")
    list_file = pd.read_csv(list_file_path)
    if task in ["IHM", "PHENO"]:
        example_indices = list_file.index
    else:
        example_indices = list_file.groupby("stay")["period_length"].idxmax().values
    discretized_paths[task].mkdir(parents=True, exist_ok=True)
    list_file.loc[example_indices].to_csv(Path(discretized_paths[task], "listfile.csv"),
                                          index=False)

    print(f"Discretizing {task} data")
    for impute_strategy in impute_strategies:
        for start_time in start_times:
            discretizer = Discretizer(timestep=1.0,
                                      store_masks=False,
                                      impute_strategy=impute_strategy,
                                      start_time=start_time)
            reader = readers[task](dataset_dir=processed_paths[task])
            for idx in example_indices:
                sample = reader.read_example(idx)
                discretized_data = discretizer.transform(sample['X'])
                discretizer_header = discretized_data[1].split(',')
                target_dir = Path(discretized_paths[task],
                                  f"imp{impute_strategy}_start{start_time}")
                target_dir.mkdir(parents=True, exist_ok=True)
                pd.DataFrame(discretized_data[0],
                             columns=discretizer_header).to_csv(Path(target_dir, sample['name']),
                                                                index=False)


### FILE: .\tests\etc\engineer_data.py ###
import os
import sys

sys.path.append(os.getenv("WORKINGDIR"))
import pandas as pd
import numpy as np
from pathlib import Path
from tests.settings import TEST_DATA_DIR, TASK_NAMES
# This is copied into the mimic3benchmark directory once cloned
from mimic3benchmark.readers import InHospitalMortalityReader, DecompensationReader, LengthOfStayReader, PhenotypingReader
from mimic3models.in_hospital_mortality.logistic.main import read_and_extract_features as ihm_extractor
from mimic3models.phenotyping.logistic.main import read_and_extract_features as phenotyping_extractor
from mimic3models.length_of_stay.logistic.main import read_and_extract_features as los_extractor
from mimic3models.decompensation.logistic.main import read_and_extract_features as decompensation_extractor

# Set the paths to the data files
processed_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation")
}

engineered_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "decompensation")
}

readers = {
    "IHM": InHospitalMortalityReader,
    "LOS": LengthOfStayReader,
    "PHENO": PhenotypingReader,
    "DECOMP": DecompensationReader
}

extractors = {
    "IHM": ihm_extractor,
    "LOS": los_extractor,
    "PHENO": phenotyping_extractor,
    "DECOMP": decompensation_extractor
}

# Create the readers for each task type
ihm_reader = InHospitalMortalityReader(dataset_dir=processed_paths["IHM"],
                                       listfile=Path(processed_paths["IHM"], "listfile.csv"))
decomp_reader = DecompensationReader(dataset_dir=processed_paths["DECOMP"],
                                     listfile=Path(processed_paths["DECOMP"], "listfile.csv"))
los_reader = LengthOfStayReader(dataset_dir=processed_paths["LOS"],
                                listfile=Path(processed_paths["LOS"], "listfile.csv"))
phenotyping_reader = PhenotypingReader(dataset_dir=processed_paths["PHENO"],
                                       listfile=Path(processed_paths["PHENO"], "listfile.csv"))

for task in TASK_NAMES:
    if engineered_paths[task].exists():
        continue
    if task in ["LOS", "DECOMP"]:
        print(f"Engineering data for task: {task}. This may take up to 30 min ...")
    else:
        print(f"Engineering data for task: {task}.")
    reader = readers[task](dataset_dir=processed_paths[task],
                           listfile=Path(processed_paths[task], "listfile.csv"))
    if task == "IHM":
        (X, y, train_names) = extractors[task](reader, period="all", features="all")
    elif task == "PHENO":
        (X, y, train_names, ts) = extractors[task](reader, period="all", features="all")
    else:
        n_samples = min(100000, reader.get_number_of_examples())
        (X, y, train_names, ts) = extractors[task](reader,
                                                   period="all",
                                                   features="all",
                                                   count=n_samples)

    print(f"Done engineering data for task: {task}.")
    # 10127_episode271544_timeseries.csv is included in the original DS despite the subject being a new born infant. Minimum age was set at 18
    X_df = pd.DataFrame(np.concatenate([np.array(train_names).reshape(-1, 1), X],
                                       axis=1)).set_index(0)
    if task == "IHM":
        y_df = pd.DataFrame(np.stack([np.array(train_names), y]).T).set_index(0)
    elif task == "PHENO":
        y_df = pd.DataFrame(
            np.concatenate([np.array(train_names).reshape(-1, 1),
                            np.array(y)], axis=1)).set_index(0)
    else:
        y_df = pd.DataFrame(
            np.concatenate([np.array(train_names).reshape(-1, 1),
                            np.array(y).reshape(-1, 1)],
                           axis=1)).set_index(0)
    engineered_paths[task].mkdir(parents=True, exist_ok=True)
    X_df.to_csv(str(Path(engineered_paths[task], "X.csv")))
    y_df.to_csv(str(Path(engineered_paths[task], "y.csv")))


### FILE: .\tests\etc\rename_files.py ###
import pdb
import re
import os
import pandas as pd
from pathlib import Path
import sys
sys.path.append(os.getenv("WORKINGDIR"))
from tests.settings import *


for subject_dir in Path(TEST_DATA_DIR, "generated-benchmark", "extracted").iterdir():
    if not subject_dir.is_dir():
        continue
    for csv in subject_dir.iterdir():
        if "episode" in csv.name and not "timeseries" in csv.name:
            df = pd.read_csv(csv)
            icustay_id = df["Icustay"].iloc[0]
            file_index = re.findall(r'\d+', csv.name).pop()
            csv.rename(Path(csv.parent, f"episode{icustay_id}.csv"))
            Path(csv.parent, f"episode{file_index}_timeseries.csv").rename(Path(csv.parent, f"episode{icustay_id}_timeseries.csv"))
            # I want to rename the files episodeX.csv and episodeX_timeseries.csv by replacing X, which in this case is a random number by the icustay


### FILE: .\tests\etc\revert_split.py ###
import os
from pathlib import Path
import pandas as pd
import sys

sys.path.append(os.getenv("WORKINGDIR"))
from tests.settings import *

result_paths = [
    Path(TEST_DATA_DIR, "generated-benchmark", "extracted"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "multitask"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation"),
]

for path in result_paths:
    for entity in path.iterdir():
        if entity.is_dir():
            for subject_entity in entity.iterdir():
                if subject_entity.is_dir():
                    for csv in subject_entity.iterdir():
                        target_path = Path(csv.parents[2], csv.parent.name, csv.name)
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        csv.rename(target_path)
                    subject_entity.rmdir()
                else:
                    target_path = Path(subject_entity.parents[1], subject_entity.name)
                    if subject_entity.name == "listfile.csv" and target_path.exists():
                        listfile_df = pd.read_csv(subject_entity)
                        listfile_df.to_csv(target_path, mode='a', index=False, header=False)
                        subject_entity.unlink()
                    else:
                        subject_entity.rename(target_path)
            entity.rmdir()


### FILE: .\tests\etc\__init__.py ###


### FILE: .\tests\pytest_utils\discretization.py ###
import pytest
import datasets
import re
import shutil

import pandas as pd
import numpy as np
from pathlib import Path
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from preprocessing.discretizers import MIMICDiscretizer
from tests.pytest_utils.general import assert_dataframe_equals
from utils.IO import *
from tests.settings import *


def prepare_processed_data(task_name: str, listfile: pd.DataFrame, reader: ProcessedSetReader):
    # Legacy period length calculation may cut off legitimate data
    source_path = Path(SEMITEMP_DIR, "processed", task_name)
    target_path = Path(TEMP_DIR, "processed", task_name)
    write = DataSetWriter(target_path)
    X_processed, y_processed = reader.read_samples(read_ids=True).values()
    if "period_length" in listfile.columns:
        for subject_id in X_processed:
            for stay_ids in X_processed[subject_id]:
                X = X_processed[subject_id][stay_ids]
                X = X[X.index < listfile.loc[f"{subject_id}_episode{stay_ids}_timeseries.csv"]
                      ["period_length"] + 1e-6]
                X_processed[subject_id][stay_ids] = X

    write.write_bysubject({"X": X_processed})
    write.write_bysubject({"y": y_processed})

    for file in source_path.iterdir():
        if not file.is_file():
            continue
        shutil.copy(str(file), str(target_path))
    return


def assert_strategy_equals(X_strategy: dict, test_data_dir: Path):
    tested_stay = set()
    subject_count = 0
    stay_count = 0

    tests_io(f"Subjects tested: {subject_count}\n"
             f"Stays tested: {stay_count}")

    for test_file_path in test_data_dir.iterdir():
        test_df = pd.read_csv(test_file_path)
        test_df.index.name = "bins"
        test_df = test_df.reset_index()

        # Extract subject id and stay id
        match = re.search(r"(\d+)_episode(\d+)_timeseries\.csv", test_file_path.name)
        subject_id = int(match.group(1))
        stay_id = int(match.group(2))
        tested_stay.add(stay_id)

        # Read sample
        subject_count += int(all([stay in tested_stay for stay in X_strategy[subject_id]]))
        stay_count += 1

        X = X_strategy[subject_id][stay_id]

        # Ensure column identity
        test_df.columns = test_df.columns.str.strip(" ")
        X.columns = X.columns.str.strip(" ")

        missing_columns = set(test_df) - set(X)
        additional_columns = set(X) - set(test_df)
        assert not missing_columns
        assert not additional_columns

        # Test X against ground truth
        test_df = test_df[X.columns]
        assert_dataframe_equals(X.astype(float), test_df)

        tests_io(f"Subjects tested: {subject_count}\n"
                 f"Stays tested: {stay_count}",
                 flush_block=True)


### FILE: .\tests\pytest_utils\extraction.py ###
import pandas as pd
from typing import Dict
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals


def compare_extracted_datasets(generated_data: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],
                               test_data: Dict[str, Dict[str, Dict[str, pd.DataFrame]]]):
    # Filtypes are: timeseries, subject_events, episodic_data, subject_icu_stays
    tests_io(f"Comparing extracted datasets\n"
             f"Compared subjects: {0}\n"
             f"Compared timeseries: {0}\n"
             f"Compared subject events: {0}\n"
             f"Compared episodic data: {0}"
             f"Compared stay data: {0}")
    n_subjects = 0
    n_timeseries = 0
    n_subject_events = 0
    n_episodic_data = 0
    n_stay_data = 0
    for subject_id, subject_data in generated_data.items():
        for file_type, type_data in subject_data.items():
            if file_type == "timeseries":
                for stay_id, stay_data in type_data.items():
                    # Timeseries is structured
                    assert_dataframe_equals(stay_data, test_data[subject_id][file_type][stay_id])
                    n_timeseries += 1
            elif file_type == "subject_events":
                sorted_type_data = type_data.sort_values(
                    by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
                sorted_type_data = sorted_type_data.reset_index(drop=True)
                sorted_gt_data = test_data[subject_id][file_type].sort_values(
                    by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
                sorted_gt_data = sorted_gt_data.reset_index(drop=True)
                assert_dataframe_equals(sorted_type_data, sorted_gt_data)
                n_subject_events += 1
            else:
                assert_dataframe_equals(type_data, test_data[subject_id][file_type])
                if file_type == "episodic_data":
                    n_episodic_data += 1
                else:
                    n_stay_data += 1

            tests_io(
                f"Comparing extracted datasets\n"
                f"Compared subjects: {n_subjects}\n"
                f"Compared timeseries: {n_timeseries}\n"
                f"Compared subject events: {n_subject_events}\n"
                f"Compared episodic data: {n_episodic_data}\n"
                f"Compared stay data: {n_stay_data}",
                flush_block=True)

        n_subjects += 1

    tests_io(
        f"Comparing extracted datasets\n"
        f"Compared subjects: {n_subjects}\n"
        f"Compared timeseries: {n_timeseries}\n"
        f"Compared subject events: {n_subject_events}\n"
        f"Compared episodic data: {n_episodic_data}\n"
        f"Compared stay data: {n_stay_data}",
        flush_block=True)


### FILE: .\tests\pytest_utils\feature_engineering.py ###
import re
import pandas as pd
import numpy as np


def extract_test_ids(df: pd.DataFrame):
    regex = r"(\d+)_episode(\d+)_timeseries\.csv"
    df["SUBJECT_ID"] = df["0"].apply(lambda x: re.search(regex, x).group(1))
    df["ICUSTAY_ID"] = df["0"].apply(lambda x: re.search(regex, x).group(2))
    df = df.drop("0", axis=1)
    return df


def concatenate_dataset(data) -> pd.DataFrame:
    data_stack = list()
    for subject_id, subject_stays in data.items():
        for stay_id, frame in subject_stays.items():
            if len(np.squeeze(frame).shape) == 1:
                data_stack.append(np.squeeze(frame).tolist() + [subject_id, stay_id])
            else:
                for row in np.squeeze(frame).tolist():
                    data_stack.append(row + [subject_id, stay_id])

    dfs = pd.DataFrame(data_stack,
                       columns=[str(idx) for idx in range(1, 715)] + ["SUBJECT_ID", "ICUSTAY_ID"])

    if not len(dfs):
        return
    return dfs


### FILE: .\tests\pytest_utils\general.py ###
import pandas as pd
import numpy as np
from utils.IO import *
from utils import is_numerical, is_colwise_numerical


def assert_dataframe_equals(generated_df: pd.DataFrame,
                            test_df: pd.DataFrame,
                            rename: dict = {},
                            normalize_by: str = "generated",
                            compare_mode: str = "single_entry"):
    """Compares the dataframes and print out the results. The age column cannot be compared, as the results are
    erroneous in the original dataset.

    Args:
        generated_df (pd.DataFrame): DataFrame generated by the MIMIC-III-Rework
        test_df (pd.DataFrame): DataFrame generated by the original MIMIC-III code.
        rename (dict, optional): Renaming of columns if necessary. Defaults to {}.
        normalize_by (str, optional): Define by which dataframe to structure the comparision. Can be 'generated' or 'groundtruth'. Defaults to "generated".
        compare_mode (str, optional): Define wether to compare data in absolute or proxemity terms. Data from numpy data files should be compared using the 
                                      proxemity mode. Can be 'absolute' or 'proxemity'. Defaults to "abolute".
    """
    assert normalize_by in ['generated', 'groundtruth'
                           ], "normalize_by needs to be one of 'generated' or 'groundtruth'."
    assert compare_mode in ['single_entry', 'multiline'
                           ], "compare_mode needs to be one of 'single_entry' or 'multiline'."

    # Assert shape 2
    assert len(generated_df.columns) == len(test_df.columns), (
        f"Generated and ground truth dataframes do not have the same amount of columns.\n"
        f"Generated: {len(generated_df.columns)}, Ground truth: {len(test_df.columns)}.")

    # Assert shape 1
    if not len(generated_df) == len(test_df):
        tests_io("Dataframe has diff!")
        difference = pd.concat([test_df, generated_df]).drop_duplicates(keep=False)
        tests_io(
            f"Length generated: {len(generated_df)}\nLength test: {len(test_df)}\nDiff is: \n{difference}"
        )
        assert len(generated_df) == len(test_df), (
            f"Generated and ground truth dataframes do not have the same amount of rows.\n"
            f"Generated: {len(generated_df)}, Ground truth: {len(test_df)}.")

    generated_df = generated_df.rename(columns=rename)
    if normalize_by == "generated":
        # Age erroneous in test code
        if "AGE" in generated_df:
            generated_df = generated_df.drop(columns=["AGE"])
        elif "Age" in test_df:
            generated_df = generated_df.drop(columns=["Age"])
        test_df = test_df[generated_df.columns]
    else:
        # Age erroneous in test code
        if "Age" in test_df:
            test_df = test_df.drop(columns=["Age"])
        elif "AGE" in test_df:
            test_df = test_df.drop(columns=["AGE"])
        generated_df = generated_df[test_df.columns]

    # Check dtypes TODO! used to work but fails on extraction because IDs are floats
    # assert (test_df.dtypes == generated_df.dtypes).all(), (
    #     f"Generated and ground truth dataframes do not have the same dtypes.\n"
    #     f"Generated: {generated_df.dtypes[(test_df.dtypes != generated_df.dtypes)]}, Ground truth: { test_df.dtypes[(test_df.dtypes != generated_df.dtypes)]}."
    # )

    # Check performed column wise
    if compare_mode == "single_entry":
        difference = 0
        frame_diff = (generated_df.fillna("NaN") != test_df.fillna("NaN"))
        for column_name in frame_diff:
            if not frame_diff[column_name].any():
                continue
            gen_col = generated_df[[column_name]]
            test_col = test_df[[column_name]]

            # Absolute vs threshold comparision depending on dtype:
            if is_numerical(gen_col) or is_numerical(test_col):
                # Checking for diffs with general float type
                # Casting float because pandas float and integer are not supported
                # Pandas has no in-house implementation of isclose
                column_diff = np.squeeze(
                    ~np.isclose(gen_col.astype(float), test_col.astype(float), equal_nan=True))
            else:
                column_diff = np.squeeze((gen_col.fillna("Na") != \
                                          test_col.fillna("Na")).values)
            if column_diff.shape == ():
                column_diff = column_diff.reshape(1)
            # If all identical continue
            if not column_diff.any():
                continue
            difference += 1
            display_diff_df = pd.concat([gen_col, test_col], axis=1)
            display_diff_df.index = gen_col.index
            display_diff_df.columns = ["Generated", "Groundtruth"]
            display_diff_df = display_diff_df[column_diff]
            tests_io(f"For column {column_name}:\n" f"-------------------\n" f"{display_diff_df}")
    elif compare_mode == "multiline":
        # For every subject and stay id
        stay_count = 0
        difference = 0
        sample_count = 0
        subject_buffer = list()
        if "SUBJECT_ID" in generated_df and "ICUSTAY_ID" in generated_df:
            is_numerical_dict = is_colwise_numerical(test_df)
            is_numerical_dict = {
                column: is_numerical_dict[column] or value
                for column, value in is_colwise_numerical(generated_df).items()
            }
            tests_io(f"Total stays checked: {stay_count}\n"
                     f"Total subjects checked: {len(set(subject_buffer))}\n"
                     f"Total samples checked: {sample_count}\n"
                     f"Total differences found: {difference}")
            for subject_id in generated_df["SUBJECT_ID"].unique():
                # Get all rows with the same subject_id
                subject_data = generated_df[generated_df["SUBJECT_ID"] == subject_id]
                test_subject_data = test_df[test_df["SUBJECT_ID"] == subject_id]
                for icustay_id in subject_data["ICUSTAY_ID"].unique():
                    # Get all rows with the same icustay_id
                    gen_rows = subject_data[subject_data["ICUSTAY_ID"] == icustay_id]
                    test_rows = test_subject_data[test_subject_data["ICUSTAY_ID"] == icustay_id]
                    for _, gen_row in gen_rows.iterrows():
                        diff = compare_homogenous_multiline_df(gen_row.to_frame().T, test_rows)

                        if diff:
                            tests_io(
                                f"For file {subject_id}_episode{icustay_id}_timeseries.csv, no equivalent entry in test_df found:\n"
                                f"-------------------\n"
                                f"{gen_row}")
                            difference += 1
                        sample_count += 1
                        tests_io(
                            f"Total stays checked: {stay_count}\n"
                            f"Total subjects checked: {len(set(subject_buffer))}\n"
                            f"Total samples checked: {sample_count}\n"
                            f"Total differences found: {difference}",
                            flush_block=True)

                    # Progress variables
                    stay_count += 1
                    subject_buffer.append(subject_id)
        else:
            # Find a matching row in test_df with the same index
            difference = compare_homogenous_multiline_df(generated_df, test_df, verbose=True)

    assert not difference, f"Diffs detected between generated and ground truth files: {difference}!"


def split_dataframes_by_type(df: pd.DataFrame, numerical_dict: dict):
    numerical_cols = [col for col, is_num in numerical_dict.items() if is_num]
    non_numerical_cols = [col for col, is_num in numerical_dict.items() if not is_num]
    return df[numerical_cols].astype(float), df[non_numerical_cols].astype("object")


def compare_homogenous_multiline_df(generated_df: pd.DataFrame,
                                    test_df: pd.DataFrame,
                                    verbose: bool = False):
    numerical_dict = is_colwise_numerical(test_df)
    numerical_dict.update({
        column: numerical_dict.get(column, False) or value
        for column, value in is_colwise_numerical(generated_df).items()
    })

    # Split both DataFrames
    gen_num_df, gen_non_num_df = split_dataframes_by_type(generated_df, numerical_dict)
    test_num_df, test_non_num_df = split_dataframes_by_type(test_df, numerical_dict)

    difference = 0
    sample_count = 0

    # Iterate over generated DataFrame rows
    for idx in gen_num_df.index:
        gen_num_row = gen_num_df.loc[idx]
        gen_non_num_row = gen_non_num_df.loc[idx]
        num_match = np.isclose(test_num_df.astype(float).values,
                               gen_num_row.astype(float).values,
                               equal_nan=True).all(axis=1)
        non_num_match = (gen_non_num_row.fillna("NaN") == test_non_num_df.fillna("NaN")).all(
            axis=1).values
        found = (num_match & non_num_match).any()
        if not found:
            # I don't remember why I did this, its been over a month
            difference += 1
            raise LookupError(
                f"No equivalent entry found for row index {idx}:\n{generated_df.loc[idx].to_frame().T}"
            )
        sample_count += 1
        if verbose:
            info_io(f"Total samples checked: {sample_count}\nTotal differences found: {difference}",
                    flush_block=True)

    return difference


### FILE: .\tests\pytest_utils\preprocessing.py ###
import shelve
import pandas as pd
import numpy as np
from pathlib import Path
from utils.IO import *
from tests.pytest_utils.general import assert_dataframe_equals
from datasets.readers import ProcessedSetReader
from tests.settings import *


def assert_reader_equals(reader: ProcessedSetReader, test_data_dir: Path):
    """_summary_

    Args:
        reader (SampleSetReader): _description_
        test_data_dir (Path): _description_
    """
    assert reader.subject_ids, "The reader subjects are empty! Extraction must have failed."
    subject_count = 0
    stay_count = 0
    tests_io(f"Stays frames compared: {stay_count}\n"
             f"Total subjects checked: {subject_count}")
    listfile = pd.read_csv(Path(test_data_dir, "listfile.csv"))
    for subject_id in reader.subject_ids:
        X_stays, y_stays = reader.read_sample(subject_id, read_ids=True).values()
        subject_count += 1
        stay_count += assert_subject_data_equals(subject_id=subject_id,
                                                 X_stays=X_stays,
                                                 y_stays=y_stays,
                                                 test_data_dir=test_data_dir,
                                                 root_path=reader.root_path,
                                                 listfile=listfile)
        tests_io(f"Compared subjects: {subject_count}\n"
                 f"Compared stays: {stay_count}\n",
                 flush_block=True)


def assert_dataset_equals(X: dict, y: dict, generated_dir: Path, test_data_dir: Path):
    """_summary_

    Args:
        reader (SampleSetReader): _description_
        test_data_dir (Path): _description_
    """
    assert len(X) and len(y), "The reader subjects are empty! Extraction must have failed."
    subject_count = 0
    stay_count = 0
    tests_io(f"Stays frames compared: {stay_count}\n"
             f"Total subjects checked: {subject_count}")
    listfile = pd.read_csv(Path(test_data_dir, "listfile.csv"))
    for subject_id in X:
        X_stays, y_stays = X[subject_id], y[subject_id]
        subject_count += 1
        stay_count += assert_subject_data_equals(subject_id=subject_id,
                                                 X_stays=X_stays,
                                                 y_stays=y_stays,
                                                 test_data_dir=test_data_dir,
                                                 root_path=generated_dir,
                                                 listfile=listfile)
        tests_io(f"Compared subjects: {subject_count}\n"
                 f"Compared stays: {stay_count}\n",
                 flush_block=True)


def assert_subject_data_equals(subject_id: int,
                               X_stays: dict,
                               y_stays: dict,
                               test_data_dir: Path,
                               root_path: Path,
                               listfile: pd.DataFrame = None):
    stay_count = 0
    for stay_id in X_stays.keys():
        X = X_stays[stay_id]
        y = y_stays[stay_id]
        try:
            test_data = pd.read_csv(
                Path(test_data_dir, f"{subject_id}_episode{stay_id}_timeseries.csv"))
        except:
            raise FileNotFoundError(f"Test set is missing {subject_id}"
                                    "_episode{stay_id}_timeseries.csv")
        y_true = listfile[listfile["stay"] == f"{subject_id}_episode{stay_id}_timeseries.csv"]
        if len(y) == 1:
            if "y_true" in y_true:
                assert np.isclose(float(np.squeeze(y_true["y_true"].values)), float(np.squeeze(y)))
            else:
                y_true_vals = y_true[[
                    value for value in y_true.columns if not value in ["stay", "period_length"]
                ]].values
                assert np.allclose(np.squeeze(y_true_vals), np.squeeze(y.values))
        else:
            if "y_true" in y_true:
                y_true = y_true.sort_values(by=["period_length"])
                assert len(
                    y_true) == y_true["period_length"].max() - y_true["period_length"].min() + 1
                assert np.allclose(y_true["y_true"].values, np.squeeze(y.values))

        # Testing the preprocessing tracker at the same time
        with shelve.open(str(Path(root_path, "progress"))) as db:
            assert int(subject_id) in db["subjects"]
            assert int(stay_id) in db["subjects"][subject_id]
            assert db["subjects"][subject_id][stay_id] == len(y)
            assert db["subjects"][subject_id]["total"] == sum([
                lengths for stay_id, lengths in db["subjects"][subject_id].items()
                if stay_id != "total"
            ])

        assert_dataframe_equals(X.reset_index(),
                                test_data, {"hours": "Hours"},
                                normalize_by="groundtruth")
        stay_count += 1
    return stay_count


### FILE: .\tests\pytest_utils\__init__.py ###
import shutil
from pathlib import Path
from tests.settings import *


def copy_dataset(folder: Path):
    if not Path(TEMP_DIR, folder).is_dir():
        source_path = Path(SEMITEMP_DIR, folder)
        target_path = Path(TEMP_DIR, folder)
        source_path.parent.mkdir(parents=True, exist_ok=True)
        target_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copytree(str(Path(SEMITEMP_DIR, folder)), str(Path(TEMP_DIR, folder)))


### FILE: .\tests\test_case_handler\test_mimic_case_handler.py ###
import datasets
import os
import pandas as pd
import json
import shutil
from pathlib import Path
from utils.IO import *
from tests.settings import *

base_path = Path(TEST_DATA_DIR, "configs", "case_configs")

test_config_paths = [
    Path(base_path, "logistic_decomp"),
    Path(base_path, "logistic_ihm"),
    Path(base_path, "logistic_los"),
    Path(base_path, "logistic_phenotyping"),
    Path(base_path, "logistic_subcases")
]

# def test_folder_creation_subcases():


### FILE: .\tests\test_case_handler\__init__.py ###


### FILE: .\tests\test_datasets\test_data_split.py ###
"""
TODO! Add current iter and test restoral
"""

import datasets
import shutil
import pytest
import random
import pandas as pd
import numpy as np
from typing import List, Dict
from pathlib import Path
from utils.IO import *
from tests.settings import *
from datasets.readers import ProcessedSetReader, SplitSetReader
from sklearn import model_selection
from pathlib import Path
from datasets.trackers import DataSplitTracker, PreprocessingTracker
from pathos.multiprocessing import Pool, cpu_count
from datasets.split.splitters import ReaderSplitter
from utils import dict_subset, is_numerical


@pytest.mark.parametrize("preprocessing_style", ["discretized", "engineered"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_ratio_split(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io(f"Test case by ratio for task: {task_name}", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    tolerance = (1e-2 if task_name in ["DECOMP", "LOS"] else 1e-1)
    for test_size in [-0.1, 1.1]:
        with pytest.raises(ValueError):
            _: SplitSetReader = datasets.train_test_split(reader, test_size=test_size, val_size=0.2)
    for val_size in [-0.1, 1.1]:
        with pytest.raises(ValueError):
            _: SplitSetReader = datasets.train_test_split(reader, test_size=0.1, val_size=val_size)

    reader = discretized_readers[task_name]
    all_split_subjects = dict()
    curr_iter = 0

    # Test splitting for a few values
    for test_size in [0.0, 0.2, 0.4]:
        all_split_subjects[test_size] = dict()
        for val_size in [0.0, 0.2, 0.4]:
            all_split_subjects[test_size][val_size] = assert_split_sanity(
                test_size, val_size, tolerance, reader,
                datasets.train_test_split(reader,
                                          test_size=test_size,
                                          val_size=val_size,
                                          storage_path=Path(TEMP_DIR, "split_test",
                                                            str(curr_iter))))
            curr_iter += 1

    # Test restoring splits for previous values
    for test_size in [0.0, 0.2, 0.4]:
        for val_size in [0.0, 0.2, 0.4]:
            split_reader = datasets.train_test_split(reader,
                                                     test_size=test_size,
                                                     val_size=val_size,
                                                     storage_path=Path(
                                                         TEMP_DIR, "split_test", str(curr_iter)))
            for split_name in all_split_subjects[test_size][val_size]:
                assert set(all_split_subjects[test_size][val_size][split_name]) == \
                    set(getattr(split_reader, split_name).subject_ids)

            curr_iter += 1


@pytest.mark.parametrize("preprocessing_style", ["discretized", "engineered"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_demographic_filter(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io("Test case by demographic filter", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    # Test on unknown demographic
    with pytest.raises(ValueError):
        demographic_filter = {"INVALID": {"less": 0.5}}
        _: SplitSetReader = datasets.train_test_split(reader, demographic_filter=demographic_filter)

    # Test on invalid range
    for less_key in ["less", "leq"]:
        for greater_key in ["greater", "geq"]:
            with pytest.raises(ValueError):
                demographic_filter = {"AGE": {less_key: 10, greater_key: 90}}
                _: SplitSetReader = datasets.train_test_split(reader,
                                                              demographic_filter=demographic_filter)

    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    curr_iter = 0
    all_filters = list()
    for attribute in set(subject_info_df.columns) - set(["SUBJECT_ID", "ICUSTAY_ID"]):
        attribute_data = subject_info_df[attribute]
        if is_numerical(attribute_data.to_frame()):
            # Test on all range key words
            sample_filters = sample_numeric_filter(attribute, attribute_data)
            for demographic_filter in sample_filters:
                split_reader: SplitSetReader = datasets.train_test_split(
                    reader,
                    demographic_filter=demographic_filter,
                    storage_path=Path(TEMP_DIR, str(curr_iter)))
                if "train" in split_reader.split_names:
                    check_numerical_attribute(attribute=attribute,
                                              demographic_filter=demographic_filter,
                                              subject_info_df=subject_info_df,
                                              subject_ids=split_reader.train.subject_ids)
            all_filters.extend(sample_filters)
        else:
            # Categorical column
            demographic_filter = sample_categorical_filter(attribute, attribute_data)
            split_reader: SplitSetReader = datasets.train_test_split(
                reader,
                demographic_filter=demographic_filter,
                storage_path=Path(TEMP_DIR, str(curr_iter)))
            if "train" in split_reader.split_names:
                check_categorical_attribute(attribute=attribute,
                                            demographic_filter=demographic_filter,
                                            subject_info_df=subject_info_df,
                                            subject_ids=split_reader.train.subject_ids)
            all_filters.append(demographic_filter)
        curr_iter += 1

    for _ in range(10):
        demographic_filter = sample_hetero_filter(3, subject_info_df)
        split_reader: SplitSetReader = datasets.train_test_split(
            reader,
            demographic_filter=demographic_filter,
            storage_path=Path(TEMP_DIR, str(curr_iter)))
        if "train" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.train.subject_ids)
        curr_iter += 1


def test_demo_split(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io("Test case by demographic filter", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    curr_iter = 0

    for _ in range(10):
        demographic_filter = sample_hetero_filter(1, subject_info_df)

        split_reader: SplitSetReader = datasets.train_test_split(
            reader, demographic_split={"test": demographic_filter})
        if "test" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)

            assert not set(split_reader.test.subject_ids) & set(split_reader.train.subject_ids)
        #TODO! Add current iter and test restoral
        tests_io(f"Succeeded testing the filter for test!")

    for _ in range(10):
        test_filter, val_filter = sample_hetero_filter(1, subject_info_df, val_set=True)
        #TODO! Add current iter and test restoral
        split_reader: SplitSetReader = datasets.train_test_split(reader,
                                                                 demographic_split={
                                                                     "test": test_filter,
                                                                     "val": val_filter
                                                                 })
        if "test" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=test_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)
            if "train" in split_reader.split_names:
                assert not set(split_reader.test.subject_ids) & set(split_reader.train.subject_ids)

        if "val" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=val_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.val.subject_ids)
            if "test" in split_reader.split_names:
                assert not set(split_reader.val.subject_ids) & set(split_reader.train.subject_ids)
        if "val" in split_reader.split_names and "test" in split_reader.split_names:
            assert not set(split_reader.test.subject_ids) & set(split_reader.val.subject_ids)

        curr_iter += 1
        tests_io(f"Succeeded testing the filter for val and test!")


def test_demo_and_ratio_split(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io("Test case demographic split with specified ratios", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    attribute = random.choice(["WARDID", "AGE"])
    attribute_data = subject_info_df[attribute]
    filters = sample_numeric_filter(attribute, attribute_data)

    tolerance = (1e-2 if task_name in ["DECOMP", "LOS"] else 1e-1)
    test_size = 0.2
    none_reader = 0

    for demographic_filter in filters:
        tests_io(f"Specified demographic filter: 'test': {demographic_filter}")
        if not all([value for value in demographic_filter.values()]):
            continue
        try:
            split_reader: SplitSetReader = datasets.train_test_split(
                reader, 0.2, demographic_split={"test": demographic_filter})
        except ValueError:
            none_reader += 1
            continue

        if "test" in split_reader.split_names and "train" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)

            assert_split_sanity(test_size, 0, tolerance, reader, split_reader, reduced_set=True)
        else:
            none_reader += 1
    if none_reader == len(filters):
        tests_io(f"All splits have invalid sets for {attribute}!")

    filters = sample_numeric_filter(attribute, attribute_data, val_set=True)

    split_size = 0.2
    none_reader = 0
    for test_filter, val_filter in filters:
        tests_io(f"Specified demographic filter: 'test': {test_filter}, 'val': {val_filter}")

        split_filters = {"test": test_filter, "val": val_filter}
        try:
            split_reader: SplitSetReader = datasets.train_test_split(
                reader, split_size, split_size, demographic_split=split_filters)
        except ValueError as e:
            none_reader += 1
            continue
        if len(split_reader.split_names) > 2:
            for set_name in split_reader.split_names:
                if set_name in ["test", "val"]:
                    check_hetero_attributes(demographic_filter=split_filters[set_name],
                                            subject_info_df=subject_info_df,
                                            subject_ids=getattr(split_reader, set_name).subject_ids)

            assert_split_sanity(split_size,
                                split_size,
                                tolerance,
                                reader,
                                split_reader,
                                reduced_set=True)

        else:
            none_reader += 1

    if none_reader == len(filters):
        tests_io(f"All splits have invalid sets for {attribute}!")


def test_train_size(task_name, preprocessing_style, discretized_readers: Dict[str,
                                                                              ProcessedSetReader],
                    engineered_readers: Dict[str, ProcessedSetReader]):

    tests_io("Test case train size", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    # Train size with ratio
    tolerance = (1e-2 if task_name in ["DECOMP", "LOS"] else 1e-1)

    curr_iter = 0
    write_bool = True
    train_size = 8

    for val_size in [0.0, 0.2]:
        if val_size and write_bool:

            write_bool = False
        for test_size in [0.2, 0.4]:
            tests_io(
                f"Specified train_size: {train_size}; test_size: {test_size}; val_size: {val_size}")
            split_reader: SplitSetReader = \
                datasets.train_test_split(reader,
                                          test_size=test_size,
                                          val_size=val_size,
                                          storage_path=Path(
                                              TEMP_DIR, "split_test",
                                              str(curr_iter) + "base"))

            test_split_reader: SplitSetReader = \
                datasets.train_test_split(reader,
                                          test_size=test_size,
                                          val_size=val_size,
                                          train_size=train_size,
                                          storage_path=Path(TEMP_DIR, "split_test",
                                          str(curr_iter) + "_test"))
            if len(split_reader.train.subject_ids) < train_size:
                assert len(test_split_reader.train.subject_ids) == len(
                    split_reader.train.subject_ids)
            else:
                assert len(test_split_reader.train.subject_ids) == train_size
            assert_split_sanity(test_size,
                                val_size,
                                tolerance,
                                reader,
                                test_split_reader,
                                reduced_set=True)
            curr_iter += 1

    # Train size with demographic split
    tests_io("Specific train size with demographic split")
    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    attribute = random.choice(["WARDID", "AGE"])
    attribute_data = subject_info_df[attribute]
    filters = sample_numeric_filter(attribute, attribute_data)

    test_size = 0.2
    val_size = 0.2
    none_reader = 0

    for demographic_filter in filters:
        print(demographic_filter)
        try:
            split_reader: SplitSetReader = datasets.train_test_split(
                reader,
                test_size,
                val_size,
                train_size=train_size,
                demographic_split={"test": demographic_filter})
        except ValueError:
            none_reader += 1
            continue

        if not set(["test", "train", "val"]) - set(split_reader.split_names):
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)

            assert_split_sanity(test_size,
                                val_size,
                                tolerance,
                                reader,
                                split_reader,
                                reduced_set=True)
        else:
            none_reader += 1
    if none_reader == len(filters):
        tests_io(f"All splits have invalid sets for {attribute}!")


def sample_categorical_filter(attribute: str, attribute_sr: pd.Series, val_set: bool = False):
    # Randomly choose half of the possible categories
    categories = attribute_sr.unique()
    test_choices = random.sample(categories.tolist(), k=int(np.floor(len(categories) / 2)))
    # Demographic filter
    if val_set and len(test_choices):
        val_choices = random.sample(test_choices, k=int(len(test_choices) / 2))
        test_choices = list(set(test_choices) - set(val_choices))
        if test_choices and val_choices:
            return {attribute: {"choice": test_choices}}, {attribute: {"choice": val_choices}}
        else:
            return {}, {}

    return {attribute: {"choice": test_choices}}


def sample_numeric_filter(attribute: str, attribute_sr: pd.Series, val_set: bool = False):

    def get_range_key(greater_key: str,
                      less_key: str,
                      min_value: int = -1e10,
                      max_value: int = -1e10):
        curr_filter = dict()
        curr_range = max_value - min_value
        if greater_key is not None:
            lower_bound = random.uniform(min_value, min_value + curr_range / 2)
            curr_filter[greater_key] = lower_bound
        else:
            lower_bound = min_value
        if less_key is not None:
            upper_bound = random.uniform(lower_bound + curr_range / 10, max_value)
            curr_filter[less_key] = upper_bound
        else:
            upper_bound = max_value
        return curr_filter

    filters = list()
    inversion_mapping = {"less": "greater", "leq": "geq", "greater": "less", "geq": "leq"}
    for less_key in ["less", "leq", None]:
        for greater_key in ["greater", "geq", None]:
            # Get a valid range
            if less_key is None and greater_key is None:
                continue
            test_filter = get_range_key(greater_key, less_key, attribute_sr.min(),
                                        attribute_sr.max())
            if val_set:
                if test_filter.get(less_key) is None:
                    if test_filter.get(greater_key) is None:
                        # Both are None
                        continue
                    # Only less is not None
                    val_filter = get_range_key(greater_key,
                                               inversion_mapping[greater_key],
                                               min_value=attribute_sr.min(),
                                               max_value=test_filter[greater_key])
                else:
                    if test_filter.get(greater_key) is None:
                        # Only greater is not None
                        val_filter = get_range_key(inversion_mapping[less_key],
                                                   less_key,
                                                   min_value=test_filter[less_key],
                                                   max_value=attribute_sr.max())
                    else:
                        # Both are set
                        if random.choice([True, False]):
                            val_filter = get_range_key(greater_key,
                                                       less_key,
                                                       min_value=test_filter[less_key],
                                                       max_value=attribute_sr.max())
                        else:
                            val_filter = get_range_key(greater_key,
                                                       less_key,
                                                       min_value=attribute_sr.min(),
                                                       max_value=test_filter[greater_key])
                filters.append(({attribute: test_filter}, {attribute: val_filter}))
            else:
                filters.append({attribute: test_filter})
    return filters


def sample_hetero_filter(num: int, subject_info_df: pd.DataFrame, val_set: bool = False):
    # Test on random demographic
    all_filters = list()
    for attribute in set(subject_info_df.columns) - set(["SUBJECT_ID", "ICUSTAY_ID"]):
        if is_numerical(subject_info_df[attribute].to_frame()):
            # Test on all range key words
            possible_filter = sample_numeric_filter(attribute=attribute,
                                                    attribute_sr=subject_info_df[attribute],
                                                    val_set=val_set)
            all_filters.extend(possible_filter)
        else:
            # Categorical column
            possible_filter = sample_categorical_filter(attribute=attribute,
                                                        attribute_sr=subject_info_df[attribute],
                                                        val_set=val_set)
            all_filters.append(possible_filter)
    test_filter = dict()
    val_filter = dict()
    filter_selection = random.sample(all_filters, k=max(num * 5, len(all_filters)))
    attributes = list()
    for curr_filter in filter_selection:
        # No duplicate for same attribute
        if val_set and not set(curr_filter[0].keys()) & set(attributes):
            # No empty filters
            if val_set and list(curr_filter[0].values())[0] and list(curr_filter[1].values())[0]:
                test_filter.update(curr_filter[0])
                val_filter.update(curr_filter[1])
                attributes.extend(curr_filter[0].keys())
        elif not val_set and not set(curr_filter.keys()) & set(attributes):
            # No empty filters
            if not val_set and list(curr_filter.values())[0]:
                test_filter.update(curr_filter)
                attributes.extend(curr_filter.keys())
        if len(test_filter) == num:
            break
    if val_set:
        return test_filter, val_filter
    return test_filter


def assert_split_sanity(test_size: float,
                        val_size: float,
                        tolerance: float,
                        reader: ProcessedSetReader,
                        split_reader: SplitSetReader,
                        reduced_set: bool = False):
    tracker = PreprocessingTracker(Path(reader.root_path, "progress"))
    subject_counts = tracker.subjects
    subject_ids = tracker.subject_ids
    split_samples = dict()
    split_subjects = dict()

    # Extract result for each split
    for split_name in split_reader.split_names:
        # Check sample is not empty
        random_sample = getattr(split_reader, split_name).random_samples()
        assert len(random_sample["X"])
        # Get subject IDs and sample counts
        split_ids = getattr(split_reader, split_name).subject_ids
        split_subjects[split_name] = split_ids
        split_samples[split_name] = sum([
            stay_counts["total"]
            for stay_counts in dict_subset(subject_counts, split_ids).values()
            if isinstance(stay_counts, dict)
        ])
    # Assert ratios are respected
    # Assert no duplicte subjects
    assert not set().intersection(*split_subjects.values())
    if not reduced_set:
        assert sum([
            len(getattr(split_reader, split_name).subject_ids)
            for split_name in split_reader.split_names
        ]) == len(subject_ids)
    check_split_sizes(split_samples, test_size, val_size, tolerance)
    return split_subjects


def check_split_sizes(split_samples: dict, test_size: float, val_size: float, tolerance: float):
    total_samples = sum(split_samples.values())
    assert abs(split_samples["train"] / total_samples - (1 - test_size - val_size)) < tolerance

    if test_size:
        assert abs(split_samples["test"] / total_samples - test_size) < tolerance

    if val_size:
        assert abs(split_samples["val"] / total_samples - val_size) < tolerance


def check_hetero_attributes(demographic_filter: dict, subject_info_df: pd.DataFrame,
                            subject_ids: List[int]):

    for attribute in demographic_filter:
        attribute_data = subject_info_df[attribute]
        if is_numerical(attribute_data.to_frame()):
            check_numerical_attribute(attribute=attribute,
                                      demographic_filter=demographic_filter,
                                      subject_info_df=subject_info_df,
                                      subject_ids=subject_ids)
        else:
            check_categorical_attribute(attribute=attribute,
                                        demographic_filter=demographic_filter,
                                        subject_info_df=subject_info_df,
                                        subject_ids=subject_ids)


def check_categorical_attribute(attribute: str, demographic_filter: dict,
                                subject_info_df: pd.DataFrame, subject_ids: List[int]):
    selected_attributes = subject_info_df[subject_info_df["SUBJECT_ID"].isin(
        subject_ids)][attribute]
    # Ensure all entries are from choices. Subjects with stays where the attribute is from selected and not selected categories
    # are not included in the demographic.
    assert not set(selected_attributes) - set(demographic_filter[attribute]["choice"])


def check_numerical_attribute(attribute: str, demographic_filter: dict,
                              subject_info_df: pd.DataFrame, subject_ids: List[int]):
    # Make the split
    selected_attributes = subject_info_df[subject_info_df["SUBJECT_ID"].isin(
        subject_ids)][attribute]
    assert_range(selected_attributes, demographic_filter[attribute])


def assert_range(column, demographic_filter, invert=False):
    """
    Asserts that all or not any elements in the column meet the specified range conditions.
    
    Parameters:
        column (iterable): The input series or array-like structure to check.
        demographic_filter (dict): A dictionary containing range conditions like 'less', 'leq', 'greater', and 'geq'.
        invert (bool): If True, use 'not any' logic; otherwise, use 'all' logic.
    """

    def range_condition(val):
        # Define a function that checks the conditions specified in demographic_filter
        if "less" in demographic_filter:
            if "greater" in demographic_filter:
                return (val < demographic_filter["less"]) and (val > demographic_filter["greater"])
            elif "geq" in demographic_filter:
                return (val < demographic_filter["less"]) and (val >= demographic_filter["geq"])
            else:
                return val < demographic_filter["less"]
        elif "leq" in demographic_filter:
            if "greater" in demographic_filter:
                return (val <= demographic_filter["leq"]) and (val > demographic_filter["greater"])
            elif "geq" in demographic_filter:
                return (val <= demographic_filter["leq"]) and (val >= demographic_filter["geq"])
            else:
                return val <= demographic_filter["leq"]
        elif "greater" in demographic_filter:
            return val > demographic_filter["greater"]
        elif "geq" in demographic_filter:
            return val >= demographic_filter["geq"]
        return True

    # Apply the appropriate assertion logic
    if invert:
        assert not any(
            range_condition(x)
            for x in column), "Condition failed: Some elements meet the specified range condition"
    else:
        assert all(range_condition(x) for x in column
                  ), "Condition failed: Not all elements meet the specified range condition"


if __name__ == "__main__":
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task='DECOMP')
    # test_demographic_filter("DECOMP", "discretized", {"DECOMP": reader}, {})
    # test_demo_split("DECOMP", "discretized", {"DECOMP": reader}, {})
    # test_demo_and_ratio_split("DECOMP", "discretized", {"DECOMP": reader}, {})

    # for _ in range(10):
    #     test_demo_and_ratio_split("DECOMP", "discretized", {"DECOMP": reader}, {})
    # test_train_size("DECOMP", "discretized", {"DECOMP": reader}, {})

    discretized_readers = dict()
    engineered_readers = dict()
    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)
    for task_name in TASK_NAMES:
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    discretize=True,
                                    time_step_size=1.0,
                                    start_at_zero=True,
                                    impute_strategy='previous',
                                    task=task_name)
        discretized_readers[task_name] = reader
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    engineer=True,
                                    task=task_name)
        engineered_readers[task_name] = reader
        for processing_style in ["discretized", "engineered"]:
            test_demographic_filter(task_name, processing_style, discretized_readers,
                                    engineered_readers)
            test_demo_split(task_name, processing_style, discretized_readers, engineered_readers)
            test_demo_and_ratio_split(task_name, processing_style, discretized_readers,
                                      engineered_readers)
            test_train_size(task_name, processing_style, discretized_readers, engineered_readers)
            test_ratio_split(task_name, processing_style, discretized_readers, engineered_readers)
    pass


### FILE: .\tests\test_datasets\test_discretizer.py ###
import pytest
import datasets
import shutil
import pandas as pd
import numpy as np
from typing import Dict
from pathlib import Path
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from tests.pytest_utils.discretization import assert_strategy_equals, prepare_processed_data
from utils.IO import *
from tests.pytest_utils import copy_dataset
from tests.settings import *
from tests.conftest import discretizer_listfiles


@pytest.mark.parametrize("start_strategy", ["zero", "relative"])
@pytest.mark.parametrize("impute_strategy", ["next", "normal_value", "previous", "zero"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_discretizer(task_name: str, start_strategy: str, impute_strategy: str,
                     discretizer_listfiles: Dict[str, pd.DataFrame],
                     preprocessed_readers: Dict[str, ProcessedSetReader]):

    copy_dataset("extracted")
    listfile = discretizer_listfiles[task_name]
    test_data_dir = Path(TEST_GT_DIR, "discretized", TASK_NAME_MAPPING[task_name])
    test_dir_path = Path(test_data_dir, f"imp{impute_strategy}_start{start_strategy}")

    tests_io(f"Testing discretizer for task {task_name}\n"
             f"Impute strategy '{impute_strategy}' \n"
             f"Start strategy '{start_strategy}'")
    if impute_strategy == "normal_value":
        impute_strategy = "normal"

    prepare_processed_data(task_name, listfile, preprocessed_readers[task_name])

    reader = datasets.load_data(chunksize=75837,
                                source_path=TEST_DATA_DEMO,
                                storage_path=TEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=(start_strategy == "zero"),
                                impute_strategy=impute_strategy,
                                task=task_name)

    X_discretized, _ = reader.read_samples(read_ids=True).values()

    assert_strategy_equals(X_discretized, test_dir_path)

    tests_io("Succeeded in testing!")


if __name__ == "__main__":

    import re
    listfiles = dict()
    # Preparing the listfiles
    for task_name in TASK_NAMES:
        # Path to discretizer sets
        test_data_dir = Path(TEST_GT_DIR, "discretized", TASK_NAME_MAPPING[task_name])
        # Listfile with truth values
        listfile = pd.read_csv(Path(test_data_dir, "listfile.csv")).set_index("stay")
        stay_name_regex = r"(\d+)_episode(\d+)_timeseries\.csv"

        listfile = listfile.reset_index()
        listfile["subject"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(1))
        listfile["icustay"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(2))
        listfile = listfile.set_index("stay")
        listfiles[task_name] = listfile
    readers = dict()
    for task_name in TASK_NAMES:
        # Simulate semi_temp fixture
        reader = datasets.load_data(chunksize=75837,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    reprocess=True,
                                    task=task_name)
        readers[task_name] = reader
        for start_strategy in ["zero", "relative"]:
            for impute_strategy in ["next", "normal_value", "previous", "zero"]:
                if TEMP_DIR.is_dir():
                    shutil.rmtree(str(TEMP_DIR))
                test_discretizer(task_name=task_name,
                                 impute_strategy=impute_strategy,
                                 start_strategy=start_strategy,
                                 discretizer_listfiles=listfiles,
                                 preprocessed_readers=readers)


### FILE: .\tests\test_datasets\test_extracted_data.py ###
import datasets
import shutil
import pandas as pd
from tests.decorators import repeat
from datasets.readers import ExtractedSetReader
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals

top_level_files = ["diagnoses.csv", "icu_history.csv"]


@repeat(2)
def test_iterative_extraction():
    """ Tests the preprocessing handled by the load data method of the dataset module (extracted data).
    """
    tests_io("Test case iterative extraction.", level=0)
    # Extract the data
    test_data_dir = Path(TEST_GT_DIR, "extracted")
    reader: ExtractedSetReader = datasets.load_data(
        chunksize=900000,  # Using a large number to run on single read for comparision
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR)

    compare_diagnoses_and_history(test_data_dir)
    tests_io("Datset creation successfully tested against original code!")
    compare_subject_directories(test_data_dir, reader.read_subjects(read_ids=True))
    tests_io("Dataset restoration successfully tested against original code!")


@repeat(2)
def test_compact_extraction():
    # Extract the data
    tests_io("Test case compact extraction.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "extracted")
    dataset = datasets.load_data(source_path=TEST_DATA_DEMO, storage_path=TEMP_DIR)
    compare_diagnoses_and_history(test_data_dir)
    tests_io("Dataset creation successfully tested against original code!")
    compare_subject_directories(test_data_dir, dataset)
    tests_io("Dataset restoration successfully tested against original code!")


def compare_diagnoses_and_history(test_data_dir: Path):
    # Compare files to ground truth
    for file_name in top_level_files:
        file_settings = TEST_SETTINGS[file_name]
        test_file_name = file_settings["name_mapping"]
        tests_io(f"Test {file_name}")
        generated_df = pd.read_csv(Path(TEMP_DIR, "extracted", file_name))
        test_df = pd.read_csv(Path(test_data_dir, test_file_name))
        if "columns" in file_settings:
            generated_df = generated_df[file_settings["columns"]]
        assert_dataframe_equals(generated_df, test_df, normalize_by="groundtruth")


def compare_subject_directories(test_data_dir: Path, dataset: dict):
    generated_dirs = [
        directory for directory in Path(TEMP_DIR, "extracted").iterdir()
        if directory.is_dir() and directory.name.isnumeric()
    ]
    assert len(dataset)

    tests_io("subject_events.csv: 0 subject\n"
             "subject_diagnoses.csv: 0 subject\n"
             "timeseries.csv: 0 subject, 0 stays\n"
             "episodic_data.csv: 0 subject")

    counts = {
        "subject_events.csv": 0,
        "subject_diagnoses.csv": 0,
        "timeseries_stay_id.csv": 0,
        "episodic_data.csv": 0
    }

    ts_counts = 0

    # Compare the subject directories
    for directory in generated_dirs:
        stay_ids = list()

        # File equivalences:
        # Generated: Ground truth
        # subject_events.csv: events.csv
        # subject_diagnoses.csv: diagnoses.csv
        # timeseries_stay_id.csv': episodestay_id_timeseries.csv
        # episodic_data.csv: episodestay_id.csv

        for file_name, file_settings in TEST_SETTINGS.items():
            if file_name in top_level_files:  # already checked
                continue
            test_file_name = file_settings["name_mapping"]
            # Stay ID from early files
            if "stay_id" in file_name or "stay_id" in test_file_name:
                # Episodic data and timeseries are stored per stay_id in ground truth
                for stay_id in stay_ids:
                    # Insert the stay id
                    stay_file_name = file_name.replace("stay_id", str(int(stay_id)))
                    test_stay_file_name = test_file_name.replace("stay_id", str(int(stay_id)))
                    # Read the generated df
                    if stay_file_name == "episodic_data.csv":
                        continue
                        # Incorrect in original repository. Remove continue if issue is resolved
                        # https://github.com/YerevaNN/mimic3-benchmarks/issues/101
                        # Episodic data stored together in this implementation
                        generated_df = generated_df[generated_df["Icustay"] == stay_id].reset_index(
                            drop=True)

                    generated_df = dataset[int(directory.name)][str(Path(file_name).stem).replace(
                        "_stay_id", "")][stay_id].reset_index()

                    # Read the test df
                    test_df = pd.read_csv(Path(test_data_dir, directory.name, test_stay_file_name))
                    assert_dataframe_equals(generated_df, test_df, rename=file_settings["rename"])

                if file_name == "timeseries_stay_id.csv":
                    counts["timeseries_stay_id.csv"] += len(stay_ids)
                    ts_counts += 1
                elif file_name == "episodic_data.csv":
                    counts[
                        "episodic_data.csv"] += 0  # Incorrect in original repository. Remove 0 if issue is resolved
                tests_io(
                    f"subject_events.csv: {counts['subject_events.csv']} subjects\n"
                    f"subject_diagnoses.csv: {counts['subject_diagnoses.csv']} subjects\n"
                    f"timeseries.csv: {ts_counts} subjects, {counts['timeseries_stay_id.csv']} stays\n"
                    f"episodic_data.csv: {counts[file_name]} subjects",
                    flush_block=True)
            else:
                generated_df = dataset[int(directory.name)][str(Path(file_name).stem)]
                if file_name == "subject_events.csv":
                    stay_ids = generated_df["ICUSTAY_ID"].unique()

                test_df = pd.read_csv(Path(test_data_dir, directory.name, test_file_name))
                assert_dataframe_equals(generated_df, test_df, rename=file_settings["rename"])
                counts[file_name] += 1
                tests_io(
                    f"subject_events.csv: {counts['subject_events.csv']} subjects\n"
                    f"subject_diagnoses.csv: {counts['subject_diagnoses.csv']} subjects\n"
                    f"timeseries.csv: {ts_counts} subjects, {counts['timeseries_stay_id.csv']} stays\n"
                    f"episodic_data.csv: {counts[file_name]} subjects",
                    flush_block=True)


if __name__ == "__main__":

    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)
    test_iterative_extraction()
    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)
    test_compact_extraction()
    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)


### FILE: .\tests\test_datasets\test_feature_engine.py ###
import datasets
import pytest
import pandas as pd
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils import copy_dataset
from tests.pytest_utils.general import assert_dataframe_equals
from tests.decorators import repeat
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset

compare_mode_map = {
    "IHM": "single_entry",
    "LOS": "multiline",
    "PHENO": "single_entry",
    "DECOMP": "multiline"
}


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_iterative_engineer_task(task_name: str):
    """ Tests the feature engineering for in hospital mortality, done for linear linear models.
    Preprocessing tests need to be completed before running this test.
    """

    tests_io(f"Test case iterative engineering for task {task_name}", level=0)

    generated_path = Path(TEMP_DIR, "engineered", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    tests_io(f"Engineering data for task {task_name}.")
    reader = datasets.load_data(chunksize=75837,
                                source_path=TEST_DATA_DEMO,
                                storage_path=TEMP_DIR,
                                engineer=True,
                                task=generated_path.name)

    # Concatenate engineered samples into labeleod data frame
    generated_samples = reader.read_samples(read_ids=True)
    generated_df = concatenate_dataset(generated_samples["X"])

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    generated_df = generated_df.sort_values(by=generated_df.columns.to_list())
    generated_df = generated_df.reset_index(drop=True)
    test_df = test_df.sort_values(by=generated_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)
    assert_dataframe_equals(generated_df,
                            test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")

    tests_io("Testing against ground truth data.")
    tests_io(f"Total stays checked: {generated_df['SUBJECT_ID'].nunique()}\n"
             f"Total subjects checked: {generated_df['ICUSTAY_ID'].nunique()}\n"
             f"Total samples checked: {len(generated_df)}")

    tests_io(f"{task_name} feature engineering successfully tested against original code!")

    return


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_compact_engineer_task(task_name: str):
    """ Tests the feature engineering for in hospital mortality, done for linear linear models.
    Preprocessing tests need to be completed before running this test.
    """

    tests_io(f"Test case compact engineering for task {task_name}", level=0)

    task_name_mapping = dict(zip(TASK_NAMES, FTASK_NAMES))

    generated_path = Path(TEMP_DIR, "engineered", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         task_name_mapping[task_name])  # Ground truth data dir

    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    # Preprocess the data
    tests_io(f"Engineering data for task {task_name}.")
    dataset = datasets.load_data(source_path=TEST_DATA_DEMO,
                                 storage_path=TEMP_DIR,
                                 engineer=True,
                                 task=generated_path.name)

    # Concatenate engineered samples into labeled data frame
    generated_df = concatenate_dataset(dataset["X"])

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    generated_df = generated_df.sort_values(by=generated_df.columns.to_list())
    generated_df = generated_df.reset_index(drop=True)
    test_df = test_df.sort_values(by=generated_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)
    assert_dataframe_equals(generated_df,
                            test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")
    tests_io("Testing against ground truth data.")
    tests_io(f"Total stays checked: {generated_df['SUBJECT_ID'].nunique()}\n"
             f"Total subjects checked: {generated_df['ICUSTAY_ID'].nunique()}\n"
             f"Total samples checked: {len(generated_df)}")

    tests_io(f"{task_name} feature engineering successfully tested against original code!")

    return


if __name__ == "__main__":
    import shutil
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    for task in TASK_NAMES:
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_compact_engineer_task(task)
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_iterative_engineer_task(task)


### FILE: .\tests\test_datasets\test_preprocessor.py ###
import datasets
import pandas as pd
import re
import pytest
from pathlib import Path
from datasets.readers import ExtractedSetReader
from utils.IO import *
from tests.pytest_utils import copy_dataset
from tests.decorators import repeat
from tests.settings import *
from tests.pytest_utils.preprocessing import assert_reader_equals, assert_dataset_equals

kwargs = {
    "IHM": {
        "minimum_length_of_stay": IHM_SETTINGS['label_start_time']
    },
    "DECOMP": {
        "label_start_time": DECOMP_SETTINGS['label_start_time']
    },
    "LOS": {
        "label_start_time": LOS_SETTINGS['label_start_time']
    },
    "PHENO": {}
}


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_iterative_processing_task(task_name: str):
    """_summary_

    Args:
        task_name (str): _description_
    """
    tests_io(f"Test iterative preprocessor for task {task_name}", level=0)

    # Resolve task name
    generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    copy_dataset("extracted")

    # Load
    reader = datasets.load_data(chunksize=75835,
                                source_path=TEST_DATA_DEMO,
                                storage_path=TEMP_DIR,
                                preprocess=True,
                                task=generated_path.name)

    assert_file_creation(reader.root_path, test_data_dir, **kwargs[generated_path.name])

    tests_io(f"All {task_name} files have been created as expected")
    # Compare the dataframes in the directory
    assert_reader_equals(reader, test_data_dir)
    tests_io(f"{task_name} preprocessing successfully tested against original code!")

    return


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_compact_processing_task(task_name: str):
    """_summary_

    Args:
        task_name (str): _description_
    """
    tests_io(f"Test compact preprocessor for task {task_name}", level=0)

    # Resolve task name
    generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    copy_dataset("extracted")

    # Load/Create data
    dataset = datasets.load_data(source_path=TEST_DATA_DEMO,
                                 storage_path=TEMP_DIR,
                                 preprocess=True,
                                 task=generated_path.name)

    assert_file_creation(generated_path, test_data_dir, **kwargs[generated_path.name])

    tests_io(f"All {task_name} files have been created as expected")
    # Compare the dataframes in the directory
    assert_dataset_equals(dataset["X"], dataset["y"], generated_path, test_data_dir)
    tests_io(f"{task_name} preprocessing successfully tested against original code!")

    return


def assert_file_creation(root_path: Path,
                         test_data_dir: Path,
                         label_start_time: float = None,
                         minimum_length_of_stay: float = None):
    """_summary_

    Args:
        reader (SampleSetReader): _description_
        test_data_dir (Path): _description_
    """
    count = 0
    removed_count = 0
    subject_ids_checked = list()
    tests_io(f"Total stays checked: {count}\n"
             f"Total subjects checked: {len(set(subject_ids_checked))}\n"
             f"Subjects removed correctly: {removed_count}")

    assert root_path.is_dir(), f"Generated directory {root_path} does not exist"
    assert test_data_dir.is_dir(), f"Test directory {test_data_dir} does not exist"
    # Test wether all files have been created correctly
    for file_path in Path(test_data_dir).iterdir():
        if file_path.name == "listfile.csv":
            continue
        match = re.search(r"(\d+)_episode(\d+)_timeseries\.csv", file_path.name)
        subject_id = int(match.group(1))
        stay_id = int(match.group(2))
        # Files that are not longer than the minimum that needs to be elapsed before label creation are removed
        test_data = pd.read_csv(Path(test_data_dir,
                                     f"{subject_id}_episode{stay_id}_timeseries.csv"))
        # Increment for logging
        count += 1
        subject_ids_checked.append(subject_id)

        if label_start_time is not None:
            if label_start_time > test_data["Hours"].max():
                assert not Path(root_path, str(subject_id), f"X_{stay_id}.csv").is_file(
                ), f"Sample file X_{stay_id}.csv for subject {subject_id} should be deleted due to minimum label start time."
                assert not Path(root_path, str(subject_id), f"y_{stay_id}.csv").is_file(
                ), f"Label file y_{stay_id}.csv for subject {subject_id} should be deleted due to minimum label start time."
                removed_count += 1
                continue

        if minimum_length_of_stay is not None:
            test_episode_data = pd.read_csv(
                Path(test_data_dir.parent.parent, "extracted", str(subject_id),
                     f"episode{stay_id}.csv"))
            if test_episode_data["Length of Stay"][0] > minimum_length_of_stay:
                assert not Path(root_path, str(subject_id), f"X_{stay_id}.csv").is_file(
                ), f"Sample file X_{stay_id}.csv for subject {subject_id} should be deleted due to minimum length of stay."
                assert not Path(root_path, str(subject_id), f"y_{stay_id}.csv").is_file(
                ), f"Label file y_{stay_id}.csv for subject {subject_id} should be deleted due to minimum length of stay."
                removed_count += 1
                continue

        assert Path(root_path, str(subject_id), f"X_{stay_id}.csv").is_file(
        ), f"Missing sample file X_{stay_id}.csv for subject {subject_id}"
        assert Path(root_path, str(subject_id), f"y_{stay_id}.csv").is_file(
        ), f"Missing label file y_{stay_id}.csv for subject {subject_id}"
        tests_io(
            f"Total stays checked: {count}\n"
            f"Total subjects checked: {len(set(subject_ids_checked))}\n"
            f"Subjects removed correctly: {removed_count}",
            flush_block=True)


if __name__ == "__main__":
    import shutil
    for task in TASK_NAMES:
        if Path(TEMP_DIR).is_dir():
            shutil.rmtree(str(Path(TEMP_DIR)))
        test_compact_processing_task(task)
        if Path(TEMP_DIR).is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_iterative_processing_task(task)


### FILE: .\tests\test_datasets\__init__.py ###


### FILE: .\tests\test_datasets\test_options\test_chunksize_option.py ###
import datasets
import os
import pandas as pd
import json
import shutil
import pytest
from itertools import chain
from typing import Dict
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals
from tests.pytest_utils.preprocessing import assert_reader_equals
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset

from datasets.readers import ExtractedSetReader, ProcessedSetReader
'''
def test_path_options():
    datasets.load_data(chunksize=5000000,
                       source_path=TEST_DATA_DEMO,
                       storage_path=TEMP_DIR,
                       preprocess=True,
                       task=generated_path.name)


def test_chunksize_option():
    ...
'''

if __name__ == "__main__":
    # test_num_samples_options()
    # extraction_reader = datasets.load_data(chunksize=75835,
    #                                        source_path=TEST_DATA_DEMO,
    #                                        preprocess=True,
    #                                        task="IHM",
    #                                        storage_path=SEMITEMP_DIR)

    # stest_num_subjects_extraction_only(extraction_reader)
    # test_num_subjects_preprocessing_only("IHM")
    if Path(TEMP_DIR, "engineered").is_dir():
        shutil.rmtree(str(Path(TEMP_DIR, "engineered")))
    test_num_subjects_engineer_only("IHM")


### FILE: .\tests\test_datasets\test_options\test_num_subjects.py ###
import datasets
import pandas as pd
import shutil
import pytest
from tests.pytest_utils import copy_dataset
from typing import Dict
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals
from tests.pytest_utils.preprocessing import assert_reader_equals, assert_dataset_equals
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset
from tests.pytest_utils.extraction import compare_extracted_datasets
from datasets.readers import ExtractedSetReader, ProcessedSetReader


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
def test_num_subjects_extraction(extracted_reader: ExtractedSetReader, extraction_style: str):
    # Test only extraction
    tests_io(f"Test case num subjects for extraction", level=0)

    # Test on existing directory
    for num_subjects in [1, 11, 21]:
        tests_io(f"-> Testing extract-only with {num_subjects} subjects on existing directory.")
        extract_and_compare(num_subjects=num_subjects,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    extracted_dir = Path(TEMP_DIR, "extracted")
    for num_subjects in [11, 21]:
        # Remove existing dir
        if extracted_dir.is_dir():
            shutil.rmtree(str(extracted_dir))
        # Compare extraction
        tests_io(f"-> Testing extract-only with {num_subjects} subjects on empty directory.")
        extract_and_compare(num_subjects=num_subjects,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    extract_and_compare(num_subjects=1,
                        extraction_style=extraction_style,
                        extracted_reader=extracted_reader)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_preprocessing_only(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for preprocessing-only for task {task_name}.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Make sure extracted data is available
    tests_io(f"Preparing fully extracted directory")
    copy_dataset("extracted")

    # Test on existing directory
    for num_subjects in [1, 11, 21]:
        tests_io(f"-> Testing preprocessing-only with {num_subjects}"
                 " subjects on existing directory.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    processed_dir = Path(TEMP_DIR, "processed")
    for num_subjects in [11, 21]:
        if processed_dir.is_dir():
            shutil.rmtree(str(processed_dir))
        tests_io(f"-> Testing preprocessing-only with {num_subjects}"
                 " subjects on empty directory.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    process_and_compare(num_subjects=1,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_engineer_only(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for engineering-only for task {task_name}.", level=0)
    # Test only engineering
    tests_io(f"Preparing fully preprocessed directory")
    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Test on existing directory
    for num_subjects in [1, 11, 16]:
        tests_io(f"-> Testing engineer-only with {num_subjects} subjects on existing directory.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    engineered_dir = Path(TEMP_DIR, "engineered")
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(engineered_dir)
        tests_io(f"-> Testing engineer-only with {num_subjects} subjects on empty directory.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    engineer_and_compare(num_subjects=1,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_process(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for preprocessing from scratch for task {task_name}.",
             level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Test on existing directory
    for num_subjects in [1, 6, 11, 16, 21]:
        tests_io(
            f"-> Testing preprocessing {num_subjects} subjects on existing directory from scratch.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    for num_subjects in [6, 11, 16, 21]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(Path(TEMP_DIR)))
        tests_io(
            f"-> Testing preprocessing {num_subjects} subjects on empty director from scratch.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    process_and_compare(num_subjects=1,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_engineer(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for engineering from scratch for task {task_name}.", level=0)

    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Test on existing directory
    for num_subjects in [1, 11, 16]:
        tests_io(
            f"-> Testing engineering {num_subjects} subjects on existing directory from scratch.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(TEMP_DIR)
        tests_io(
            f"-> Testing preprocessing {num_subjects} subjects on empty directory from scratch.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    engineer_and_compare(num_subjects=1,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


def extract_and_compare(num_subjects: int, extraction_style: str,
                        extracted_reader: ExtractedSetReader):
    return_entity = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        num_subjects=num_subjects,
        storage_path=TEMP_DIR,
        extract=True)
    if extraction_style == "iterative":
        reader: ExtractedSetReader = return_entity
        assert len(reader.subject_ids) == num_subjects
        subject_ids = reader.subject_ids
        generated_dataset = reader.read_subjects(reader.subject_ids, read_ids=True)
    else:
        generated_dataset: dict = return_entity
        assert len(generated_dataset) == num_subjects
        subject_ids = list(generated_dataset.keys())
    test_dataset = extracted_reader.read_subjects(subject_ids, read_ids=True)

    compare_extracted_datasets(generated_dataset, test_dataset)


def process_and_compare(num_subjects: int, task_name: str, extraction_style: str,
                        test_data_dir: Path):
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        num_subjects=num_subjects,
        preprocess=True,
        task=task_name)

    if extraction_style == "iterative":
        reader = return_entity
        assert len(reader.subject_ids) == num_subjects
        assert_reader_equals(reader, test_data_dir)
    else:
        generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
        test_data_dir = Path(TEST_GT_DIR, "processed",
                             TASK_NAME_MAPPING[task_name])  # Ground truth data dir
        generated_samples = return_entity
        assert len(generated_samples["X"]) == num_subjects
        X = generated_samples["X"]
        y = generated_samples["y"]
        assert_dataset_equals(X, y, generated_path, test_data_dir)


def engineer_and_compare(num_subjects: int, task_name: str, extraction_style: str, test_df: Path):
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        num_subjects=num_subjects,
        engineer=True,
        task=task_name)
    if extraction_style == "iterative":
        reader = return_entity
        generated_samples = reader.read_samples(read_ids=True)
    else:
        generated_samples = return_entity
    assert len(generated_samples["X"])  # Generated df is None if this doesn't pass
    generated_df = concatenate_dataset(generated_samples["X"])
    # Align unstructured frames
    generated_df = generated_df.sort_values(by=generated_df.columns.to_list())
    generated_df = generated_df.reset_index(drop=True)
    stay_ids = generated_df["ICUSTAY_ID"].unique()

    curr_test_df = test_df[test_df["ICUSTAY_ID"].isin(stay_ids.astype("str").tolist())]
    curr_test_df = curr_test_df.reset_index(drop=True)

    assert generated_df["SUBJECT_ID"].nunique() == num_subjects
    assert_dataframe_equals(generated_df,
                            curr_test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")
    tests_io("Engineered subjects are identical!")


if __name__ == "__main__":
    extraction_reader = datasets.load_data(chunksize=75835,
                                           source_path=TEST_DATA_DEMO,
                                           storage_path=SEMITEMP_DIR)
    for extraction_style in ["compact", "iterative"]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_num_subjects_extraction(extraction_reader, extraction_style)
        for task in TASK_NAMES:
            if not Path(SEMITEMP_DIR, "processed", task).is_dir():
                reader = datasets.load_data(chunksize=75835,
                                            source_path=TEST_DATA_DEMO,
                                            storage_path=SEMITEMP_DIR,
                                            preprocess=True,
                                            task=task)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_preprocessing_only(task, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_engineer_only(task, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_process(task, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_engineer(task, extraction_style)

    tests_io("All tests completed successfully!")


### FILE: .\tests\test_datasets\test_options\test_paths.py ###



### FILE: .\tests\test_datasets\test_options\test_subject_ids.py ###
import datasets
import random
import shutil
import pytest
import re
import pandas as pd
from copy import deepcopy
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils import copy_dataset
from tests.pytest_utils.general import assert_dataframe_equals
from tests.pytest_utils.preprocessing import assert_reader_equals, assert_dataset_equals
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset
from tests.pytest_utils.extraction import compare_extracted_datasets
from datasets.readers import ExtractedSetReader, ProcessedSetReader


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
def test_subject_ids_extraction(extracted_reader: ExtractedSetReader, subject_ids: list,
                                extraction_style: str):
    # Comapring subject event dfs for correctniss is costly due to unorderedness
    # These are subjects with a very low subject event count
    tests_io(f"Test case subject ids for extraction", level=0)

    # Retrive subject ids
    subjects = deepcopy(subject_ids)
    curr_subjects = list()
    # Sample subjects and test result by extending the subject id list
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))

        # Compare the extracted data with the test data
        tests_io(f"-> Testing extract-only with {len(curr_subjects)} subjects on empty directory.\n"
                 f"Subject IDs: {*curr_subjects,}")

        extract_and_compare(subject_ids=curr_subjects,
                            test_subject_ids=extracted_reader.subject_ids,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing extending subject id.")

    # Sample subject from scratch and extract into empty dir
    extracted_dir = Path(TEMP_DIR, "extracted")
    for num_subjects in [11, 21]:
        # Remove the extracted directory
        if extracted_dir.is_dir():
            shutil.rmtree(str(extracted_dir))
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the extracted data with the test data
        tests_io(f"-> Testing extract-only with {len(curr_subjects)} subjects on empty directory.\n"
                 f"Subject IDs: {*curr_subjects,}")
        extract_and_compare(subject_ids=curr_subjects,
                            test_subject_ids=extracted_reader.subject_ids,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count to a single subject
    curr_subjects = random.sample(subjects, 1)
    extract_and_compare(subject_ids=curr_subjects,
                        test_subject_ids=extracted_reader.subject_ids,
                        extraction_style=extraction_style,
                        extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_preprocessing_only(task_name: str, subject_ids: list, extraction_style: str):
    tests_io(f"Test case subject ids for preprocessing-only for task {task_name}.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Make sure extracted data is available
    tests_io(f"Preparing fully extracted directory")
    copy_dataset("extracted")

    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Sample subjects and test result by extending the subject id list
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on existing directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Sample subject from scratch and extract into empty dir
    processed_dir = Path(TEMP_DIR, "processed")

    for num_subjects in [11, 21]:
        # Remove the processed directory
        if processed_dir.is_dir():
            shutil.rmtree(str(processed_dir))
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on empty directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    process_and_compare(subject_ids=curr_subjects,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_engineer_only(task_name: str, subject_ids: list, extraction_style: str):
    tests_io(f"Test case subject ids for engineering-only for task {task_name}.", level=0)

    # Test only engineering
    tests_io(f"Preparing fully preprocessed directory")
    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))
    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Test on existing directory
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(
            f"-> Testing engineer-only with {len(curr_subjects)} subjects on existing directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    engineered_dir = Path(TEMP_DIR, "engineered")
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(engineered_dir)
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing engineer-only with {len(curr_subjects)} subjects on empty directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    engineer_and_compare(subject_ids=curr_subjects,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_preprocessing(task_name: str, subject_ids: list, extraction_style: str):
    # Test preprocessing from scratch, that means no fully extracted data in temp beforehand
    tests_io(f"Test case subject ids for preprocessing from scratch for task {task_name}.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Make sure extracted data is available
    tests_io(f"Preparing fully extracted directory")

    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Sample subjects and test result by extending the subject id list
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on existing directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Sample subject from scratch and extract into empty dir
    processed_dir = Path(TEMP_DIR, "processed")

    for num_subjects in [11, 21]:
        # Remove the processed directory
        if processed_dir.is_dir():
            shutil.rmtree(str(processed_dir))
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on empty directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    process_and_compare(subject_ids=curr_subjects,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_engineer(task_name: str, subject_ids: list, extraction_style: str):
    tests_io(f"Test case subject ids for engineering from scratch for task {task_name}.", level=0)
    # Test engineering from scratch, that means no fully processed data in temp
    tests_io(f"Preparing fully preprocessed directory")
    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Test on existing directory
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(
            f"-> Testing engineer-only with {len(curr_subjects)} subjects on existing directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    engineered_dir = Path(TEMP_DIR, "engineered")
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(engineered_dir)
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing engineer-only with {len(curr_subjects)} subjects on empty directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    engineer_and_compare(subject_ids=curr_subjects,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


def extract_and_compare(subject_ids: list, test_subject_ids: list, extraction_style: str,
                        extracted_reader: ExtractedSetReader):
    return_entity: ExtractedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        subject_ids=subject_ids,
        storage_path=TEMP_DIR,
        extract=True)
    if extraction_style == "iterative":
        reader = return_entity
        # Some ids will not be processed due to no in unit subject events
        assert len(reader.subject_ids)
        missing_ids = list(set(subject_ids) - set(reader.subject_ids))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(reader.subject_ids) - set(subject_ids))
        assert not additional_ids
        generated_dataset = reader.read_subjects(reader.subject_ids, read_ids=True)
    else:
        generated_dataset = return_entity
        assert len(generated_dataset)
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_dataset.keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(generated_dataset.keys()) - set(subject_ids))
        assert not additional_ids
    tests_io(f"-> The following IDs have not been extracted: {*missing_ids,}")
    test_dataset = extracted_reader.read_subjects(subject_ids, read_ids=True)
    compare_extracted_datasets(generated_dataset, test_dataset)


def process_and_compare(subject_ids: list, task_name: str, extraction_style: str,
                        test_data_dir: Path):
    # Process the dataset
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        subject_ids=subject_ids,
        preprocess=True,
        task=task_name)

    listfile_df = pd.read_csv(Path(test_data_dir, "listfile.csv"))
    regex = r"(\d+)_episode(\d+)_timeseries\.csv"
    test_subject_ids = listfile_df.apply(lambda x: re.search(regex, x["stay"]).group(1), axis=1)
    test_subject_ids = test_subject_ids.astype(int)
    test_subject_ids = test_subject_ids.unique()

    if extraction_style == "iterative":
        reader = return_entity
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(reader.subject_ids))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(reader.subject_ids) - set(subject_ids))
        assert not additional_ids
        if reader.subject_ids:
            assert_reader_equals(reader, test_data_dir)

    else:
        generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
        test_data_dir = Path(TEST_GT_DIR, "processed",
                             TASK_NAME_MAPPING[task_name])  # Ground truth data dir
        generated_samples = return_entity
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_samples["X"].keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(generated_samples["X"].keys()) - set(subject_ids))
        assert not additional_ids
        X = generated_samples["X"]
        y = generated_samples["y"]
        if len(X):
            assert_dataset_equals(X, y, generated_path, test_data_dir)


def engineer_and_compare(subject_ids: list, task_name: str, extraction_style: str, test_df: Path):
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        subject_ids=subject_ids,
        engineer=True,
        task=task_name)
    if extraction_style == "iterative":
        reader = return_entity
        generated_samples = reader.read_samples(read_ids=True)
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_samples["X"].keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_df["SUBJECT_ID"].unique())
        additional_ids = list(set(generated_samples["X"].keys()) - set(subject_ids))
        assert not additional_ids
    else:
        generated_samples = return_entity
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_samples["X"].keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_df["SUBJECT_ID"].unique())
        additional_ids = list(set(generated_samples["X"].keys()) - set(subject_ids))
        assert not additional_ids
    generated_df = concatenate_dataset(generated_samples["X"])

    # Align unstructured frames
    if generated_df is None:
        tests_io(f"-> No data generated for {task_name} task.")
        return
    columns = generated_df.columns.to_list()

    generated_samples = return_entity

    generated_df = generated_df.sort_values(by=columns)
    generated_df = generated_df.reset_index(drop=True)
    stay_ids = generated_df["ICUSTAY_ID"].unique()

    curr_test_df = test_df[test_df["ICUSTAY_ID"].isin(stay_ids.astype("str").tolist())]
    curr_test_df = curr_test_df.sort_values(by=columns)
    curr_test_df = curr_test_df.reset_index(drop=True)

    assert_dataframe_equals(generated_df,
                            curr_test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")


if __name__ == "__main__":
    extraction_reader = datasets.load_data(chunksize=75835,
                                           source_path=TEST_DATA_DEMO,
                                           storage_path=SEMITEMP_DIR)
    icu_history = extraction_reader._read_csv("icu_history.csv")
    subjects = icu_history["SUBJECT_ID"].astype(int).unique().tolist()
    for extraction_style in ["iterative"]:  #"compact", "iterative"]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_subject_ids_extraction(extraction_reader, subjects, extraction_style)
        for task in TASK_NAMES:
            if not Path(SEMITEMP_DIR, "processed", task).is_dir():
                reader = datasets.load_data(chunksize=75835,
                                            source_path=TEST_DATA_DEMO,
                                            storage_path=SEMITEMP_DIR,
                                            preprocess=True,
                                            task=task)

            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_preprocessing_only(task, subjects, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_engineer_only(task, subjects, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_preprocessing(task, subjects, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_engineer(task, subjects, extraction_style)


### FILE: .\tests\test_datasets\test_readers\test_event_reader.py ###
import shutil
import numpy as np
import pandas as pd
import datasets
from time import sleep
from pathlib import Path
from datasets.trackers import ExtractionTracker
from datasets.readers import EventReader
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals
from utils.IO import *

DTYPES = DATASET_SETTINGS["subject_events"]["dtype"]

columns = ["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID", "VALUE", "VALUEUOM"]


def assert_dtypes(dataframe: pd.DataFrame):
    assert all([
        dataframe.dtypes[column] == "object"
        if dtype == "str" else dataframe.dtypes[column] == dtype  # Might be translated to obj
        for column, dtype in DTYPES.items()
        if column in dataframe
    ])
    return True


def test_get_full_chunk():
    tests_io("Test case getting full chunk", level=0)
    tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    event_reader = EventReader(chunksize=900000, dataset_folder=TEST_DATA_DEMO, tracker=tracker)

    samples, frame_lengths = event_reader.get_chunk()
    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == len(frame)
        assert len(frame) == frame_lengths[csv_name]
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)
    tests_io("Test case getting full chunk succeeded")


def test_get_mutlitple_chunks():
    tests_io("Test case getting multiple chunks", level=0)

    tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    event_reader = EventReader(chunksize=5000, dataset_folder=TEST_DATA_DEMO, tracker=tracker)

    samples, frame_lengths = event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[-1] == 4999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    samples, frame_lengths = event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[0] == 5000
        assert frame.index[-1] == 9999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    tests_io("Test case getting multiple chunks succeeded")


def test_resume_get_chunk():
    tests_io("Test case resuming get_chunk", level=0)
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=5000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[0] == 0
        assert frame.index[-1] == 4999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Tracker register the frames as read
    orig_tracker.count_subject_events += frame_lengths

    restored_tracker = ExtractionTracker(TEMP_DIR)
    restored_event_reader = EventReader(chunksize=5000,
                                        dataset_folder=TEST_DATA_DEMO,
                                        tracker=restored_tracker)

    samples, frame_lengths = restored_event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[0] == 5000
        assert frame.index[-1] == 9999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Event producer only read 200 samples then crashed
    orig_tracker.count_subject_events += {
        "CHARTEVENTS.csv": 200,
        "LABEVENTS.csv": 200,
        "OUTPUTEVENTS.csv": 200
    }

    restored_tracker = ExtractionTracker(TEMP_DIR)
    restored_event_reader = EventReader(chunksize=5000,
                                        dataset_folder=TEST_DATA_DEMO,
                                        tracker=restored_tracker)

    # Resume reading after crash from same chunk
    samples, frame_lengths = restored_event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 4800
        assert frame_lengths[csv_name] == 4800
        assert frame.index[0] == 5200
        assert frame.index[-1] == 9999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    tests_io("Test case resuming get_chunk succeeded")


def test_switch_chunk_sizes():
    tests_io("Test case switching chunk sizes", level=0)
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=4000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 4000
        assert frame_lengths[csv_name] == 4000
        assert frame.index[0] == 0
        assert frame.index[-1] == 3999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Crash and resume with half the chunk size
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=2000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 2000
        assert frame_lengths[csv_name] == 2000
        assert frame.index[0] == 4000
        assert frame.index[-1] == 5999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Crash and resume with half the chunk size
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=1000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 1000
        assert frame_lengths[csv_name] == 1000
        assert frame.index[0] == 6000
        assert frame.index[-1] == 6999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Crash and resume with half the chunk size
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=500,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 500
        assert frame_lengths[csv_name] == 500
        assert frame.index[0] == 7000
        assert frame.index[-1] == 7499
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    tests_io("Test case switching chunk sizes succeeded")


def test_subject_ids():
    """Test if the event reader stops on last subject occurence.
    """
    tests_io("Test case with subject ids", level=0)
    tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    subject_ids = ["40124"]  # int version 40124
    event_reader = EventReader(chunksize=1000000,
                               subject_ids=subject_ids,
                               dataset_folder=TEST_DATA_DEMO,
                               tracker=tracker)
    # Takes only a second on my machine
    sleep(2)
    frame_lengths = True
    previous_samples = {}
    test_csv = ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]

    sample_buffer = {"CHARTEVENTS.csv": list(), "LABEVENTS.csv": list(), "OUTPUTEVENTS.csv": list()}

    while frame_lengths:
        samples, frame_lengths = event_reader.get_chunk()
        for csv in frame_lengths:
            if frame_lengths[csv]:
                previous_samples[csv] = samples[csv]
                sample_buffer[csv].append(samples[csv])
    assert sample_buffer
    for csv in previous_samples.keys():
        last_occurence = event_reader._last_occurrence[csv]
        last_sample = previous_samples[csv].index[-1]

        # Test if all events have been read
        assert np.isclose(
            last_occurence, last_sample, atol=1
        ) and last_occurence >= last_sample, f"Last occurence line: {last_occurence} and last sample line: {last_sample}"
        tests_io(
            f"{csv}: Last occurence line: {last_occurence} and last sample line: {last_sample}")
        test_csv.remove(csv)

    all_data = event_reader.get_all()
    all_data = all_data[all_data["SUBJECT_ID"].isin([40124])]
    all_data = all_data.sort_values(
        by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
    all_data = all_data.reset_index(drop=True)
    chunk_data = pd.concat([pd.concat(buffer) for buffer in sample_buffer.values()])
    chunk_data = chunk_data.sort_values(
        by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
    chunk_data = chunk_data.reset_index(drop=True)
    assert_dataframe_equals(chunk_data.astype("object"), all_data.astype("object"))
    assert_dtypes(chunk_data)
    assert_dtypes(all_data)

    assert not test_csv, f"Test csvs not empty: {test_csv}"
    tests_io("Test case with subject ids succeeded")


if __name__ == '__main__':
    # if TEMP_DIR.is_dir():
    #     shutil.rmtree(str(TEMP_DIR))
    import shelve
    reader = datasets.load_data(chunksize=75837, source_path=TEST_DATA_DEMO, storage_path=TEMP_DIR)

    def reset():
        with shelve.open(str(Path(TEMP_DIR, "extracted"))) as db:
            db.clear()

    reset()
    test_subject_ids()
    reset()
    test_get_full_chunk()
    reset()
    test_get_mutlitple_chunks()
    reset()
    test_switch_chunk_sizes()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))


### FILE: .\tests\test_datasets\test_readers\test_extracted_set_reader.py ###
import datasets
import os
import pandas as pd
import re
import shutil
import random
import pytest
from pathlib import Path
from utils.IO import *
from tests.settings import *
from datasets.readers import ExtractedSetReader
from datasets.mimic_utils import convert_dtype_dict

ground_truth_subject_ids = [
    int(subject_dir.name)
    for subject_dir in Path(TEST_GT_DIR, "extracted").iterdir()
    if subject_dir.name.isnumeric()
]

ground_truth_stay_ids = [
    int(icustay) for subject_dir in Path(TEST_GT_DIR, "extracted").iterdir()
    if subject_dir.name.isnumeric()
    for icustay in pd.read_csv(Path(subject_dir, "stays.csv")).ICUSTAY_ID.to_numpy().tolist()
]

# Consider storing this somewhere else
frame_properties = {
    "timeseries": {
        "columns": [
            'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen',
            'Glascow coma scale eye opening', 'Glascow coma scale motor response',
            'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose',
            'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', 'pH',
            'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight'
        ],
        "index": "hours"
    },
    "episodic_data": {
        "columns": ['AGE', 'LOS', 'MORTALITY', 'GENDER', 'ETHNICITY', 'Height', 'Weight'],
        "index": "Icustay"
    },
    "subject_events": {
        "columns": [
            'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM'
        ],
        "index": None
    },
    "subject_diagnoses": {
        "columns": [
            'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE', 'SHORT_TITLE', 'LONG_TITLE',
            'ICUSTAY_ID', 'HCUP_CCS_2015', 'USE_IN_BENCHMARK'
        ],
        "index": None
    },
    "subject_icu_history": {
        "columns": [
            'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'LAST_CAREUNIT', 'DBSOURCE', 'INTIME', 'OUTTIME',
            'LOS', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ETHNICITY', 'DIAGNOSIS', 'GENDER', 'DOB',
            'DOD', 'AGE', 'MORTALITY_INUNIT', 'MORTALITY', 'MORTALITY_INHOSPITAL'
        ],
        "index": None
    }
}

idx_to_file_type = {
    0: "timeseries",
    1: "episodic_data",
    2: "subject_events",
    3: "subject_diagnoses",
    4: "subject_icu_history"
}

FILE_TYPE_KEYS = ("timeseries", "episodic_data", "subject_events", "subject_diagnoses",
                  "subject_icu_history")

DTYPES = {
    file_type: DATASET_SETTINGS[file_index]["dtype"]
    for file_type, file_index in zip(FILE_TYPE_KEYS, [
        "timeseries",
        "episodic_data",
        "subject_events",
        "diagnosis",
        "icu_history",
    ])
}

READER_TO_FILE_TYPE = {
    "read_episodic_data": "episodic_data",
    "read_events": "subject_events",
    "read_diagnoses": "subject_diagnoses",
    "read_icu_history": "subject_icu_history"
}


def test_properties(extracted_reader: ExtractedSetReader):
    # TODO! test dtypes
    tests_io("Test case properties for ExtractedSetReader", level=0)
    reader = extracted_reader
    assert reader.root_path == Path(
        SEMITEMP_DIR, "extracted"
    ), f"Expected root path is {str(Path(SEMITEMP_DIR, 'extracted'))}, but reader root path is {str(reader.root_path)}"

    # Should not be able to set root path
    with pytest.raises(AttributeError) as error:
        reader.root_path = Path("test")
        assert error.info == "can't set attribute"
    tests_io("Root path is correctly set")

    assert reader.subject_ids == ground_truth_subject_ids, f"Subjects are not in the ground truth: {list(set(reader.subject_ids) - set(ground_truth_subject_ids))}\n Subjects are missing from the reader attribute: {list(set(ground_truth_subject_ids) - set(reader.subject_ids))}"

    # Should not be able to set subject ids
    with pytest.raises(AttributeError) as error:
        reader.subject_ids = []
        assert error.info == "can't set attribute"
    tests_io("Subject ids are correct")


def assert_dtypes(dataframe: pd.DataFrame, dtypes: dict):
    assert all([
        dataframe.dtypes[column] == "object"
        if dtype == "str" else dataframe.dtypes[column] == dtype  # Might be translated to obj
        for column, dtype in dtypes.items()
        if column in dataframe
    ])
    return True


def test_read_csv(extracted_reader: ExtractedSetReader):
    # TODO! test dtypes
    tests_io("Test case read csv for ExtractedSetReader", level=0)
    reader = extracted_reader

    # Test dtypes of different bites sizes
    dtype_mapping_template = {
        'ROW_ID': 'Int32',
        'SUBJECT_ID': 'Int64',
        'HADM_ID': 'Int32',
        'SEQ_NUM': 'Int8',
        'ICD9_CODE': 'str',
        'ICUSTAY_ID': 'float'
    }
    dtype_mapping = convert_dtype_dict(dtype_mapping_template, add_lower=False)

    absolute_diagnoses_path = Path(SEMITEMP_DIR, "extracted", "diagnoses.csv")
    absolute_df = reader.read_csv(absolute_diagnoses_path, dtypes=dtype_mapping)
    assert not absolute_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using absolute resolution!"
    assert_dtypes(absolute_df, dtype_mapping)

    relative_df = reader.read_csv("diagnoses.csv", dtypes=dtype_mapping)
    assert not relative_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using relative resolution!"
    assert_dtypes(relative_df, dtype_mapping)

    tests_io("Read CSV working with file name and absolute path")

    # Test dtype too small error
    dtype_mapping_template['ROW_ID'] = 'Int8'
    dtype_mapping = convert_dtype_dict(dtype_mapping_template, add_lower=False)
    with pytest.raises(TypeError) as error:
        absolute_df = reader.read_csv(absolute_diagnoses_path, dtypes=dtype_mapping)

    # Test for file types this is used on
    for file_name in ["episodic_info_df.csv", "icu_history.csv", "subject_info.csv"]:
        absolute_diagnoses_path = Path(SEMITEMP_DIR, "extracted", file_name)
        # Get dtypes
        settings_index_name = file_name.rstrip(".csv").rstrip("_df")
        dtypes = convert_dtype_dict(DATASET_SETTINGS[settings_index_name]["dtype"], add_lower=False)
        # Test absoulte read
        absolute_df = reader.read_csv(absolute_diagnoses_path, dtypes=dtypes)
        assert not absolute_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using absolute resolution!"
        assert_dtypes(absolute_df, dtypes)
        # Test relative read
        relative_df = reader.read_csv(file_name, dtypes=dtypes)
        assert not relative_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using relative resolution!"
        assert_dtypes(relative_df, dtypes)
        tests_io(f"Read CSV working correctly with {file_name}")
    tests_io("Read CSV tested successfully")


def test_read_timeseries(extracted_reader: ExtractedSetReader):
    tests_io("Test case read timeseries for ExtractedSetReader", level=0)
    reader = extracted_reader

    # --- test correct structure ---
    data = reader.read_timeseries(read_ids=True)
    # Make sure all subjects have stays dict
    assert all([isinstance(stays_dict, dict) for _, stays_dict in data.items()])

    # Make sure all indices are integers
    assert all([
        isinstance(subj_id, int) and isinstance(stay_id, int)
        for subj_id, stays_dict in data.items()
        for stay_id, _ in stays_dict.items()
    ])

    # Make sure all stays are frames
    assert all([
        isinstance(frame, pd.DataFrame)
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    # Ensure dtype correcteness
    assert all([
        assert_dtypes(frame, DTYPES["timeseries"])
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    # Make sure all frames are read
    assert all(
        [not frame.empty for _, stays_dict in data.items() for _, frame in stays_dict.items()])
    tests_io("Correct structure of timeseries data with ids")

    # --- test correct num subjects ---
    data = reader.read_timeseries(read_ids=True, num_subjects=10)

    # Make sure all indices are integers
    assert all([
        isinstance(subj_id, int) and isinstance(stay_id, int)
        for subj_id, stays_dict in data.items()
        for stay_id, _ in stays_dict.items()
    ])
    assert [isinstance(stays_dict, dict) for _, stays_dict in data.items()]
    # Make sure all stays are frames
    assert all([
        isinstance(frame, pd.DataFrame)
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    # Assert no empty frames
    assert all(
        [not frame.empty for _, stays_dict in data.items() for _, frame in stays_dict.items()])
    assert len(data) == 10
    assert all([
        assert_dtypes(frame, DTYPES["timeseries"])
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    tests_io("Correct num subjects when sepcified for timeseries data")

    data = reader.read_timeseries()
    assert len(data) == len(ground_truth_stay_ids)
    assert all([isinstance(frame, pd.DataFrame) for frame in data])
    assert all([not frame.empty for frame in data])
    assert all([assert_dtypes(frame, DTYPES["timeseries"]) for frame in data])
    tests_io("Correct dimension of timeseries data")
    tests_io("Timeseries read tested successfully")


@pytest.mark.parametrize(
    "reader_name", ["read_episodic_data", "read_events", "read_diagnoses", "read_icu_history"])
def test_read_remaining_file_types(reader_name: str, extracted_reader: ExtractedSetReader):
    tests_io(f"Test case {reader_name} for ExtractedSetReader", level=0)
    reader_method = getattr(extracted_reader, reader_name)
    file_dtypes = DTYPES[READER_TO_FILE_TYPE[reader_name]]

    # --- test correct structure ---
    data = reader_method(read_ids=True)
    assert all([
        isinstance(subj_id, int)
        for subj_id, stays_dict in data.items()
        if isinstance(stays_dict, dict)
    ])
    # Make sure all subjects have stays dict
    assert [isinstance(stays_dict, pd.DataFrame) for _, stays_dict in data.items()]
    # Make sure all stays are frames
    assert all([not frame.empty for _, frame in data.items()])
    assert all([assert_dtypes(frame, file_dtypes) for frame in data.values()])
    tests_io(f"Correct structure of {reader_name} data with ids")

    # --- test correct num subjects ---
    data = reader_method(read_ids=True, num_subjects=10)
    assert [isinstance(stays_dict, pd.DataFrame) for _, stays_dict in data.items()]
    assert all([not frame.empty for _, frame in data.items()])
    assert all([assert_dtypes(frame, file_dtypes) for frame in data.values()])
    assert len(data) == 10
    tests_io(f"Correct num subjects when sepcified for {reader_name} data")

    data = reader_method()
    assert len(data) == len(ground_truth_subject_ids)
    assert all([not frame.empty for frame in data])
    assert all([assert_dtypes(frame, file_dtypes) for frame in data])
    tests_io(f"Correct dimension of {reader_name} data")
    tests_io(f"{reader_name} read tested successfully")


def test_read_subjects(extracted_reader: ExtractedSetReader):
    tests_io("Test case read subjects for ExtractedSetReader", level=0)

    reader = extracted_reader
    data_with_ids = reader.read_subjects(read_ids=True)

    # Make sure no subjects missing or additional
    gt_subject_ids = [
        int(directory.name)
        for directory in Path(TEST_GT_DIR, "extracted").iterdir()
        if directory.name.isnumeric()
    ]
    assert not set(gt_subject_ids) - set(data_with_ids.keys()) and not set(
        data_with_ids.keys()) - set(gt_subject_ids)
    tests_io("Correct dimension of dataset relative to subject ids")

    # Make sure no stays missing or additional
    gt_stay_ids = [
        re.findall('[0-9]+', file.name).pop()
        for directory in Path(TEST_GT_DIR, "extracted").iterdir() if directory.name.isnumeric()
        for file in directory.iterdir() if re.findall('[0-9]+', file.name)
    ]
    extracted_stay_ids = [
        str(stay_id)
        for subject_data in data_with_ids.values()
        for stay_id in subject_data["timeseries"].keys()
    ]
    assert not set(gt_stay_ids) - set(extracted_stay_ids) and \
           not set(extracted_stay_ids) - set(gt_stay_ids)
    tests_io("Correct dimension of dataset relative to stay ids")

    # Make sure the correct columns and indices are read for every file:
    for subject_data in data_with_ids.values():
        validate_subject_data(subject_data, file_type_keys=True, read_ids=True)

    data_without_ids = reader.read_subjects()
    assert len(data_without_ids) == len(gt_subject_ids)
    assert sum([len(subject_data["timeseries"]) for subject_data in data_without_ids
               ]) == len(extracted_stay_ids)

    for subject_data in data_without_ids:
        validate_subject_data(subject_data, file_type_keys=True, read_ids=False)

    data_without_keys = reader.read_subjects(file_type_keys=False)
    assert len(data_without_keys) == len(gt_subject_ids)
    assert sum([len(subject_data[0]) for subject_data in data_without_keys
               ]) == len(extracted_stay_ids)

    for subject_data in data_without_keys:
        validate_subject_data(subject_data, file_type_keys=False, read_ids=False)
    tests_io("Correct structure of dataset relative to file type keys")

    # Test the num samples option
    data_with_num_subjects = reader.read_subjects(read_ids=True, num_subjects=10)
    assert len(data_with_num_subjects) == 10
    for subject_data in data_with_num_subjects.values():
        validate_subject_data(subject_data, file_type_keys=True, read_ids=True)
    tests_io("Correct number of subjects when specified")

    # Test the subject ids option
    data_with_subject_ids = reader.read_subjects(read_ids=True,
                                                 subject_ids=["10006", "10011", "10036", "10088"])
    assert len(data_with_subject_ids) == 4
    assert list(data_with_subject_ids.keys()) == [10006, 10011, 10036, 10088]
    for subject_data in data_with_subject_ids.values():
        validate_subject_data(subject_data, file_type_keys=True, read_ids=True)
    tests_io("Correct subjects when specified")
    tests_io("Read subjects tested successfully")


def test_read_subjects_dir(extracted_reader: ExtractedSetReader):
    tests_io("Test case read subjects ExtractedSetReader", level=0)
    reader = extracted_reader
    ## With ids
    # Absolute
    data_with_ids = reader.read_subject(Path(reader.root_path, "10019"), read_ids=True)
    validate_subject_data(data_with_ids, file_type_keys=True, read_ids=True)
    # Relative
    data_with_ids = reader.read_subject("10019", read_ids=True)
    validate_subject_data(data_with_ids, file_type_keys=True, read_ids=True)
    tests_io("Correct relative and absolute resolution of subject data with ids")

    ## Without ids
    # Absolute
    data_without_ids = reader.read_subject(Path(reader.root_path, "10019"))
    validate_subject_data(data_without_ids, file_type_keys=True, read_ids=False)
    # Relative
    data_without_ids = reader.read_subject("10019")
    validate_subject_data(data_without_ids, file_type_keys=True, read_ids=False)
    tests_io("Correct relative and absolute resolution of subject data without ids")

    ## Without ids without file type keys
    # Absolute
    data_without_keys = reader.read_subject(Path(reader.root_path, "10019"), file_type_keys=False)
    validate_subject_data(data_without_keys, file_type_keys=False, read_ids=False)
    # Relative
    data_without_keys = reader.read_subject(Path(reader.root_path, "10019"), file_type_keys=False)
    validate_subject_data(data_without_keys, file_type_keys=False, read_ids=False)
    tests_io("Correct relative and absolute resolution of "
             "subject data without ids and file type keys")

    file_types = ("episodic_data", "subject_events", "subject_diagnoses", "subject_icu_history",
                  "timeseries")
    with pytest.raises(ValueError) as error:
        reader.read_subject(Path(reader.root_path, "10019"), file_types=("episodic_data"))
        assert error.value == f'file_types must be a tuple but is {type("")}'

    for _ in range(10):
        random_filetypes = random.sample(file_types, random.randint(1, 5))
        data = reader.read_subject("10019", file_types=random_filetypes, read_ids=True)
        validate_subject_data(data, file_type_keys=True, read_ids=True)

        data = reader.read_subject("10019", file_types=random_filetypes, read_ids=False)
        validate_subject_data(data, file_type_keys=True, read_ids=False)

        file_type_map = dict(enumerate(list(data.keys())))
        data = reader.read_subject("10019",
                                   file_types=random_filetypes,
                                   read_ids=False,
                                   file_type_keys=False)
        validate_subject_data(data,
                              file_type_keys=False,
                              read_ids=False,
                              file_type_mapping=file_type_map)
    tests_io("Correct file types subsampling when specified")
    tests_io("Read subjects tested successfully")


def validate_subject_data(subject_data,
                          file_type_keys,
                          read_ids,
                          file_type_mapping=idx_to_file_type):
    assert subject_data, "Subject data is empty!"
    # Make sure all file types are present in the data by checking their column names
    for idx, frame in subject_data.items() if file_type_keys else enumerate(subject_data):
        if not file_type_keys:
            idx = file_type_mapping[idx]
        if isinstance(frame, pd.DataFrame):
            filtered_colnames = [
                col for col in frame.columns if not col.isnumeric() and not col[1:].isnumeric()
            ]
            assert_dtypes(frame, DTYPES[idx])
            assert filtered_colnames == frame_properties[idx]["columns"]
            assert frame.index.name == frame_properties[idx]["index"]
        else:
            for value in frame.values() if read_ids else frame:
                filtered_colnames = [
                    col for col in value.columns if not col.isnumeric() and not col[1:].isnumeric()
                ]
                assert_dtypes(value, DTYPES[idx])
                assert filtered_colnames == frame_properties[idx]["columns"]
                assert value.index.name == frame_properties[idx]["index"]
    return


if __name__ == "__main__":
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    reader = datasets.load_data(chunksize=75835,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                task="PHENO")
    test_properties(reader)
    test_read_csv(reader)
    test_read_timeseries(reader)
    for reader_mname in ["read_episodic_data", "read_events", "read_diagnoses", "read_icu_history"]:
        test_read_remaining_file_types(reader_mname, reader)
    test_read_subjects_dir(reader)
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))


### FILE: .\tests\test_datasets\test_readers\test_processed_set_reader.py ###
import datasets
import pytest
import pandas as pd
import numpy as np
from copy import deepcopy
from utils.IO import *
from tests.settings import *
from datasets.readers import ProcessedSetReader
from datasets.mimic_utils import upper_case_column_names

timeseries_label_cols = {
    "IHM":
        "Y",
    "DECOMP":
        "Y",
    "LOS":
        "Y",
    "PHENO": [
        'y0', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12', 'y13',
        'y14', 'y15', 'y16', 'y17', 'y18', 'y19', 'y20', 'y21', 'y22', 'y23', 'y24'
    ]
}

DTYPES = {
    column_name.upper(): np.float64 if dtype == "float64" else object
    for column_name, dtype in DATASET_SETTINGS["timeseries"]["dtype"].items()
}


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_read_sample(task_name: str, preprocessed_readers: ProcessedSetReader):
    tests_io(f"Test case read sample for task {task_name}", level=0)
    reader = preprocessed_readers[task_name]

    # 10017: Single stay
    # 40124: Multiple stays

    # Testing without reading ids and timestamps
    check_sample(reader.read_sample(10017),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_sample(40124),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(f"Suceeded testing read sample for task {task_name} without ids and timestamps passed")

    # Testing with reading ids and without timestamps
    check_sample(reader.read_sample(40124, read_ids=True),
                 read_ids=True,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_sample(10017, read_ids=True),
                 read_ids=True,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(
        f"Suceeded testing read sample for task {task_name} with ids and without timestamps passed")

    # Testing with reading ids and timestamps
    check_sample(reader.read_sample(10017, read_ids=True, read_timestamps=True),
                 read_ids=True,
                 read_timestamps=True,
                 task_name=task_name)
    check_sample(reader.read_sample(10017, read_ids=True, read_timestamps=True),
                 read_ids=True,
                 read_timestamps=True,
                 task_name=task_name)
    tests_io(f"Suceeded testing read sample for task {task_name} with ids and timestamps passed")

    # Testing convert to numpy on read
    check_sample(reader.read_sample(40124, data_type=np.ndarray),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=np.ndarray,
                 task_name=task_name)

    check_sample(reader.read_sample(40124, data_type=pd.DataFrame),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=pd.DataFrame,
                 task_name=task_name)
    tests_io(f"Suceeded testing read sample for task {task_name} with numpy conversion passed")


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_read_samples(task_name: str, preprocessed_readers: ProcessedSetReader):
    tests_io(f"Test case read samples for task {task_name}", level=0)
    reader = preprocessed_readers[task_name]
    # Testing without reading ids and timestamps
    check_sample(reader.read_samples([10017, 40124]),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_samples([10017]),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_samples(reader.subject_ids),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(
        f"Suceeded testing read samples for task {task_name} without ids and timestamps passed")

    # Testing with reading ids but without timestamps
    check_samples(reader.read_samples([10017, 40124], read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    check_samples(reader.read_samples([10017], read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    check_samples(reader.read_samples(reader.subject_ids, read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    tests_io(
        f"Suceeded testing read samples for task {task_name} with ids and without timestamps passed"
    )

    # Testing with reading ids and timestamps
    check_samples(reader.read_samples([10017, 40124], read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.read_samples([10017], read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.read_samples(reader.subject_ids, read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    tests_io(f"Suceeded testing read samples for task {task_name} with ids and timestamps passed")

    # Testing convert to numpy on read
    check_sample(reader.read_samples([40124, 10017], data_type=np.ndarray),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=np.ndarray,
                 task_name=task_name)

    check_sample(reader.read_samples([40124, 10017], data_type=pd.DataFrame),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=pd.DataFrame,
                 task_name=task_name)
    tests_io(f"Suceeded testing read samples for task {task_name} with numpy conversion passed")


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_random_sample(task_name: str, preprocessed_readers: ProcessedSetReader):
    tests_io(f"Test case random samples for task {task_name}", level=0)
    reader = preprocessed_readers[task_name]
    # Testing without reading ids and timestamps
    check_sample(reader.random_samples(10),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.random_samples(),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.random_samples(n_samples=10),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(
        f"Suceeded testing random samples for task {task_name} without ids and timestamps passed")

    # Testing with reading ids but without timestamps
    check_samples(reader.random_samples(10, read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    check_samples(reader.random_samples(read_ids=True), read_timestamps=False, task_name=task_name)
    check_samples(reader.random_samples(n_samples=10, read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    tests_io(
        f"Suceeded testing random samples for task {task_name} with ids and without timestamps passed"
    )

    # Testing with reading ids and timestamps
    check_samples(reader.random_samples(10, read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.random_samples(read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.random_samples(n_samples=10, read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    tests_io(f"Suceeded testing random samples for task {task_name} with ids and timestamps passed")

    # Test without replacement property
    samples = reader.random_samples(len(reader.subject_ids), read_ids=True)
    check_samples(samples=samples, read_timestamps=False, task_name=task_name)
    assert len(set(samples["X"].keys())) == len(samples["X"].keys())
    assert len(set(samples["X"].keys())) == len(reader.subject_ids)

    # Exceeding the set size results in warning and set sized sample collection
    samples = reader.random_samples(2 * len(reader.subject_ids), read_ids=True)
    check_samples(samples, read_timestamps=False, task_name=task_name)
    assert len(set(samples["X"].keys())) == len(samples["X"].keys())
    assert len(set(samples["X"].keys())) == len(reader.subject_ids)
    tests_io(f"Suceeded testing random samples for task {task_name} without replacement passed")


def check_samples(samples: dict, read_timestamps: bool, data_type=None, task_name=None):
    assert isinstance(samples, dict)
    assert set(samples.keys()) == set(["X", "y", "t"] if read_timestamps else ["X", "y"])
    for subject_id in samples["X"]:
        assert isinstance(subject_id, int)
        sample = {"X": samples["X"][subject_id], "y": samples["y"][subject_id]}
        if read_timestamps:
            sample.update({"t": samples["t"][subject_id]})
        check_sample(sample,
                     read_ids=True,
                     read_timestamps=read_timestamps,
                     data_type=data_type,
                     task_name=task_name)


def check_sample(sample: dict,
                 read_ids: bool,
                 read_timestamps: bool,
                 data_type=None,
                 task_name=None):
    assert set(sample.keys()) == set(["X", "y", "t"] if read_timestamps else ["X", "y"])
    X, Y = sample["X"], sample["y"]

    if read_ids:
        assert isinstance(X, dict)
        assert isinstance(Y, dict)
        assert len(X) == len(Y)
        assert all([
            isinstance(stay_id, int) and
            (isinstance(timeseries, np.ndarray) if data_type == np.ndarray else isinstance(
                timeseries, pd.DataFrame)) for stay_id, timeseries in X.items()
        ])
    else:
        assert isinstance(X, list)
        assert isinstance(Y, list)
        assert len(X) == len(Y)
        assert all([(isinstance(timeseries, np.ndarray) if data_type == np.ndarray else isinstance(
            timeseries, pd.DataFrame)) for timeseries in X])
    if data_type == pd.DataFrame or data_type is None:
        for X_sample, Y_sample in (zip(X.values(), Y.values()) if read_ids else zip(X, Y)):
            X_sample = upper_case_column_names(deepcopy(X_sample))

            assert set([
                column.upper() for column in DATASET_SETTINGS["timeseries"]["dtype"].keys()
            ]) == set(X_sample)
            assert set([column.upper() for column in timeseries_label_cols[task_name]
                       ]) == set(Y_sample.columns.str.upper())

            assert all([X_sample[column].dtype == DTYPES[column] for column in X_sample.columns])
    else:
        for X_sample, Y_sample in (zip(X.values(), Y.values()) if read_ids else zip(X, Y)):
            assert isinstance(X_sample, np.ndarray)
            assert isinstance(Y_sample, np.ndarray)
            assert X_sample.shape[1] == len(DATASET_SETTINGS["timeseries"]["dtype"])
            assert Y_sample.shape[1] == len(timeseries_label_cols[task_name])


if __name__ == "__main__":
    import shutil
    # if SEMITEMP_DIR.is_dir():
    #     shutil.rmtree(str(SEMITEMP_DIR))
    reader_dict = dict()
    for task in TASK_NAMES:
        reader = datasets.load_data(chunksize=75835,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    preprocess=True,
                                    task=task)
        reader_dict[task] = reader
    for task in TASK_NAMES:
        test_read_sample(task, reader_dict)
        test_read_samples(task, reader_dict)
        test_random_sample(task, reader_dict)

    print("All tests passed!")
    # if TEMP_DIR.is_dir():
    #     shutil.rmtree(str(TEMP_DIR))


### FILE: .\tests\test_datasets\test_readers\__init__.py ###


### FILE: .\tests\test_datasets\test_trackers.py\test_extraction_tracker.py ###
import pytest
import shutil
from datasets.trackers import ExtractionTracker
from pathlib import Path
from utils.IO import *
from tests.settings import *

TRACKER_STATE = {
    "count_subject_events": {
        "OUTPUTEVENTS.csv": 0,
        "LABEVENTS.csv": 0,
        "CHARTEVENTS.csv": 0
    },
    "has_episodic_data": False,
    "has_timeseries": False,
    "subject_ids": list(),
    "has_subject_events": False,
    "count_total_samples": 0,
    "has_icu_history": False,
    "has_diagnoses": False,
    "has_bysubject_info": False,
    "is_finished": False,
    "num_samples": None
}

EVENT_BOOLS = [
    "has_episodic_data", "has_subject_events", "has_timeseries", "has_bysubject_info", "is_finished"
]


def test_extraction_tracker_basics():
    tests_io("Test case basic capabilities of ExtractionTracker.", level=0)
    # Create an instance of ExtractionTracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)

    # Test correct initialization
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        assert getattr(tracker, attribute) == value

    tests_io("Succeeded testing initialization.")
    # Assignment bools and nums
    for attribute, value in TRACKER_STATE.items():
        # These states influence the state of the tracker from within the ExtractionTracker __init__
        # and are therefore not part of basic funcitonalities
        if attribute not in ["count_subject_events", "subject_ids", "num_samples", "num_subjects"]:
            setattr(tracker, attribute,
                    True if isinstance(getattr(tracker, attribute), bool) else 10)

    tracker.count_subject_events = {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }

    # Test correct assignment
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in ["count_subject_events", "subject_ids", "num_samples", "num_subjects"]:
            assert getattr(
                tracker,
                attribute) == (True if isinstance(getattr(tracker, attribute), bool) else 10)
            assert tracker._progress[attribute] == (True if isinstance(
                getattr(tracker, attribute), bool) else 10)

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }
    tests_io("Succeeded testing assignment.")

    # Test correct restoration after assignment
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in ["count_subject_events", "subject_ids", "num_samples", "num_subjects"]:
            assert getattr(
                tracker,
                attribute) == (True if isinstance(getattr(tracker, attribute), bool) else 10)
            assert tracker._progress[attribute] == (True if isinstance(
                getattr(tracker, attribute), bool) else 10)

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }
    tests_io("Succeeded testing restoration.")

    # Test correct __iadd__ implementation
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if isinstance(getattr(tracker, attribute), (int, float)):
            setattr(tracker, attribute,
                    getattr(tracker, attribute) +
                    10)  #This is how iadd is perfomed for simple properties

    tracker.count_subject_events += {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }

    for attribute, value in TRACKER_STATE.items():
        if isinstance(attribute, (int, float)):
            assert getattr(tracker, attribute) == 20
            assert tracker._progress[attribute] == 20

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 20,
        "LABEVENTS.csv": 20,
        "CHARTEVENTS.csv": 20
    }

    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)

    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if isinstance(attribute, (int, float)):
            assert getattr(tracker, attribute) == 20

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 20,
        "LABEVENTS.csv": 20,
        "CHARTEVENTS.csv": 20
    }
    tests_io("Succeeded testing __iadd__ implementation.")


def test_num_samples_option():
    # Test the logic of increasing and decreasing num sapmles upon reinstantiation
    tests_io("Test case sample target option of ExtractionTracker.", level=0)
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=10)

    assert tracker.num_samples == 10
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute != "num_samples":
            assert getattr(tracker, attribute) == value

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.count_total_samples = 10

    # Test decreasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=5)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples"]:
            assert getattr(tracker, attribute) == value
    # Asser total samples unchanged
    assert tracker.count_total_samples == 10

    # Assert still done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == True

    tests_io("Succeeded testing sample target reduction")
    # Test increasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=15)
    # Asser total samples unchanged
    assert tracker.count_total_samples == 10

    # Assert not done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False

    # Check set correctly
    assert tracker.num_samples == 15

    tests_io("Succeeded testing sample target increase")

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.count_total_samples = 15
    # Test setting sample_target to None
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples"]:
            assert getattr(tracker, attribute) == value

    # Check extraction done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False

    # Check event read not done
    assert tracker.num_samples is None
    tests_io("Succeeded testing sample target set to None")


def test_num_subjects_option():
    # Test the logic of increasing and decreasing num sapmles upon reinstantiation
    tests_io("Test case sample target option of ExtractionTracker.", level=0)
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=10)
    # Check init
    assert tracker.num_subjects == 10
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute != "num_subjects":
            assert getattr(tracker, attribute) == value

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.subject_ids.extend(list(range(10)))

    # Test decreasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=5)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.subject_ids == list(range(10))
    # Assert done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == True

    tests_io("Succeeded testing num subject reduction")

    # Test increasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=15)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.subject_ids == list(range(10))
    # Assert done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False
    tracker.subject_ids.extend(list(range(10, 15)))

    tests_io("Succeeded testing num subject increase")

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)

    # Test setting sample_target to None
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=None)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value

    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False
    # Assert subject ids unchanged
    assert tracker.subject_ids == list(range(15))

    tests_io("Succeeded testing num subject set to None")


def test_subject_ids_option():
    # Test the logic of passing subject ids to the tracker
    # If all ids have already been extracted is_finished should be set to True
    # If some ids have not yet been extracted is_finished should be set to False
    # The tracker does not know all subject ids in advance, so it cannot provide
    # the list of still to be processed subjects
    tests_io("Test case subject ids option of ExtractionTracker.", level=0)

    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"),
                                subject_ids=list(range(10)))
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute != "subject_ids":
            assert getattr(tracker, attribute) == value

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.count_total_samples = 10
    tracker.subject_ids.extend(list(range(10)))

    # Test decreasing the number of subjects upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), subject_ids=list(range(5)))
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.count_total_samples == 10
    assert tracker.subject_ids == list(range(10))
    # Assert done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == True

    tests_io("Succeeded testing subject ids reduction")
    # Test increasing the number of subjects upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"),
                                subject_ids=list(range(15)))
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.count_total_samples == 10
    assert tracker.subject_ids == list(range(10))
    # Assert not done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False

    tracker.subject_ids.extend(list(range(10, 15)))
    tracker.count_total_samples += 5
    assert tracker.subject_ids == list(range(15))

    # Test setting subject_ids to None
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), subject_ids=None)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.count_total_samples == 15
    assert tracker.subject_ids == list(range(15))

    # Assert not done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False


if __name__ == "__main__":
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_subject_ids_option()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_num_subjects_option()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_extraction_tracker_basics()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_num_samples_option()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    print("All tests passed!")


### FILE: .\tests\test_datasets\test_trackers.py\test_processing_tracker.py ###
# TODO! subject ids may be string or numbers this might have some effect on the tracker
import shutil
from pathlib import Path
from datasets.trackers import PreprocessingTracker
from utils.IO import *
from tests.settings import *

tracker_state = {"subjects": {}, "finished": False, "num_subjects": None}


def test_processing_tracker_basics():
    tests_io("Test case basic capabilities of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)

    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Assignment attriubutes
    tracker.subjects = {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4
        }
    }
    assert tracker._progress["subjects"] == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    # Test correct assignment
    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing initialization.")

    # Test correct restoration after assignment
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    assert tracker.num_subjects == None
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    tests_io("Succeeded testing restoration.")

    # Test custom update implementation
    tracker.subjects.update({"subject_3": {"stay_1": 3, "stay_2": 6}})
    assert tracker._progress["subjects"] == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 18
    }
    tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 18
    }
    tracker.subjects.update({"subject_1": {"stay_3": 3}})
    assert tracker._progress["subjects"] == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "stay_3": 3,
            "total": 6
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 21
    }
    tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "stay_3": 3,
            "total": 6
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 21
    }
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)

    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "stay_3": 3,
            "total": 6
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 21
    }
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)
    tests_io("Succeeded testing custom __iadd__ implementation.")


def test_finishing_mechanism():
    tests_io("Test case finishing mechanism of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)

    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Simulate processing of two subjects
    tracker.subjects.update({
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4
        }
    })
    # Make sure subjects are set
    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    # Finish tracker
    tracker.is_finished = True
    assert tracker.is_finished == True
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    for key, stays in tracker.subjects.items():
        if key == "total":
            continue
        # Make sure total length is set per subject
        assert stays["total"] == sum([count for key, count in stays.items() if key != "total"])
    tests_io("Succeeded testing total length creation.")

    # Test correct restoration after finishing
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    assert tracker.is_finished == True
    for key, stays in tracker.subjects.items():
        if key == "total":
            continue
        # Make sure total length is set per subject
        assert stays["total"] == sum([count for key, count in stays.items() if key != "total"])
    tests_io("Succeeded testing restoration after finishing.")


def test_num_subject_option():
    tests_io("Test case num_subjects option of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=2,
                                   subject_ids=None)

    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "num_subjects":
            assert getattr(tracker, attribute) == 2
        elif attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Simulate processing of two subjects
    tracker.subjects.update({
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
        }
    })
    tracker.is_finished = True

    # Test decrease of num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=1,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to True
    assert tracker.num_subjects == 2
    assert tracker.is_finished == True
    assert len(tracker.subjects) - 1 == 2
    assert tracker.subjects["total"] == 9
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing decrease of num_subjects.")

    # Test increase to original num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=2,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to True
    assert tracker.num_subjects == 2
    assert tracker.is_finished == True
    assert len(tracker.subjects) - 1 == 2
    assert tracker.subjects["total"] == 9
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing increase to original num_subjects.")

    # Test increase above original num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=3,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to True
    assert tracker.num_subjects == 3
    assert tracker.is_finished == False
    assert len(tracker.subjects) - 1 == 2
    assert tracker.subjects["total"] == 9
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing increase above original num_subjects.")

    # Simulate processing of one more subject
    tracker.subjects.update({"subject_3": {"stay_1": 3, "stay_2": 6}})
    tracker.is_finished = True
    assert tracker.num_subjects == 3
    assert tracker.is_finished == True
    assert len(tracker.subjects) - 1 == 3
    assert tracker.subjects["subject_3"]["total"] == 9
    assert tracker.subjects["total"] == 18
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)
    # Test switch to None
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to False
    assert tracker.num_subjects == None
    assert tracker.finished == False
    assert len(tracker.subjects) - 1 == 3
    assert tracker.subjects["total"] == 18
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)
    tests_io("Succeeded testing switch to None.")


def test_subject_ids_option():
    tests_io("Test case subject_ids option of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Simulate processing of two subjects
    tracker.subjects.update({
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4
        }
    })
    tracker.is_finished = True

    # Test truthy subject_ids
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=["subject_1", "subject_2"])

    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # All necessary subjects are processed
    assert tracker.is_finished == True
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    # Test decrease of num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=["subject_1"])
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    assert tracker.is_finished == True
    tests_io("Succeeded testing truthy subject_ids.")

    # Test falsey subject_ids
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=["subject_1", "subject_2", "subject_3"])
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    assert tracker.is_finished == False
    tests_io("Succeeded testing falsey subject_ids.")


if __name__ == "__main__":
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_processing_tracker_basics()
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_finishing_mechanism()
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_num_subject_option()
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_subject_ids_option()
    tests_io("Succeeded testing PreprocessingTracker.")


### FILE: .\tests\test_datasets\test_trackers.py\test_storage_decorator.py ###
import pytest
import shutil
from pathlib import Path
from utils.IO import *
from tests.settings import *
from storable import storable


@storable
class TestClass:
    num_samples: int = 0
    time_elapsed: float = 0.0
    finished: bool = False
    subjects: dict = {"a": 0, "b": 0}
    names: dict = {"a": "a", "b": "b"}


@storable
class CountTestClass:
    num_samples: int = 0
    time_elapsed: float = 0.0
    finished: bool = False
    subject_ids: list = list()
    subjects: dict = {"a": 0, "b": 0}
    _store_total: bool = True


# Test the storable
def test_storable_basics():
    tests_io("Test case basic capabilities of Storable.", level=0)

    # Test the default values
    assert TestClass.num_samples == 0
    assert TestClass.time_elapsed == 0.0
    assert TestClass.finished == False
    assert TestClass.subjects == {"a": 0, "b": 0}
    assert TestClass.names == {"a": "a", "b": "b"}

    # Test the correct recreation of its attributes with the correct types
    check_dtypes(TestClass)
    tests_io("Succeeded testing initialization.")

    # Test the storage path
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert Path(TEMP_DIR, "progress.dat").is_file()

    # Test assignment
    test_instance.num_samples = 10
    test_instance.time_elapsed = 1.0
    test_instance.finished = True
    test_instance.subjects = {"a": 1, "b": 2}
    test_instance.names = {"a": "b", "b": "a"}

    assert test_instance.num_samples == 10
    assert test_instance.time_elapsed == 1.0
    assert test_instance.finished == True
    assert test_instance.subjects == {"a": 1, "b": 2}
    assert test_instance.names == {"a": "b", "b": "a"}

    check_dtypes(test_instance)
    tests_io("Succeeded testing assignment.")

    # Test restorable assignment
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert test_instance.num_samples == 10
    assert test_instance.time_elapsed == 1.0
    assert test_instance.finished == True
    assert test_instance.subjects == {"a": 1, "b": 2}
    assert test_instance.names == {"a": "b", "b": "a"}

    check_dtypes(test_instance)

    tests_io("Succeeded testing restoration.")

    # Test __iadd__
    test_instance.num_samples += 10
    test_instance.time_elapsed += 1.1

    assert test_instance.num_samples == 20
    assert test_instance.time_elapsed == 2.1

    check_dtypes(test_instance)

    # Test restorable __iadd__
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert test_instance.num_samples == 20
    assert test_instance.time_elapsed == 2.1
    check_dtypes(test_instance)

    tests_io("Succeeded testing __iadd__.")


def test_dictionary_iadd():
    tests_io("Test case dictionary iadd.", level=0)
    test_instance = TestClass(Path(TEMP_DIR, "progress"))
    test_instance.subjects == {"a": 0, "b": 0}

    test_instance.subjects += {"a": 1, "b": 2}
    assert test_instance.subjects == {"a": 1, "b": 2}

    test_instance.subjects += {"a": 1}
    assert test_instance.subjects == {"a": 2, "b": 2}

    tests_io("Succeeded testing numerical dictionary iadd.")

    # Test restorable dictionary iadd
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert test_instance.subjects == {"a": 2, "b": 2}
    tests_io("Succeeded testing restoration of numerical dictionary iadd.")


def test_dictionary_update():
    tests_io("Test case dictionary update.", level=0)
    test_instance = TestClass(Path(TEMP_DIR, "progress"))
    test_instance.names = {"a": {"a": 1, "b": 1}, "b": {"a": 2, "b": 2}}
    test_instance.names.update({"a": {"c": 1, "d": 1}, "c": {"a": 1, "b": 1}})
    assert test_instance.names == {
        "a": {
            "a": 1,
            "b": 1,
            "c": 1,
            "d": 1
        },
        "b": {
            "a": 2,
            "b": 2
        },
        "c": {
            "a": 1,
            "b": 1
        }
    }
    tests_io("Succeeded testing dictionary update.")
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.names == {
        "a": {
            "a": 1,
            "b": 1,
            "c": 1,
            "d": 1
        },
        "b": {
            "a": 2,
            "b": 2
        },
        "c": {
            "a": 1,
            "b": 1
        }
    }
    tests_io("Succeeded testing restoration of dictionary update.")


def test_total_count():
    # Only implemented for single nestation and not for iadd
    tests_io("Test case total count for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    test_instance.subjects = {"a": {"a": 1, "b": 2}, "b": {"a": 2, "b": 4}}
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "total": 9
    }
    tests_io("Succeeded dictionary assignment total count.")
    test_instance.subjects.update({"a": {"c": 3}})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "c": 3,
            "total": 6
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "total": 12
    }
    tests_io("Succeeded dictionary nested update total count.")

    test_instance.subjects = {"a": 0, "b": 0}
    assert test_instance.subjects == {"a": 0, "b": 0, "total": 0}
    tests_io("Succeeded dictionary reassignment total count.")

    test_instance.subjects = {}
    assert test_instance.subjects == {"total": 0}
    tests_io("Succeeded dictionary empty reassignment total count.")

    test_instance.subjects.update({"a": 1, "b": 2})
    assert test_instance.subjects == {"a": 1, "b": 2, "total": 3}
    tests_io("Succeeded dictionary simple data type overwrite total count.")

    test_instance.subjects.update({"a": 1, "b": 1})
    assert test_instance.subjects == {"a": 1, "b": 1, "total": 2}

    test_instance.subjects.update({"a": 2, "b": 2})
    assert test_instance.subjects == {"a": 2, "b": 2, "total": 4}

    test_instance.subjects.update({"c": 1, "d": 1})
    assert test_instance.subjects == {"a": 2, "b": 2, "c": 1, "d": 1, "total": 6}

    test_instance.subjects.update({"a": {"a": 1, "b": 2}, "b": {"a": 2, "b": 4}})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "c": 1,
        "d": 1,
        "total": 11
    }
    tests_io("Succeeded dictionary complex data type overwrite total count.")
    test_instance.subjects.update({"c": 3})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "c": 3,
        "d": 1,
        "total": 13
    }

    test_instance.subjects.update({"d": {"a": 1, "b": 1}})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "c": 3,
        "d": {
            "a": 1,
            "b": 1,
            "total": 2
        },
        "total": 14
    }


def test_list_basic():
    tests_io("Test case basic list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subjects"] == ["a", "b", "c"]

    tests_io("Succeeded testing basic list assignment.")
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subjects"] == ["a", "b", "c"]
    tests_io("Succeeded testing restoration of basic list assignment.")
    # We are tempering with the cls so need to make sure nothing is permanent
    del test_instance
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == []
    assert test_instance._progress["subjects"] == []
    tests_io("Succeeded testing list non permanency on database deletion.")


def test_list_extend():
    tests_io("Test case extend list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    test_instance.subject_ids.extend(["a", "b", "c"])
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]
    tests_io("Succeeded testing extend list assignment.")

    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]
    tests_io("Succeeded testing restoration of extend list assignment.")

    test_instance.subject_ids.extend(["d", "e", "f"])
    assert test_instance.subject_ids == ["a", "b", "c", "d", "e", "f"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d", "e", "f"]
    tests_io("Succeeded testing extend list assignment.")

    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c", "d", "e", "f"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d", "e", "f"]
    tests_io("Succeeded testing restoration of extend list assignment.")


def test_list_pop():
    tests_io("Test case pop list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    # Assign and make sure
    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Now pop
    popper = test_instance.subject_ids.pop()
    assert popper == "c"
    assert test_instance.subject_ids == ["a", "b"]
    assert test_instance._progress["subject_ids"] == ["a", "b"]
    tests_io("Succeeded testing pop list assignment.")
    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b"]
    assert test_instance._progress["subject_ids"] == ["a", "b"]
    tests_io("Succeeded testing restoration of pop list assignment.")


def test_list_remove():
    tests_io("Test case remove list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    # Assign and make sure
    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Now remove
    test_instance.subject_ids.remove("b")
    assert test_instance.subject_ids == ["a", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "c"]
    tests_io("Succeeded testing remove list assignment.")

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "c"]
    tests_io("Succeeded testing restoration of remove list assignment.")


def test_list_append():
    tests_io("Test case append list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    # Assign and make sure
    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Now append
    test_instance.subject_ids.append("d")
    assert test_instance.subject_ids == ["a", "b", "c", "d"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d"]
    tests_io("Succeeded testing append list assignment.")

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c", "d"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d"]
    tests_io("Succeeded testing restoration of append list assignment.")


def check_dtypes(instance):
    assert isinstance(instance.num_samples, int)
    assert isinstance(instance.time_elapsed, float)
    assert isinstance(instance.finished, bool)
    assert isinstance(instance.subjects, dict)
    assert isinstance(instance.names, dict)


if __name__ == "__main__":
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_extend()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_append()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_pop()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_remove()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_total_count()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    TEMP_DIR.mkdir(exist_ok=True, parents=True)
    test_storable_basics()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    TEMP_DIR.mkdir(exist_ok=True, parents=True)
    test_dictionary_iadd()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    TEMP_DIR.mkdir(exist_ok=True, parents=True)
    test_dictionary_update()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))

    tests_io("All tests passed")


### FILE: .\tests\test_pipelines\test_regression_pipeline.py ###
'''
import datasets
import os
import pandas as pd
import json
import shutil
from pathlib import Path
from utils.IO import *
from settings import *
from datasets.readers import ProcessedSetReader
import numpy as np
from pipelines.nn import MIMICPipeline as MIMICNNPipeline
from pipelines.regression import MIMICPipeline as MIMICRegPipeline
from model.sklearn.standard.linear_models import StandardLogReg
from model.tf2.logistic_regression import IncrementalLogReg

settings = json.load(Path(os.getenv("CONFIG"), "test.json").open())


def test_regression_pipeline_ihm():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "ihm")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="IHM")
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': 'in_hospital_mortality'
    }
    pipeline = MIMICRegPipeline(StandardLogReg('in_hospital_mortality'), **pipeline_config)
    pipeline.fit(data_path=data_path)

    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': 'in_hospital_mortality'
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg('in_hospital_mortality'), **pipeline_config)
    pipeline.fit(data_path=data_path)


def test_regression_pipeline_phenotyping():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "PHENO")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="PHENO")
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ["auc_roc_micro", "auc_roc_macro"],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': "PHENO"
    }
    pipeline = MIMICRegPipeline(StandardLogReg("PHENO"), **pipeline_config)
    pipeline.fit(data_path=data_path)

    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ["auc_roc_micro", "auc_roc_macro"],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': "PHENO"
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg("PHENO"), **pipeline_config)
    pipeline.fit(data_path=data_path)


def test_regression_pipeline_decomp():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "DECOMP")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="DECOMP")
    """
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': 'decompensation'
    }
    pipeline = MIMICRegPipeline(StandardLogReg('decompensation'), **pipeline_config)
    pipeline.fit(data_path=data_path)
    """
    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': 'decompensation'
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg('decompensation'), **pipeline_config)
    pipeline.fit(data_path=data_path)


def test_regression_pipeline_los():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "LOS")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="LOS")
    # One shot optimization not supported for los
    """
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ['accuracy'],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': 'length_of_stay'
    }
    pipeline = MIMICRegPipeline(StandardLogReg('length_of_stay'), **pipeline_config)
    pipeline.fit(data_path=data_path)
    """

    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ['accuracy'],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': 'length_of_stay'
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg('length_of_stay'), **pipeline_config)
    pipeline.fit(data_path=data_path)


if __name__ == "__main__":
    test_regression_pipeline_ihm()
    test_regression_pipeline_phenotyping()
    test_regression_pipeline_decomp()
    test_regression_pipeline_los()
'''


### FILE: .\tests\test_pipelines\__init__.py ###


### FILE: .\tests\test_preprocessing\test_imputer.py ###
# Test the preprocessing.imputer class using the preprocessing_readers from conftest.py. You can find the imputer use case in preprocessing.discretizer and preprocessing.normalizer
import pytest
from preprocessing.imputers import PartialImputer
from tests.settings import *


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_imputer_fit_dataset(preprocessing_readers, task_name):
    # Arrange
    reader = preprocessing_readers()
    data = reader.get_random()  # assuming the reader has a read method that returns data
    imp = PartialImputer()  # replace with actual imputer initialization if different

    # Act
    imputed_data = imp.fit_transform(discretized_data)

    # Assert
    # replace with actual assertions based on your expectations
    assert imputed_data is not None
    assert imputed_data.isnull().sum().sum() == 0  # assuming imputer fills all NaN values


def test_imputer_with_normalizer():
    # Arrange
    reader = preprocessing_readers()
    data = reader.read()  # assuming the reader has a read method that returns data
    norm = normalizer.Normalizer()  # replace with actual normalizer initialization if different
    imp = imputer.Imputer()  # replace with actual imputer initialization if different

    # Act
    normalized_data = norm.fit_transform(data)
    imputed_data = imp.fit_transform(normalized_data)

    # Assert
    # replace with actual assertions based on your expectations
    assert imputed_data is not None
    assert imputed_data.isnull().sum().sum() == 0  # assuming imputer fills all NaN values


### FILE: .\tests\test_pytest_suite.py\test_pytest_utils.py ###
import pytest
import pandas as pd
import numpy as np
from tests.pytest_utils.general import assert_dataframe_equals
from copy import deepcopy


def generate_random_frame(n_rows, n_cols, numeric=False):
    # Generate random data of different types for the DataFrame

    data = {
        f"col_{i}":
            np.where(
                np.random.rand(n_rows) < 0.2,
                np.nan,  # Inject NaNs
                np.random.randint(0, 100, size=n_rows) if i % 3 == 0 else np.random.rand(n_rows) *
                100 if i %
                3 == 1 or numeric else np.random.choice(['A', 'B', 'C', 'D'], size=n_rows))
        for i in range(n_cols)
    }

    # Create the DataFrame
    df_random = pd.DataFrame(data)
    return df_random


def generate_random_series(dtypes: pd.Series):
    series_data = pd.Series(index=dtypes.index, dtype=object)

    for col, dtype in dtypes.items():
        dtype_name = dtype.name
        if dtype_name.startswith('int'):
            series_data[col] = np.random.randint(0, 100)
        elif dtype_name.startswith('float'):
            series_data[col] = np.random.random() * 100
        elif dtype_name == 'object':
            series_data[col] = ''.join(
                np.random.choice(list('abcdefghijklmnopqrstuvwxyz')) for _ in range(5))
        elif dtype_name == 'datetime64[ns]':
            series_data[col] = pd.Timestamp('2020-01-01') + pd.to_timedelta(
                np.random.randint(0, 365), unit='D')
        elif dtype_name == 'category':
            categories = ['A', 'B', 'C', 'D']
            series_data[col] = pd.Categorical([np.random.choice(categories)], categories=categories)
        else:
            series_data[col] = np.nan

    return series_data


def test_compare_dataframes():
    nested_mock_columns = {
        "int": {
            "1": [1, 2, 3],
            "2": [1, 4, 3]
        },
        "float": {
            "1": [1.0, 2.5, 3.5],
            "2": [1.0, 2.4, 3.5]
        },
        "string1": {
            "1": ["a", "b", "c"],
            "2": ["a", "B", "c"]
        },
        "string2": {
            "1": ["a", "b", "c"],
            "2": ["a", "bb", "c"]
        },
        "np_int": {
            "1": np.array([1, 2, 3]),
            "2": np.array([1, 4, 3])
        },
        "np_float": {
            "1": np.array([1.0, 2.5, 3.5]),
            "2": np.array([1.0, 2.4, 3.5])
        },
        "pandas_categorical": {
            "1": pd.Categorical(["test", "train", "test"], categories=["test", "train", "val"]),
            "2": pd.Categorical(["test", "train", "val"], categories=["test", "train", "val"])
        }
    }

    # Test single entry differences
    base_frame_df = generate_random_frame(3, 3, True)
    assert_dataframe_equals(base_frame_df, deepcopy(base_frame_df))
    for noramlized_by in ["generated", "groundtruth"]:
        for dtype, columns in nested_mock_columns.items():
            type_1_df = deepcopy(base_frame_df)
            type_1_df["col_3"] = columns["1"]
            type_2_df = deepcopy(base_frame_df)
            type_2_df["col_3"] = columns["2"]
            assert_dataframe_equals(type_1_df,
                                    type_1_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert_dataframe_equals(type_2_df,
                                    type_2_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_1_df,
                                        type_2_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="single_entry")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_2_df,
                                        type_1_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="single_entry")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

        # Test single entry dimension mismatch
        long_frame = deepcopy(base_frame_df)
        long_frame.loc[len(long_frame)] = generate_random_series(long_frame.dtypes)
        assert long_frame.shape == (4, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    long_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 3, Ground truth: 4.")

        short_frame = deepcopy(base_frame_df)
        short_frame = short_frame.drop(short_frame.index[-1])
        assert short_frame.shape == (2, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    short_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 3, Ground truth: 2.")

        wide_frame = deepcopy(base_frame_df)
        wide_frame["col_3"] = generate_random_series(pd.Series([long_frame.dtypes[0]] * 3))
        assert wide_frame.shape == (3, 4)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    wide_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 3, Ground truth: 4.")

        narrow_frame = deepcopy(base_frame_df)
        narrow_frame = narrow_frame.iloc[:, :-1]
        assert narrow_frame.shape == (3, 2)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    narrow_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 3, Ground truth: 2.")

    # Test multi entry differences
    base_frame_df = pd.concat([generate_random_frame(3, 3) for _ in range(3)])
    base_frame_df.index = [f"{idx}_episode{idx}_timeseries.csv" for idx in base_frame_df.index]
    assert_dataframe_equals(base_frame_df, deepcopy(base_frame_df))
    for noramlized_by in ["generated", "groundtruth"]:
        for _, columns in nested_mock_columns.items():
            if "string" in dtype or "categorical" in dtype:
                # This is done for numeric dfs from the feature engine so no objects
                continue
            type_1_df = deepcopy(base_frame_df)
            type_1_df["col_3"] = columns["1"] * 3
            type_2_df = deepcopy(base_frame_df)
            type_2_df["col_3"] = columns["2"] * 3
            assert_dataframe_equals(type_1_df,
                                    type_1_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert_dataframe_equals(type_2_df,
                                    type_2_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_1_df,
                                        type_2_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="multiline")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_2_df,
                                        type_1_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="multiline")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

        # Test single entry dimension mismatch
        long_frame = deepcopy(base_frame_df)
        long_frame.loc[len(long_frame)] = generate_random_series(long_frame.dtypes)
        assert long_frame.shape == (10, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    long_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 9, Ground truth: 10.")

        short_frame = deepcopy(base_frame_df)
        short_frame = short_frame.iloc[0:len(short_frame) - 1]
        assert short_frame.shape == (8, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    short_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 9, Ground truth: 8.")

        wide_frame = deepcopy(base_frame_df)
        wide_frame["col_3"] = generate_random_series(pd.Series([long_frame.dtypes[0]] * 3))
        assert wide_frame.shape == (9, 4)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    wide_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 9, Ground truth: 4.")

        narrow_frame = deepcopy(base_frame_df)
        narrow_frame = narrow_frame.iloc[:, :-1]
        assert narrow_frame.shape == (9, 2)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    narrow_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 9, Ground truth: 2.")


if __name__ == "__main__":
    test_compare_dataframes()


### FILE: .\tests\test_pytest_suite.py\__init__.py ###


