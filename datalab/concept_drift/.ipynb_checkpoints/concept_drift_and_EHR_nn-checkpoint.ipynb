{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37cc672",
   "metadata": {},
   "source": [
    "# MIMIC-III: Concept-Drift and EHR for Neural Networks\n",
    "\n",
    "The MIMIC-III database has been gathered between 2001 and 2012 at the Beth Israel Deaconess Medical Center. In 2008, the hospital switched the its electronic health records (EHR) devices from the Carevue to Metavision system. This change reflects a more general phenomenon: the constant update and morphing of care practices, which is reflected on the database as concept drift.\n",
    "\n",
    "This notebook will investigate the effect of this specific change in care practices on the predictive power of the models from the MIMIC-III benchmark. Note, this notebook will treat dnn methods, while another notebook was setup to treat linear methods. To do so, we will split the data by chartevents generated by the carevue and metavision system.\n",
    "\n",
    "We will begin by training the benchmark models with randomly picked subject and subsequently apply our split. We will evaluate the effect on the metrics of the models and additionally analysze the change in calibration charcteristics for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad099b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 09:13:44.839977: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-20 09:13:46.411354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:46.415641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:46.415822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from utils.mimic import get_sample_size\n",
    "\n",
    "import datasets\n",
    "from preprocessing.mimic import Preprocessor\n",
    "from preprocessing import Discretizer, BatchGenerator, Normalizer\n",
    "from models.lstm import LSTMNetwork\n",
    "from utils.IO import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824ebda",
   "metadata": {},
   "source": [
    "## Running the Benchmark Models\n",
    "We will begin with the linear models for the sake of simplicity.\n",
    "\n",
    "The phontyping task needs the full dataset to make valid predictions, therefore we do not recomend using it here.\n",
    "\n",
    "### Load and Prepare the Data\n",
    "Next we load and preprocess the data for a given set into sample, label pairs which can directly be fed to a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c43837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a task\n",
    "task = \"decompensation\" # length_of_stay, phenotyping, in_hospital_mortality, decompensation\n",
    "\n",
    "# Generate directories\n",
    "storage_path = Path(\"resources\", task)\n",
    "storage_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "raw_path = Path(storage_path / \"full_set\")\n",
    "raw_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8bec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-20 09:13:46:datasets/mimic.py:L 51 - task data\n",
      "INFO - 2022-06-20 09:13:48:preprocessing/mimic.py:L 70 - Only type available for this task is binary! Argument disregarded\n",
      "INFO - 2022-06-20 09:13:48:datasets/__init__.py:L 59 - Approximating test set size to 0.48999\n"
     ]
    }
   ],
   "source": [
    "# Load the data into usable, subject-wise timeseries elements\n",
    "(timeseries,\n",
    " episodic_data,\n",
    " subject_events,\n",
    " subject_diagnoses,\n",
    " subject_icu_history) = datasets.load_data(storage_path=raw_path)\n",
    "\n",
    "# Preprocess the data for our task\n",
    "preprocessor = Preprocessor(timeseries,\n",
    "                            episodic_data,\n",
    "                            subject_diagnoses,\n",
    "                            subject_icu_history,\n",
    "                            config_dict=Path(os.getenv(\"CONFIG\"), \"datasets.json\"))\n",
    "\n",
    "X_subjects, y_subjects = preprocessor.make_task_data(task)\n",
    "\n",
    "# Splitting the data\n",
    "(X_train, X_test, \n",
    " y_train, y_test) = datasets.train_test_split(X_subjects, \n",
    "                                              y_subjects, \n",
    "                                              test_size=0.49,\n",
    "                                              concatenate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180219da",
   "metadata": {},
   "source": [
    "### Setting up the Model Run\n",
    "\n",
    "Next we need to initialize the classes we will need for our model run. This includes:\n",
    "* Discretizer\n",
    "* Normalizer\n",
    "* Batch generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07e1e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-20 09:13:49:preprocessing/__init__.py:L 491 - Done computing new normalizer in 0.8831212520599365!\n"
     ]
    }
   ],
   "source": [
    "sample_period = 1.0\n",
    "eps = 1e-6\n",
    "\n",
    "discretizer = Discretizer(config_dictionary=Path(os.getenv(\"CONFIG\"), \"mimic\", \"discretizer_config.json\"),\n",
    "                          sample_period=sample_period,\n",
    "                          eps=eps)\n",
    "\n",
    "normalizer_file = Path(os.getenv(\"MODEL\"), \"mimic\", \"normalizer\", f\"normalizer_{task}.obj\")\n",
    "normalizer = Normalizer(normalizer_file)\n",
    "normalizer.fit_dataset(discretizer, X_train)\n",
    "\n",
    "\n",
    "train_generator = BatchGenerator(X_train, \n",
    "                                 y_train,                                  \n",
    "                                 discretizer,\n",
    "                                 \"fixed_cat\",\n",
    "                                 normalizer,\n",
    "                                 load_normalizer=True)\n",
    "\n",
    "test_generator = BatchGenerator(X_test, \n",
    "                                y_test,\n",
    "                                discretizer,\n",
    "                                \"fixed_cat\",\n",
    "                                normalizer,\n",
    "                                load_normalizer=True)\n",
    "\n",
    "saver = ModelCheckpoint(Path(os.getenv('MODEL'), \"mimic\"), save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04012ec6",
   "metadata": {},
   "source": [
    "### Setting up the Model\n",
    "Nest we need to setup the model. The model parameters are all predefined by the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63ecfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 09:13:49.880625: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-20 09:13:49.881292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:49.881511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:49.881646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:50.303177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:50.303372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:50.303504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 09:13:50.303621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4658 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " x (InputLayer)              [(None, None, 59)]        0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 59)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               96256     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,385\n",
      "Trainable params: 96,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNetwork(layer_size=128, \n",
    "                    depth=1, \n",
    "                    dropout_rate=0, \n",
    "                    task=task,\n",
    "                    input_dim=59)\n",
    "\n",
    "with open(Path(os.getenv(\"CONFIG\"), \"mimic\", \"model_config.json\")) as file: \n",
    "    model_config = json.load(file)[\"lstm\"][task]\n",
    "\n",
    "metrics_switch = {\n",
    "    \"length_of_stay\": 'accuracy', # [CohenKappa(num_classes=10, sparse_labels=True)], \n",
    "    \"phenotyping\": [AUC(curve='ROC'), AUC(curve='PR')], \n",
    "    \"in_hospital_mortality\": [AUC(curve='ROC'), AUC(curve='PR')], \n",
    "    \"decompensation\": [AUC(curve='ROC'), AUC(curve='PR'), 'accuracy']\n",
    "}\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=model_config[\"loss\"],\n",
    "              metrics=metrics_switch[task])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10339b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 09:13:53.575250: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_LEGACY_VARIANT\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_25'\n",
      "2022-06-20 09:13:54.114766: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - ETA: 0s - loss: 0.2243 - auc_4: 0.4467 - auc_5: 0.0292 - accuracy: 0.9477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 35s 47ms/step - loss: 0.2243 - auc_4: 0.4467 - auc_5: 0.0292 - accuracy: 0.9477\n",
      "Epoch 2/5\n",
      "675/676 [============================>.] - ETA: 0s - loss: 0.1506 - auc_4: 0.6246 - auc_5: 0.0924 - accuracy: 0.9645"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 43s 64ms/step - loss: 0.1504 - auc_4: 0.6245 - auc_5: 0.0924 - accuracy: 0.9646\n",
      "Epoch 3/5\n",
      "675/676 [============================>.] - ETA: 0s - loss: 0.1181 - auc_4: 0.8292 - auc_5: 0.3239 - accuracy: 0.9668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 40s 59ms/step - loss: 0.1179 - auc_4: 0.8293 - auc_5: 0.3239 - accuracy: 0.9669\n",
      "Epoch 4/5\n",
      "674/676 [============================>.] - ETA: 0s - loss: 0.0961 - auc_4: 0.9087 - auc_5: 0.4745 - accuracy: 0.9705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 46s 68ms/step - loss: 0.0958 - auc_4: 0.9089 - auc_5: 0.4745 - accuracy: 0.9706\n",
      "Epoch 5/5\n",
      "676/676 [==============================] - ETA: 0s - loss: 0.0831 - auc_4: 0.8794 - auc_5: 0.6341 - accuracy: 0.9779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "676/676 [==============================] - 44s 66ms/step - loss: 0.0831 - auc_4: 0.8794 - auc_5: 0.6341 - accuracy: 0.9779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f592c42d130>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          steps_per_epoch=train_generator.steps,\n",
    "          epochs=5,\n",
    "          callbacks=[saver]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee036ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666/666 [==============================] - 42s 60ms/step - loss: 0.4125 - auc_4: 0.6606 - auc_5: 0.0998 - accuracy: 0.9002\n"
     ]
    }
   ],
   "source": [
    "prediction = model.evaluate(test_generator, \n",
    "                            steps=test_generator.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d3d6e",
   "metadata": {},
   "source": [
    "# Creating Concept Drift\n",
    "Now that we have our benchmark model ready, we can begin by investigating the concept drift, which is induced through the EHR switch. The first concern to make sure that the comparision is valid is to deduce the test set size and retrospectively align it for our benchmark. To do so, we will count the number of total available samples for the carevue system.\n",
    "\n",
    "We can load the carevue-only timeseries data by specifying the 'ehr' parameter with our load_data function. For subsequent variables, we will use the mv suffix for metavision samples and the cv suffix for carevue samples.\n",
    "\n",
    "### Preparing Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841cadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a task\n",
    "task = \"decompensation\" # length_of_stay, phenotyping, in_hospital_mortality, decompensation\n",
    "\n",
    "# Generate directories\n",
    "storage_path = Path(\"resources\", task)\n",
    "storage_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "raw_path_cv = Path(storage_path / \"carevue\")\n",
    "raw_path_mv = Path(storage_path / \"metavision\")\n",
    "raw_path_cv.mkdir(parents=True, exist_ok=True)\n",
    "raw_path_mv.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logistic_path_cv = Path(storage_path / \"logistic\" / \"carevue\")\n",
    "logistic_path_mv = Path(storage_path / \"logistic\" / \"metavision\")\n",
    "logistic_path_cv.mkdir(parents=True, exist_ok=True)\n",
    "logistic_path_mv.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922b8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-20 09:18:02:datasets/mimic.py:L 51 - task data\n",
      "INFO - 2022-06-20 09:18:03:preprocessing/mimic.py:L 70 - Only type available for this task is binary! Argument disregarded\n"
     ]
    }
   ],
   "source": [
    "# Choose a task\n",
    "storage_path = Path(\"resources\", task)\n",
    "\n",
    "# Load the data into usable, subject-wise timeseries elements\n",
    "(timeseries_cv,\n",
    " episodic_data_cv,\n",
    " subject_events_cv,\n",
    " subject_diagnoses_cv,\n",
    " subject_icu_history_cv) = datasets.load_data(storage_path=raw_path_cv,\n",
    "                                                    ehr='carevue')\n",
    "\n",
    "# Preprocess the data for our task\n",
    "preprocessor = Preprocessor(timeseries_cv,\n",
    "                            episodic_data_cv,\n",
    "                            subject_diagnoses_cv,\n",
    "                            subject_icu_history_cv,\n",
    "                            config_dict=Path(os.getenv(\"CONFIG\"), \"datasets.json\"))\n",
    "\n",
    "X_subjects_cv, y_subjects_cv = preprocessor.make_task_data(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64b44010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-20 09:18:04:datasets/mimic.py:L 51 - task data\n",
      "INFO - 2022-06-20 09:18:06:preprocessing/mimic.py:L 70 - Only type available for this task is binary! Argument disregarded\n"
     ]
    }
   ],
   "source": [
    "# Load the data into usable, subject-wise timeseries elements\n",
    "(timeseries_mv,\n",
    " episodic_data_mv,\n",
    " subject_events_mv,\n",
    " subject_diagnoses_mv,\n",
    " subject_icu_history_mv) = datasets.load_data(ehr=\"metavision\", storage_path=raw_path_mv)\n",
    "\n",
    "# Preprocess the data for our task\n",
    "preprocessor = Preprocessor(timeseries_mv,\n",
    "                            episodic_data_mv,\n",
    "                            subject_diagnoses_mv,\n",
    "                            subject_icu_history_mv,\n",
    "                            config_dict=Path(os.getenv(\"CONFIG\"), \"datasets.json\"))\n",
    "\n",
    "X_subjects_mv, y_subjects_mv = preprocessor.make_task_data(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8370f3",
   "metadata": {},
   "source": [
    "### Setting up the Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9248c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-20 09:18:11:preprocessing/__init__.py:L 491 - Done computing new normalizer in 1.429790735244751!\n"
     ]
    }
   ],
   "source": [
    "sample_period = 1.0\n",
    "eps = 1e-6\n",
    "\n",
    "X_train_cd = [*X_subjects_cv.values()]\n",
    "y_train_cd = [*y_subjects_cv.values()]\n",
    "X_test_cd = [*X_subjects_mv.values()]\n",
    "y_test_cd = [*y_subjects_mv.values()]\n",
    "\n",
    "discretizer_cd = Discretizer(config_dictionary=Path(os.getenv(\"CONFIG\"), \"mimic\", \"discretizer_config.json\"),\n",
    "                             sample_period=sample_period,\n",
    "                             eps=eps)\n",
    "\n",
    "normalizer_cd = Normalizer(normalizer_file)\n",
    "normalizer.fit_dataset(discretizer, X_subjects_cv.values())\n",
    "\n",
    "\n",
    "train_generator_cd = BatchGenerator(X_train_cd, \n",
    "                                    y_train_cd,                                  \n",
    "                                    discretizer,\n",
    "                                    \"fixed_cat\",\n",
    "                                    normalizer,\n",
    "                                    load_normalizer=True)\n",
    "\n",
    "test_generator_cd = BatchGenerator(X_test_cd, \n",
    "                                   y_test_cd,\n",
    "                                   discretizer,\n",
    "                                   \"fixed_cat\",\n",
    "                                   normalizer,\n",
    "                                   load_normalizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b9fc7",
   "metadata": {},
   "source": [
    "### Determining the Test Set Size\n",
    "Luckily our train_test_split function already set the requirement to count the total sample size of a subset of subjects. This functionality is implemented with the get_sample_size function which we imported previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c300dd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4948559670781893"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = get_sample_size(X_subjects_mv) / (get_sample_size(X_subjects_mv) +  get_sample_size(X_subjects_cv))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d9179",
   "metadata": {},
   "source": [
    "### Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ce029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_network_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " x (InputLayer)              [(None, None, 59)]        0         \n",
      "                                                                 \n",
      " masking_1 (Masking)         (None, None, 59)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               96256     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,385\n",
      "Trainable params: 96,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cd = LSTMNetwork(layer_size=128, \n",
    "                       depth=1, \n",
    "                       dropout_rate=0, \n",
    "                       task=task,\n",
    "                       input_dim=59)\n",
    "\n",
    "model_cd.compile(optimizer='adam',\n",
    "              loss=model_config[\"loss\"],\n",
    "              metrics=metrics_switch[task])\n",
    "\n",
    "model_cd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619690f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "627/631 [============================>.] - ETA: 0s - loss: 0.1519 - auc_4: 0.6406 - auc_5: 0.0746 - accuracy: 0.9668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631/631 [==============================] - 48s 69ms/step - loss: 0.1515 - auc_4: 0.6398 - auc_5: 0.0744 - accuracy: 0.9670\n",
      "Epoch 2/5\n",
      "628/631 [============================>.] - ETA: 0s - loss: 0.1249 - auc_4: 0.6414 - auc_5: 0.0459 - accuracy: 0.9691"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /work/model/mimic/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631/631 [==============================] - 52s 82ms/step - loss: 0.1255 - auc_4: 0.6386 - auc_5: 0.0443 - accuracy: 0.9693\n",
      "Epoch 3/5\n",
      "299/631 [=============>................] - ETA: 19s - loss: 0.0823 - auc_4: 0.3029 - auc_5: 0.0087 - accuracy: 0.9878"
     ]
    }
   ],
   "source": [
    "model_cd.fit(train_generator_cd,\n",
    "          steps_per_epoch=train_generator_cd.steps,\n",
    "          epochs=5,\n",
    "          callbacks=[saver]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f01584",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_cd.evaluate(test_generator_cd, \n",
    "                               steps=test_generator_cd.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2df3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
